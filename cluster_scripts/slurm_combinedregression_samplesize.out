Using TensorFlow backend.
27.04.2018 09:45:32 INFO    Hi! How are you today?
27.04.2018 09:45:32 INFO    Startup options:
27.04.2018 09:45:32 INFO      Algorithm:                     combinedregression
27.04.2018 09:45:32 INFO      Point by point:                False
27.04.2018 09:45:32 INFO      Morphing-aware mode:           False
27.04.2018 09:45:32 INFO      Smeared data:                  False
27.04.2018 09:45:32 INFO      Training sample:               baseline
27.04.2018 09:45:32 INFO      alpha:                         None
27.04.2018 09:45:32 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
27.04.2018 09:45:32 INFO      AFC epsilon:                   None
27.04.2018 09:45:32 INFO      Denominator theta:             False
27.04.2018 09:45:32 INFO      Neyman construction toys:      0
27.04.2018 09:45:32 INFO      Training sample size limit:    2000000
27.04.2018 09:45:32 INFO      Other options:                 ['deep']
27.04.2018 09:45:32 INFO      Base directory:                /home/jb6504/higgs_inference
27.04.2018 09:45:32 INFO      ML-based strategies available: True
27.04.2018 09:45:32 INFO    Starting parameterized inference
27.04.2018 09:45:32 INFO    Main settings:
27.04.2018 09:45:32 INFO      Algorithm:                combinedregression
27.04.2018 09:45:32 INFO      Morphing-aware:           False
27.04.2018 09:45:32 INFO      Training sample:          baseline
27.04.2018 09:45:32 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
27.04.2018 09:45:32 INFO    Options:
27.04.2018 09:45:32 INFO      Number of hidden layers:  5
27.04.2018 09:45:32 INFO      alpha:                    100.0
27.04.2018 09:45:32 INFO      Batch size:               128
27.04.2018 09:45:32 INFO      Learning rate:            0.001
27.04.2018 09:45:32 INFO      Learning rate decay:      0.018420680744
27.04.2018 09:45:32 INFO      Number of epochs:         250
27.04.2018 09:45:32 INFO      Training samples:         2000000
27.04.2018 09:45:32 INFO      NC experiments:           False
27.04.2018 09:45:32 INFO      Debug mode:               False
27.04.2018 09:45:56 INFO    Reduced training sample size from 9999997 to 2000000 (factor 5)
27.04.2018 09:46:15 INFO    Starting training
2018-04-27 09:46:16.458950: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 09:46:16.458984: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 09:46:16.458989: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 09:46:16.458993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 09:46:16.458998: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 09:46:16.758619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:05:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-27 09:46:16.758655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-27 09:46:16.758661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-27 09:46:16.758669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0)
Epoch 00242: early stopping
27.04.2018 17:25:11 INFO    Starting evaluation
27.04.2018 17:28:10 INFO    Starting theta 100 / 1017
27.04.2018 17:31:04 INFO    Starting theta 200 / 1017
27.04.2018 17:33:58 INFO    Starting theta 300 / 1017
27.04.2018 17:36:52 INFO    Starting theta 400 / 1017
27.04.2018 17:39:47 INFO    Starting theta 500 / 1017
27.04.2018 17:42:41 INFO    Starting theta 600 / 1017
27.04.2018 17:45:35 INFO    Starting theta 700 / 1017
27.04.2018 17:48:29 INFO    Starting theta 800 / 1017
27.04.2018 17:51:23 INFO    Starting theta 900 / 1017
27.04.2018 17:54:17 INFO    Starting theta 1000 / 1017
27.04.2018 17:54:48 INFO    Evaluation timing: median 1.73031687737 s, mean 1.73012906588 s
27.04.2018 17:54:48 INFO    Starting roaming
27.04.2018 17:55:23 INFO    Starting calibrated evaluation and roaming
27.04.2018 19:55:19 INFO    Starting theta 100 / 1017
27.04.2018 21:56:45 INFO    Starting theta 200 / 1017
27.04.2018 23:57:40 INFO    Starting theta 300 / 1017
28.04.2018 01:58:29 INFO    Starting theta 400 / 1017
28.04.2018 03:59:20 INFO    Starting theta 500 / 1017
28.04.2018 06:00:06 INFO    Starting theta 600 / 1017
28.04.2018 08:00:59 INFO    Starting theta 700 / 1017
28.04.2018 10:01:50 INFO    Starting theta 800 / 1017
28.04.2018 12:03:00 INFO    Starting theta 900 / 1017
28.04.2018 14:04:34 INFO    Starting theta 1000 / 1017
28.04.2018 14:26:29 INFO    Calibrated evaluation timing: median 1.74010109901 s, mean 1.74102531492 s
28.04.2018 14:26:29 INFO    Interpolating calibrated roaming
28.04.2018 14:31:17 INFO    That's it -- have a great day!
