Using TensorFlow backend.
16.07.2018 11:30:31 INFO    Hi! How are you today?
16.07.2018 11:30:31 INFO    Startup options:
16.07.2018 11:30:31 INFO      Algorithm:                     combinedregression
16.07.2018 11:30:31 INFO      Point by point:                False
16.07.2018 11:30:31 INFO      Morphing-aware mode:           False
16.07.2018 11:30:31 INFO      Smeared data:                  False
16.07.2018 11:30:31 INFO      Training sample:               baseline
16.07.2018 11:30:31 INFO      alpha:                         None
16.07.2018 11:30:31 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
16.07.2018 11:30:31 INFO      AFC epsilon:                   None
16.07.2018 11:30:31 INFO      Denominator theta:             True
16.07.2018 11:30:31 INFO      Neyman construction toys:      0
16.07.2018 11:30:31 INFO      Training sample size limit:    100000
16.07.2018 11:30:31 INFO      Other options:                 ['deep', 'neyman2']
16.07.2018 11:30:31 INFO      Base directory:                /home/jb6504/higgs_inference
16.07.2018 11:30:31 INFO      ML-based strategies available: True
16.07.2018 11:30:31 INFO    Starting parameterized inference
16.07.2018 11:30:31 INFO    Main settings:
16.07.2018 11:30:31 INFO      Algorithm:                combinedregression
16.07.2018 11:30:31 INFO      Morphing-aware:           False
16.07.2018 11:30:31 INFO      Training sample:          baseline
16.07.2018 11:30:31 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
16.07.2018 11:30:31 INFO    Options:
16.07.2018 11:30:31 INFO      Number of hidden layers:  5
16.07.2018 11:30:31 INFO      alpha:                    100.0
16.07.2018 11:30:31 INFO      Batch size:               128
16.07.2018 11:30:31 INFO      Learning rate:            0.001
16.07.2018 11:30:31 INFO      Learning rate decay:      0.000921034037198
16.07.2018 11:30:31 INFO      Number of epochs:         5000
16.07.2018 11:30:31 INFO      Training samples:         100000
16.07.2018 11:30:31 INFO      NC experiments:           (10000 alternate + 10000 null) experiments with 1 alternate events each
16.07.2018 11:30:31 INFO      Debug mode:               False
16.07.2018 11:30:51 INFO    Reduced training sample size from 9999997 to 100000 (factor 100)
16.07.2018 11:31:05 INFO    Starting training
2018-07-16 11:31:05.781560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-16 11:31:05.781595: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-16 11:31:05.781600: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-07-16 11:31:05.781604: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-16 11:31:05.781608: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-07-16 11:31:06.127658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:85:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-07-16 11:31:06.127697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-07-16 11:31:06.127703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-07-16 11:31:06.127711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:85:00.0)
Epoch 01064: early stopping
16.07.2018 13:10:47 INFO    Starting evaluation
16.07.2018 13:20:50 INFO    Starting theta 100 / 1017
16.07.2018 13:24:52 INFO    Starting theta 200 / 1017
16.07.2018 13:28:55 INFO    Starting theta 300 / 1017
16.07.2018 13:32:58 INFO    Starting theta 400 / 1017
16.07.2018 13:37:01 INFO    Starting theta 500 / 1017
16.07.2018 13:41:04 INFO    Starting theta 600 / 1017
16.07.2018 13:45:06 INFO    Starting theta 700 / 1017
16.07.2018 13:49:09 INFO    Starting theta 800 / 1017
16.07.2018 13:53:12 INFO    Starting theta 900 / 1017
16.07.2018 13:57:15 INFO    Starting theta 1000 / 1017
16.07.2018 13:57:59 INFO    Evaluation timing: median 1.71666693687 s, mean 1.71766832351 s
16.07.2018 13:57:59 INFO    Starting roaming
16.07.2018 13:58:34 INFO    Starting calibrated evaluation and roaming
