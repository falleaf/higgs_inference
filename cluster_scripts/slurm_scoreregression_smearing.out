Using TensorFlow backend.
16.03.2018 14:10:19 INFO    Hi! How are you today?
16.03.2018 14:10:19 INFO    Startup options:
16.03.2018 14:10:19 INFO      Algorithm:                     scoreregression
16.03.2018 14:10:19 INFO      Point by point:                False
16.03.2018 14:10:19 INFO      Morphing-aware mode:           False
16.03.2018 14:10:19 INFO      Smeared data:                  True
16.03.2018 14:10:19 INFO      Training sample:               baseline
16.03.2018 14:10:19 INFO      alpha:                         None
16.03.2018 14:10:19 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
16.03.2018 14:10:19 INFO      AFC epsilon:                   None
16.03.2018 14:10:19 INFO      Denominator theta:             True
16.03.2018 14:10:19 INFO      Neyman construction toys:      0
16.03.2018 14:10:19 INFO      Other options:                 ['deep', 'new', 'neyman2']
16.03.2018 14:10:19 INFO      Base directory:                /home/jb6504/higgs_inference
16.03.2018 14:10:19 INFO      ML-based strategies available: True
16.03.2018 14:10:19 INFO    Starting score regression inference
16.03.2018 14:10:19 INFO    Options:
16.03.2018 14:10:19 INFO      Denominator theta:       denominator 0 = theta 708 = [ 0.39293227  0.43229216]
16.03.2018 14:10:19 INFO      Number of hidden layers: 5
16.03.2018 14:10:19 INFO      Batch size:              128
16.03.2018 14:10:19 INFO      Learning rate:           0.001
16.03.2018 14:10:19 INFO      Learning rate decay:     0.0921034037198
16.03.2018 14:10:19 INFO      Number of epochs:        50
16.03.2018 14:10:19 INFO      NC experiments:          (10000 alternate + 10000 null) experiments with 1 alternate events each
16.03.2018 14:10:51 INFO    Starting training of score regression
Train on 8000000 samples, validate on 2000000 samples
2018-03-16 14:10:52.332253: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:52.332607: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:52.332617: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:52.332621: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:52.332625: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:52.631456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:05:00.0
Total memory: 15.89GiB
Free memory: 15.60GiB
2018-03-16 14:10:52.631492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-03-16 14:10:52.631498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-03-16 14:10:52.631506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0)
Epoch 1/50
342s - loss: 0.0790 - val_loss: 0.0611
Epoch 2/50
343s - loss: 0.0568 - val_loss: 0.0542
Epoch 3/50
343s - loss: 0.0467 - val_loss: 0.0428
Epoch 4/50
342s - loss: 0.0421 - val_loss: 0.0376
Epoch 5/50
343s - loss: 0.0391 - val_loss: 0.0332
Epoch 6/50
341s - loss: 0.0346 - val_loss: 0.0270
Epoch 7/50
342s - loss: 0.0317 - val_loss: 0.0310
Epoch 8/50
341s - loss: 0.0299 - val_loss: 0.0261
Epoch 9/50
341s - loss: 0.0283 - val_loss: 0.0205
Epoch 10/50
344s - loss: 0.0262 - val_loss: 0.0235
Epoch 11/50
341s - loss: 0.0244 - val_loss: 0.0207
Epoch 12/50
343s - loss: 0.0231 - val_loss: 0.0185
Epoch 13/50
342s - loss: 0.0223 - val_loss: 0.0175
Epoch 14/50
341s - loss: 0.0210 - val_loss: 0.0197
Epoch 15/50
341s - loss: 0.0200 - val_loss: 0.0162
Epoch 16/50
341s - loss: 0.0194 - val_loss: 0.0154
Epoch 17/50
343s - loss: 0.0186 - val_loss: 0.0162
Epoch 18/50
342s - loss: 0.0179 - val_loss: 0.0148
Epoch 19/50
342s - loss: 0.0174 - val_loss: 0.0144
Epoch 20/50
