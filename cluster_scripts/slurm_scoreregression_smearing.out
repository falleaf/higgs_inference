Using TensorFlow backend.
06.03.2018 19:49:56 INFO    Hi! How are you today?
06.03.2018 19:49:56 INFO    Startup options:
06.03.2018 19:49:56 INFO      Algorithm:                     scoreregression
06.03.2018 19:49:56 INFO      Point by point:                False
06.03.2018 19:49:56 INFO      Morphing-aware mode:           False
06.03.2018 19:49:56 INFO      Smeared data:                  True
06.03.2018 19:49:56 INFO      Training sample:               baseline
06.03.2018 19:49:56 INFO      alpha:                         None
06.03.2018 19:49:56 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
06.03.2018 19:49:56 INFO      AFC epsilon:                   None
06.03.2018 19:49:56 INFO      Neyman construction toys:      False
06.03.2018 19:49:56 INFO      Other options:                 ['deep', 'new']
06.03.2018 19:49:56 INFO      Base directory:                /home/jb6504/higgs_inference
06.03.2018 19:49:56 INFO      ML-based strategies available: True
06.03.2018 19:49:56 INFO    Starting score regression inference
06.03.2018 19:49:56 INFO    Options:
06.03.2018 19:49:56 INFO      Number of hidden layers: 5
06.03.2018 19:49:56 INFO      Batch size:              128
06.03.2018 19:49:56 INFO      Learning rate:           0.001
06.03.2018 19:49:56 INFO      Learning rate decay:     0.0921034037198
06.03.2018 19:49:56 INFO      Number of epochs:        50
06.03.2018 19:50:25 INFO    Starting training of score regression
Train on 8000000 samples, validate on 2000000 samples
2018-03-06 19:50:25.569020: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-06 19:50:25.569320: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-06 19:50:25.569328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-06 19:50:25.569333: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-06 19:50:25.569337: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-06 19:50:25.899806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla P40
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:04:00.0
Total memory: 22.38GiB
Free memory: 22.21GiB
2018-03-06 19:50:25.899838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-03-06 19:50:25.899843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-03-06 19:50:25.899850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
Epoch 1/50
359s - loss: 0.0795 - val_loss: 0.0516
Epoch 2/50
360s - loss: 0.0612 - val_loss: 0.0563
Epoch 3/50
361s - loss: 0.0526 - val_loss: 0.0468
Epoch 4/50
359s - loss: 0.0443 - val_loss: 0.0395
Epoch 5/50
358s - loss: 0.0398 - val_loss: 0.0325
Epoch 6/50
360s - loss: 0.0362 - val_loss: 0.0299
Epoch 7/50
361s - loss: 0.0336 - val_loss: 0.0257
Epoch 8/50
358s - loss: 0.0310 - val_loss: 0.0333
Epoch 9/50
356s - loss: 0.0292 - val_loss: 0.0228
Epoch 10/50
342s - loss: 0.0275 - val_loss: 0.0212
Epoch 11/50
340s - loss: 0.0259 - val_loss: 0.0250
Epoch 12/50
340s - loss: 0.0251 - val_loss: 0.0196
Epoch 13/50
339s - loss: 0.0235 - val_loss: 0.0194
Epoch 14/50
340s - loss: 0.0221 - val_loss: 0.0197
Epoch 15/50
340s - loss: 0.0211 - val_loss: 0.0187
Epoch 16/50
341s - loss: 0.0202 - val_loss: 0.0170
Epoch 17/50
338s - loss: 0.0195 - val_loss: 0.0180
Epoch 18/50
338s - loss: 0.0189 - val_loss: 0.0153
Epoch 19/50
339s - loss: 0.0180 - val_loss: 0.0144
Epoch 20/50
340s - loss: 0.0177 - val_loss: 0.0145
Epoch 21/50
339s - loss: 0.0172 - val_loss: 0.0144
Epoch 22/50
340s - loss: 0.0168 - val_loss: 0.0137
Epoch 23/50
340s - loss: 0.0164 - val_loss: 0.0134
Epoch 24/50
339s - loss: 0.0162 - val_loss: 0.0132
Epoch 25/50
337s - loss: 0.0158 - val_loss: 0.0132
Epoch 26/50
336s - loss: 0.0156 - val_loss: 0.0132
Epoch 27/50
335s - loss: 0.0153 - val_loss: 0.0127
Epoch 28/50
335s - loss: 0.0151 - val_loss: 0.0129
Epoch 29/50
334s - loss: 0.0149 - val_loss: 0.0129
Epoch 30/50
335s - loss: 0.0148 - val_loss: 0.0124
Epoch 31/50
334s - loss: 0.0147 - val_loss: 0.0122
Epoch 32/50
334s - loss: 0.0145 - val_loss: 0.0123
Epoch 33/50
335s - loss: 0.0144 - val_loss: 0.0122
Epoch 34/50
335s - loss: 0.0144 - val_loss: 0.0119
Epoch 35/50
334s - loss: 0.0142 - val_loss: 0.0123
Epoch 36/50
335s - loss: 0.0141 - val_loss: 0.0124
Epoch 37/50
335s - loss: 0.0140 - val_loss: 0.0120
Epoch 38/50
334s - loss: 0.0140 - val_loss: 0.0118
Epoch 39/50
335s - loss: 0.0139 - val_loss: 0.0119
Epoch 40/50
335s - loss: 0.0138 - val_loss: 0.0119
Epoch 41/50
334s - loss: 0.0138 - val_loss: 0.0117
Epoch 42/50
335s - loss: 0.0137 - val_loss: 0.0117
Epoch 43/50
335s - loss: 0.0137 - val_loss: 0.0117
Epoch 44/50
334s - loss: 0.0136 - val_loss: 0.0116
Epoch 45/50
335s - loss: 0.0136 - val_loss: 0.0116
Epoch 46/50
334s - loss: 0.0136 - val_loss: 0.0118
Epoch 47/50
335s - loss: 0.0135 - val_loss: 0.0117
Epoch 48/50
335s - loss: 0.0135 - val_loss: 0.0116
Epoch 49/50
335s - loss: 0.0135 - val_loss: 0.0116
Epoch 50/50
335s - loss: 0.0134 - val_loss: 0.0115
07.03.2018 00:34:42 INFO    Starting evaluation
07.03.2018 00:34:47 INFO    Starting density estimation
07.03.2018 00:35:01 INFO    Starting theta 100 / 1017
07.03.2018 00:35:16 INFO    Starting theta 200 / 1017
07.03.2018 00:35:30 INFO    Starting theta 300 / 1017
07.03.2018 00:35:45 INFO    Starting theta 400 / 1017
07.03.2018 00:35:59 INFO    Starting theta 500 / 1017
07.03.2018 00:36:13 INFO    Starting theta 600 / 1017
07.03.2018 00:36:28 INFO    Starting theta 700 / 1017
07.03.2018 00:36:42 INFO    Starting theta 800 / 1017
07.03.2018 00:36:57 INFO    Starting theta 900 / 1017
07.03.2018 00:37:11 INFO    Starting theta 1000 / 1017
07.03.2018 00:37:14 INFO    That's it -- have a great day!
