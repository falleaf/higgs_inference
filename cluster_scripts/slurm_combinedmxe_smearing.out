Using TensorFlow backend.
17.07.2018 16:25:23 INFO    Hi! How are you today?
17.07.2018 16:25:23 INFO    Startup options:
17.07.2018 16:25:23 INFO      Algorithm:                     combinedmxe
17.07.2018 16:25:23 INFO      Point by point:                False
17.07.2018 16:25:23 INFO      Morphing-aware mode:           False
17.07.2018 16:25:23 INFO      Smeared data:                  True
17.07.2018 16:25:23 INFO      Training sample:               baseline
17.07.2018 16:25:23 INFO      alpha:                         None
17.07.2018 16:25:23 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
17.07.2018 16:25:23 INFO      AFC epsilon:                   None
17.07.2018 16:25:23 INFO      Denominator theta:             True
17.07.2018 16:25:23 INFO      Neyman construction toys:      0
17.07.2018 16:25:23 INFO      Training sample size limit:    100000
17.07.2018 16:25:23 INFO      Other options:                 ['neyman2', 'deep']
17.07.2018 16:25:23 INFO      Base directory:                /home/jb6504/higgs_inference
17.07.2018 16:25:23 INFO      ML-based strategies available: True
17.07.2018 16:25:23 INFO    Starting parameterized inference
17.07.2018 16:25:23 INFO    Main settings:
17.07.2018 16:25:23 INFO      Algorithm:                combinedmxe
17.07.2018 16:25:23 INFO      Morphing-aware:           False
17.07.2018 16:25:23 INFO      Training sample:          baseline
17.07.2018 16:25:23 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
17.07.2018 16:25:23 INFO    Options:
17.07.2018 16:25:23 INFO      Number of hidden layers:  5
17.07.2018 16:25:23 INFO      alpha:                    5.0
17.07.2018 16:25:23 INFO      Batch size:               128
17.07.2018 16:25:23 INFO      Learning rate:            0.001
17.07.2018 16:25:23 INFO      Learning rate decay:      0.000921034037198
17.07.2018 16:25:23 INFO      Number of epochs:         5000
17.07.2018 16:25:23 INFO      Training samples:         100000
17.07.2018 16:25:23 INFO      NC experiments:           (10000 alternate + 10000 null) experiments with 1 alternate events each
17.07.2018 16:25:23 INFO      Debug mode:               False
17.07.2018 16:25:54 INFO    Reduced training sample size from 9999997 to 100000 (factor 100)
17.07.2018 16:26:01 INFO    Starting training
2018-07-17 16:26:01.657179: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-17 16:26:01.657209: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-17 16:26:01.657214: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-07-17 16:26:01.657217: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-17 16:26:01.657221: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-07-17 16:26:01.864775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla P40
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:85:00.0
Total memory: 22.38GiB
Free memory: 22.22GiB
2018-07-17 16:26:01.864809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-07-17 16:26:01.864815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-07-17 16:26:01.864823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:85:00.0)
Epoch 01522: early stopping
17.07.2018 18:34:24 INFO    Starting evaluation
17.07.2018 18:41:26 INFO    Starting theta 100 / 1017
17.07.2018 18:44:02 INFO    Starting theta 200 / 1017
17.07.2018 18:46:37 INFO    Starting theta 300 / 1017
17.07.2018 18:49:13 INFO    Starting theta 400 / 1017
17.07.2018 18:51:49 INFO    Starting theta 500 / 1017
17.07.2018 18:54:25 INFO    Starting theta 600 / 1017
17.07.2018 18:57:01 INFO    Starting theta 700 / 1017
17.07.2018 18:59:36 INFO    Starting theta 800 / 1017
17.07.2018 19:02:13 INFO    Starting theta 900 / 1017
17.07.2018 19:04:48 INFO    Starting theta 1000 / 1017
17.07.2018 19:05:16 INFO    Evaluation timing: median 1.08833098412 s, mean 1.09580422948 s
17.07.2018 19:05:16 INFO    Starting roaming
17.07.2018 19:05:21 INFO    Starting calibrated evaluation and roaming
17.07.2018 20:26:35 INFO    Starting theta 100 / 1017
17.07.2018 21:44:46 INFO    Starting theta 200 / 1017
17.07.2018 23:02:57 INFO    Starting theta 300 / 1017
18.07.2018 00:21:10 INFO    Starting theta 400 / 1017
18.07.2018 01:39:23 INFO    Starting theta 500 / 1017
18.07.2018 02:57:09 INFO    Starting theta 600 / 1017
18.07.2018 04:14:47 INFO    Starting theta 700 / 1017
18.07.2018 05:32:23 INFO    Starting theta 800 / 1017
18.07.2018 06:50:48 INFO    Starting theta 900 / 1017
18.07.2018 08:09:13 INFO    Starting theta 1000 / 1017
18.07.2018 08:23:20 INFO    Calibrated evaluation timing: median 1.08693313599 s, mean 1.08977043617 s
18.07.2018 08:23:20 INFO    Interpolating calibrated roaming
18.07.2018 08:24:43 INFO    That's it -- have a great day!
