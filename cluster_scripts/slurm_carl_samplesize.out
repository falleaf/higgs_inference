Using TensorFlow backend.
19.04.2018 10:34:06 INFO    Hi! How are you today?
19.04.2018 10:34:06 INFO    Startup options:
19.04.2018 10:34:06 INFO      Algorithm:                     carl
19.04.2018 10:34:06 INFO      Point by point:                False
19.04.2018 10:34:06 INFO      Morphing-aware mode:           False
19.04.2018 10:34:06 INFO      Smeared data:                  False
19.04.2018 10:34:06 INFO      Training sample:               baseline
19.04.2018 10:34:06 INFO      alpha:                         None
19.04.2018 10:34:06 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
19.04.2018 10:34:06 INFO      AFC epsilon:                   None
19.04.2018 10:34:06 INFO      Denominator theta:             False
19.04.2018 10:34:06 INFO      Neyman construction toys:      0
19.04.2018 10:34:06 INFO      Training sample size limit:    5000
19.04.2018 10:34:06 INFO      Other options:                 ['shallow']
19.04.2018 10:34:06 INFO      Base directory:                /home/jb6504/higgs_inference
19.04.2018 10:34:06 INFO      ML-based strategies available: True
19.04.2018 10:34:06 INFO    Starting parameterized inference
19.04.2018 10:34:06 INFO    Main settings:
19.04.2018 10:34:06 INFO      Algorithm:                carl
19.04.2018 10:34:06 INFO      Morphing-aware:           False
19.04.2018 10:34:06 INFO      Training sample:          baseline
19.04.2018 10:34:06 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
19.04.2018 10:34:06 INFO    Options:
19.04.2018 10:34:06 INFO      Number of hidden layers:  2
19.04.2018 10:34:06 INFO      Batch size:               128
19.04.2018 10:34:06 INFO      Learning rate:            0.001
19.04.2018 10:34:06 INFO      Learning rate decay:      4.60517018599e-05
19.04.2018 10:34:06 INFO      Number of epochs:         100000
19.04.2018 10:34:06 INFO      Training samples:         5000
19.04.2018 10:34:06 INFO      NC experiments:           False
19.04.2018 10:34:06 INFO      Debug mode:               False
19.04.2018 10:34:27 INFO    Reduced training sample size from 9999997 to 5000 (factor 2000)
19.04.2018 10:34:42 INFO    Starting training
2018-04-19 10:34:42.928594: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-19 10:34:42.928632: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-19 10:34:42.928637: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-19 10:34:42.928642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-19 10:34:42.928646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-19 10:34:43.230961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-19 10:34:43.230996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-19 10:34:43.231002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-19 10:34:43.231010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 20002: early stopping
19.04.2018 11:54:11 INFO    Starting evaluation
19.04.2018 11:56:24 INFO    Starting theta 100 / 1017
19.04.2018 11:58:32 INFO    Starting theta 200 / 1017
19.04.2018 12:00:40 INFO    Starting theta 300 / 1017
19.04.2018 12:02:48 INFO    Starting theta 400 / 1017
19.04.2018 12:04:57 INFO    Starting theta 500 / 1017
19.04.2018 12:07:05 INFO    Starting theta 600 / 1017
19.04.2018 12:09:13 INFO    Starting theta 700 / 1017
19.04.2018 12:11:21 INFO    Starting theta 800 / 1017
19.04.2018 12:13:30 INFO    Starting theta 900 / 1017
19.04.2018 12:15:38 INFO    Starting theta 1000 / 1017
19.04.2018 12:16:01 INFO    Evaluation timing: median 1.26979398727 s, mean 1.27101221783 s
19.04.2018 12:16:01 INFO    Starting roaming
19.04.2018 12:16:27 INFO    Starting calibrated evaluation and roaming
19.04.2018 13:44:35 INFO    Starting theta 100 / 1017
19.04.2018 15:13:30 INFO    Starting theta 200 / 1017
19.04.2018 16:42:26 INFO    Starting theta 300 / 1017
19.04.2018 18:11:25 INFO    Starting theta 400 / 1017
19.04.2018 19:40:23 INFO    Starting theta 500 / 1017
19.04.2018 21:09:11 INFO    Starting theta 600 / 1017
19.04.2018 22:38:02 INFO    Starting theta 700 / 1017
20.04.2018 00:06:51 INFO    Starting theta 800 / 1017
20.04.2018 01:35:58 INFO    Starting theta 900 / 1017
20.04.2018 03:05:19 INFO    Starting theta 1000 / 1017
20.04.2018 03:21:23 INFO    Calibrated evaluation timing: median 1.27446818352 s, mean 1.27530459598 s
20.04.2018 03:21:23 INFO    Interpolating calibrated roaming
20.04.2018 03:26:20 INFO    That's it -- have a great day!
Using TensorFlow backend.
20.04.2018 03:26:25 INFO    Hi! How are you today?
20.04.2018 03:26:25 INFO    Startup options:
20.04.2018 03:26:25 INFO      Algorithm:                     carl
20.04.2018 03:26:25 INFO      Point by point:                False
20.04.2018 03:26:25 INFO      Morphing-aware mode:           False
20.04.2018 03:26:25 INFO      Smeared data:                  False
20.04.2018 03:26:25 INFO      Training sample:               baseline
20.04.2018 03:26:25 INFO      alpha:                         None
20.04.2018 03:26:25 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
20.04.2018 03:26:25 INFO      AFC epsilon:                   None
20.04.2018 03:26:25 INFO      Denominator theta:             False
20.04.2018 03:26:25 INFO      Neyman construction toys:      0
20.04.2018 03:26:25 INFO      Training sample size limit:    10000
20.04.2018 03:26:25 INFO      Other options:                 ['shallow']
20.04.2018 03:26:25 INFO      Base directory:                /home/jb6504/higgs_inference
20.04.2018 03:26:25 INFO      ML-based strategies available: True
20.04.2018 03:26:25 INFO    Starting parameterized inference
20.04.2018 03:26:25 INFO    Main settings:
20.04.2018 03:26:25 INFO      Algorithm:                carl
20.04.2018 03:26:25 INFO      Morphing-aware:           False
20.04.2018 03:26:25 INFO      Training sample:          baseline
20.04.2018 03:26:25 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
20.04.2018 03:26:25 INFO    Options:
20.04.2018 03:26:25 INFO      Number of hidden layers:  2
20.04.2018 03:26:25 INFO      Batch size:               128
20.04.2018 03:26:25 INFO      Learning rate:            0.001
20.04.2018 03:26:25 INFO      Learning rate decay:      9.21034037198e-05
20.04.2018 03:26:25 INFO      Number of epochs:         50000
20.04.2018 03:26:25 INFO      Training samples:         10000
20.04.2018 03:26:25 INFO      NC experiments:           False
20.04.2018 03:26:25 INFO      Debug mode:               False
20.04.2018 03:26:45 INFO    Reduced training sample size from 9999997 to 10000 (factor 1000)
20.04.2018 03:26:59 INFO    Starting training
2018-04-20 03:26:59.722805: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 03:26:59.722844: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 03:26:59.722849: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 03:26:59.722857: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 03:26:59.722862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 03:27:00.011298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-20 03:27:00.011334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-20 03:27:00.011340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-20 03:27:00.011349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 10002: early stopping
20.04.2018 04:40:47 INFO    Starting evaluation
20.04.2018 04:42:53 INFO    Starting theta 100 / 1017
20.04.2018 04:44:56 INFO    Starting theta 200 / 1017
20.04.2018 04:46:58 INFO    Starting theta 300 / 1017
20.04.2018 04:49:01 INFO    Starting theta 400 / 1017
20.04.2018 04:51:03 INFO    Starting theta 500 / 1017
20.04.2018 04:53:06 INFO    Starting theta 600 / 1017
20.04.2018 04:55:08 INFO    Starting theta 700 / 1017
20.04.2018 04:57:11 INFO    Starting theta 800 / 1017
20.04.2018 04:59:14 INFO    Starting theta 900 / 1017
20.04.2018 05:01:16 INFO    Starting theta 1000 / 1017
20.04.2018 05:01:38 INFO    Evaluation timing: median 1.21540808678 s, mean 1.21630207627 s
20.04.2018 05:01:38 INFO    Starting roaming
20.04.2018 05:02:03 INFO    Starting calibrated evaluation and roaming
20.04.2018 06:26:53 INFO    Starting theta 100 / 1017
20.04.2018 07:54:28 INFO    Starting theta 200 / 1017
20.04.2018 09:22:10 INFO    Starting theta 300 / 1017
20.04.2018 10:49:47 INFO    Starting theta 400 / 1017
20.04.2018 12:17:38 INFO    Starting theta 500 / 1017
20.04.2018 13:45:20 INFO    Starting theta 600 / 1017
20.04.2018 15:12:57 INFO    Starting theta 700 / 1017
20.04.2018 16:40:38 INFO    Starting theta 800 / 1017
20.04.2018 18:08:27 INFO    Starting theta 900 / 1017
20.04.2018 19:36:05 INFO    Starting theta 1000 / 1017
20.04.2018 19:51:50 INFO    Calibrated evaluation timing: median 1.25441098213 s, mean 1.25163225415 s
20.04.2018 19:51:50 INFO    Interpolating calibrated roaming
20.04.2018 19:57:09 INFO    That's it -- have a great day!
Using TensorFlow backend.
20.04.2018 19:57:11 INFO    Hi! How are you today?
20.04.2018 19:57:11 INFO    Startup options:
20.04.2018 19:57:11 INFO      Algorithm:                     carl
20.04.2018 19:57:11 INFO      Point by point:                False
20.04.2018 19:57:11 INFO      Morphing-aware mode:           False
20.04.2018 19:57:11 INFO      Smeared data:                  False
20.04.2018 19:57:11 INFO      Training sample:               baseline
20.04.2018 19:57:11 INFO      alpha:                         None
20.04.2018 19:57:11 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
20.04.2018 19:57:11 INFO      AFC epsilon:                   None
20.04.2018 19:57:11 INFO      Denominator theta:             False
20.04.2018 19:57:11 INFO      Neyman construction toys:      0
20.04.2018 19:57:11 INFO      Training sample size limit:    20000
20.04.2018 19:57:11 INFO      Other options:                 ['shallow']
20.04.2018 19:57:11 INFO      Base directory:                /home/jb6504/higgs_inference
20.04.2018 19:57:11 INFO      ML-based strategies available: True
20.04.2018 19:57:11 INFO    Starting parameterized inference
20.04.2018 19:57:11 INFO    Main settings:
20.04.2018 19:57:11 INFO      Algorithm:                carl
20.04.2018 19:57:11 INFO      Morphing-aware:           False
20.04.2018 19:57:11 INFO      Training sample:          baseline
20.04.2018 19:57:11 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
20.04.2018 19:57:11 INFO    Options:
20.04.2018 19:57:11 INFO      Number of hidden layers:  2
20.04.2018 19:57:11 INFO      Batch size:               128
20.04.2018 19:57:11 INFO      Learning rate:            0.001
20.04.2018 19:57:11 INFO      Learning rate decay:      0.00018420680744
20.04.2018 19:57:11 INFO      Number of epochs:         25000
20.04.2018 19:57:11 INFO      Training samples:         20000
20.04.2018 19:57:11 INFO      NC experiments:           False
20.04.2018 19:57:11 INFO      Debug mode:               False
20.04.2018 19:57:37 INFO    Reduced training sample size from 9999997 to 20000 (factor 500)
20.04.2018 19:57:51 INFO    Starting training
2018-04-20 19:57:52.301117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 19:57:52.301152: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 19:57:52.301158: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 19:57:52.301162: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 19:57:52.301166: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 19:57:52.600744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-20 19:57:52.600780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-20 19:57:52.600787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-20 19:57:52.600796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 05005: early stopping
20.04.2018 21:16:30 INFO    Starting evaluation
Traceback (most recent call last):
  File "experiments.py", line 154, in <module>
    options=args.options)
  File "/home/jb6504/higgs_inference/higgs_inference/strategies/parameterized.py", line 496, in parameterized_inference
    mse_log_r.append(calculate_mean_squared_error(np.log(r_test[t]), this_log_r, 0.))
  File "/home/jb6504/higgs_inference/higgs_inference/various/utils.py", line 47, in calculate_mean_squared_error
    return mean_squared_error(y_true, y_pred)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/metrics/regression.py", line 231, in mean_squared_error
    y_true, y_pred, multioutput)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/metrics/regression.py", line 76, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/utils/validation.py", line 407, in check_array
    _assert_all_finite(array)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
Using TensorFlow backend.
20.04.2018 21:16:34 INFO    Hi! How are you today?
20.04.2018 21:16:34 INFO    Startup options:
20.04.2018 21:16:34 INFO      Algorithm:                     carl
20.04.2018 21:16:34 INFO      Point by point:                False
20.04.2018 21:16:34 INFO      Morphing-aware mode:           False
20.04.2018 21:16:34 INFO      Smeared data:                  False
20.04.2018 21:16:34 INFO      Training sample:               baseline
20.04.2018 21:16:34 INFO      alpha:                         None
20.04.2018 21:16:34 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
20.04.2018 21:16:34 INFO      AFC epsilon:                   None
20.04.2018 21:16:34 INFO      Denominator theta:             False
20.04.2018 21:16:34 INFO      Neyman construction toys:      0
20.04.2018 21:16:34 INFO      Training sample size limit:    50000
20.04.2018 21:16:34 INFO      Other options:                 ['shallow']
20.04.2018 21:16:34 INFO      Base directory:                /home/jb6504/higgs_inference
20.04.2018 21:16:34 INFO      ML-based strategies available: True
20.04.2018 21:16:34 INFO    Starting parameterized inference
20.04.2018 21:16:34 INFO    Main settings:
20.04.2018 21:16:34 INFO      Algorithm:                carl
20.04.2018 21:16:34 INFO      Morphing-aware:           False
20.04.2018 21:16:34 INFO      Training sample:          baseline
20.04.2018 21:16:34 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
20.04.2018 21:16:34 INFO    Options:
20.04.2018 21:16:34 INFO      Number of hidden layers:  2
20.04.2018 21:16:34 INFO      Batch size:               128
20.04.2018 21:16:34 INFO      Learning rate:            0.001
20.04.2018 21:16:34 INFO      Learning rate decay:      0.000460517018599
20.04.2018 21:16:34 INFO      Number of epochs:         10000
20.04.2018 21:16:34 INFO      Training samples:         50000
20.04.2018 21:16:34 INFO      NC experiments:           False
20.04.2018 21:16:34 INFO      Debug mode:               False
20.04.2018 21:16:59 INFO    Reduced training sample size from 9999997 to 50000 (factor 200)
20.04.2018 21:17:14 INFO    Starting training
2018-04-20 21:17:14.543443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 21:17:14.543477: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 21:17:14.543482: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 21:17:14.543487: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 21:17:14.543491: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 21:17:14.840584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-20 21:17:14.840622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-20 21:17:14.840628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-20 21:17:14.840637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 02011: early stopping
20.04.2018 22:35:28 INFO    Starting evaluation
Traceback (most recent call last):
  File "experiments.py", line 154, in <module>
    options=args.options)
  File "/home/jb6504/higgs_inference/higgs_inference/strategies/parameterized.py", line 496, in parameterized_inference
    mse_log_r.append(calculate_mean_squared_error(np.log(r_test[t]), this_log_r, 0.))
  File "/home/jb6504/higgs_inference/higgs_inference/various/utils.py", line 47, in calculate_mean_squared_error
    return mean_squared_error(y_true, y_pred)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/metrics/regression.py", line 231, in mean_squared_error
    y_true, y_pred, multioutput)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/metrics/regression.py", line 76, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/utils/validation.py", line 407, in check_array
    _assert_all_finite(array)
  File "/share/apps/scikit-learn/0.18.1/intel/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
Using TensorFlow backend.
20.04.2018 22:35:32 INFO    Hi! How are you today?
20.04.2018 22:35:32 INFO    Startup options:
20.04.2018 22:35:32 INFO      Algorithm:                     carl
20.04.2018 22:35:32 INFO      Point by point:                False
20.04.2018 22:35:32 INFO      Morphing-aware mode:           False
20.04.2018 22:35:32 INFO      Smeared data:                  False
20.04.2018 22:35:32 INFO      Training sample:               baseline
20.04.2018 22:35:32 INFO      alpha:                         None
20.04.2018 22:35:32 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
20.04.2018 22:35:32 INFO      AFC epsilon:                   None
20.04.2018 22:35:32 INFO      Denominator theta:             False
20.04.2018 22:35:32 INFO      Neyman construction toys:      0
20.04.2018 22:35:32 INFO      Training sample size limit:    100000
20.04.2018 22:35:32 INFO      Other options:                 ['shallow']
20.04.2018 22:35:32 INFO      Base directory:                /home/jb6504/higgs_inference
20.04.2018 22:35:32 INFO      ML-based strategies available: True
20.04.2018 22:35:32 INFO    Starting parameterized inference
20.04.2018 22:35:32 INFO    Main settings:
20.04.2018 22:35:32 INFO      Algorithm:                carl
20.04.2018 22:35:32 INFO      Morphing-aware:           False
20.04.2018 22:35:32 INFO      Training sample:          baseline
20.04.2018 22:35:32 INFO      Denominator theta:        denominator 0 = theta 708 = [ 0.39293227  0.43229216]
20.04.2018 22:35:32 INFO    Options:
20.04.2018 22:35:32 INFO      Number of hidden layers:  2
20.04.2018 22:35:32 INFO      Batch size:               128
20.04.2018 22:35:32 INFO      Learning rate:            0.001
20.04.2018 22:35:32 INFO      Learning rate decay:      0.000921034037198
20.04.2018 22:35:32 INFO      Number of epochs:         5000
20.04.2018 22:35:32 INFO      Training samples:         100000
20.04.2018 22:35:32 INFO      NC experiments:           False
20.04.2018 22:35:32 INFO      Debug mode:               False
20.04.2018 22:35:55 INFO    Reduced training sample size from 9999997 to 100000 (factor 100)
20.04.2018 22:36:10 INFO    Starting training
2018-04-20 22:36:10.792519: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 22:36:10.792557: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 22:36:10.792562: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 22:36:10.792567: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 22:36:10.792571: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-20 22:36:11.086929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-04-20 22:36:11.086966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-20 22:36:11.086972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-20 22:36:11.086980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 01006: early stopping
20.04.2018 23:54:13 INFO    Starting evaluation
20.04.2018 23:56:25 INFO    Starting theta 100 / 1017
20.04.2018 23:58:34 INFO    Starting theta 200 / 1017
21.04.2018 00:00:42 INFO    Starting theta 300 / 1017
21.04.2018 00:02:51 INFO    Starting theta 400 / 1017
21.04.2018 00:05:00 INFO    Starting theta 500 / 1017
21.04.2018 00:07:08 INFO    Starting theta 600 / 1017
21.04.2018 00:09:17 INFO    Starting theta 700 / 1017
21.04.2018 00:11:26 INFO    Starting theta 800 / 1017
21.04.2018 00:13:35 INFO    Starting theta 900 / 1017
21.04.2018 00:15:43 INFO    Starting theta 1000 / 1017
21.04.2018 00:16:06 INFO    Evaluation timing: median 1.27235913277 s, mean 1.27425283997 s
21.04.2018 00:16:06 INFO    Starting roaming
21.04.2018 00:16:32 INFO    Starting calibrated evaluation and roaming
21.04.2018 01:44:18 INFO    Starting theta 100 / 1017
21.04.2018 03:12:52 INFO    Starting theta 200 / 1017
21.04.2018 04:41:20 INFO    Starting theta 300 / 1017
21.04.2018 06:09:56 INFO    Starting theta 400 / 1017
21.04.2018 07:38:29 INFO    Starting theta 500 / 1017
21.04.2018 09:06:56 INFO    Starting theta 600 / 1017
21.04.2018 10:35:25 INFO    Starting theta 700 / 1017
21.04.2018 12:04:11 INFO    Starting theta 800 / 1017
21.04.2018 13:33:03 INFO    Starting theta 900 / 1017
