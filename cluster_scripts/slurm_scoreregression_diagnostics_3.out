Using TensorFlow backend.
16.03.2018 14:09:45 INFO    Hi! How are you today?
16.03.2018 14:09:45 INFO    Startup options:
16.03.2018 14:09:45 INFO      Algorithm:                     scoreregression
16.03.2018 14:09:45 INFO      Point by point:                False
16.03.2018 14:09:45 INFO      Morphing-aware mode:           False
16.03.2018 14:09:45 INFO      Smeared data:                  False
16.03.2018 14:09:45 INFO      Training sample:               baseline
16.03.2018 14:09:45 INFO      alpha:                         None
16.03.2018 14:09:45 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
16.03.2018 14:09:45 INFO      AFC epsilon:                   None
16.03.2018 14:09:45 INFO      Denominator theta:             False
16.03.2018 14:09:45 INFO      Neyman construction toys:      3
16.03.2018 14:09:45 INFO      Other options:                 ['deep']
16.03.2018 14:09:45 INFO      Base directory:                /home/jb6504/higgs_inference
16.03.2018 14:09:45 INFO      ML-based strategies available: True
16.03.2018 14:09:45 INFO    Starting score regression inference
16.03.2018 14:09:45 INFO    Options:
16.03.2018 14:09:45 INFO      Denominator theta:       denominator 3 = theta 498 = [-0.04796927 -0.24493299]
16.03.2018 14:09:45 INFO      Number of hidden layers: 5
16.03.2018 14:09:45 INFO      Batch size:              128
16.03.2018 14:09:45 INFO      Learning rate:           0.001
16.03.2018 14:09:45 INFO      Learning rate decay:     0.0921034037198
16.03.2018 14:09:45 INFO      Number of epochs:        50
16.03.2018 14:09:45 INFO      NC experiments:          False
16.03.2018 14:10:06 INFO    Starting training of score regression
Train on 8000000 samples, validate on 2000000 samples
2018-03-16 14:10:07.146077: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:07.146463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:07.146471: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:07.146475: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:07.146479: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-16 14:10:07.472060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla P40
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:04:00.0
Total memory: 22.38GiB
Free memory: 22.21GiB
2018-03-16 14:10:07.472094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-03-16 14:10:07.472100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-03-16 14:10:07.472108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
Epoch 1/50
334s - loss: 0.0633 - val_loss: 0.0544
Epoch 2/50
333s - loss: 0.0365 - val_loss: 0.0439
Epoch 3/50
330s - loss: 0.0289 - val_loss: 0.0294
Epoch 4/50
331s - loss: 0.0250 - val_loss: 0.0295
Epoch 5/50
330s - loss: 0.0230 - val_loss: 0.0278
Epoch 6/50
334s - loss: 0.0209 - val_loss: 0.0282
Epoch 7/50
329s - loss: 0.0178 - val_loss: 0.0268
Epoch 8/50
330s - loss: 0.0163 - val_loss: 0.0184
Epoch 9/50
329s - loss: 0.0145 - val_loss: 0.0205
Epoch 10/50
329s - loss: 0.0127 - val_loss: 0.0201
Epoch 11/50
329s - loss: 0.0113 - val_loss: 0.0185
Epoch 12/50
330s - loss: 0.0102 - val_loss: 0.0172
Epoch 13/50
330s - loss: 0.0092 - val_loss: 0.0142
Epoch 14/50
329s - loss: 0.0081 - val_loss: 0.0139
Epoch 15/50
329s - loss: 0.0074 - val_loss: 0.0136
Epoch 16/50
329s - loss: 0.0069 - val_loss: 0.0139
Epoch 17/50
329s - loss: 0.0065 - val_loss: 0.0118
Epoch 18/50
329s - loss: 0.0059 - val_loss: 0.0126
Epoch 19/50
329s - loss: 0.0056 - val_loss: 0.0114
Epoch 20/50
329s - loss: 0.0053 - val_loss: 0.0128
Epoch 21/50
