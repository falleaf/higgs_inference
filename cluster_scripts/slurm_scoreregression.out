Using TensorFlow backend.
11.03.2018 03:15:18 INFO    Hi! How are you today?
11.03.2018 03:15:18 INFO    Startup options:
11.03.2018 03:15:18 INFO      Algorithm:                     scoreregression
11.03.2018 03:15:18 INFO      Point by point:                False
11.03.2018 03:15:18 INFO      Morphing-aware mode:           False
11.03.2018 03:15:18 INFO      Smeared data:                  False
11.03.2018 03:15:18 INFO      Training sample:               baseline
11.03.2018 03:15:18 INFO      alpha:                         None
11.03.2018 03:15:18 INFO      Histogram / AFC X indices:     [1, 38, 39, 40, 41]
11.03.2018 03:15:18 INFO      AFC epsilon:                   None
11.03.2018 03:15:18 INFO      Neyman construction toys:      True
11.03.2018 03:15:18 INFO      Other options:                 ['deep', 'new']
11.03.2018 03:15:18 INFO      Base directory:                /home/jb6504/higgs_inference
11.03.2018 03:15:18 INFO      ML-based strategies available: True
11.03.2018 03:15:18 INFO    Starting score regression inference
11.03.2018 03:15:18 INFO    Options:
11.03.2018 03:15:18 INFO      Number of hidden layers: 5
11.03.2018 03:15:18 INFO      Batch size:              128
11.03.2018 03:15:18 INFO      Learning rate:           0.001
11.03.2018 03:15:18 INFO      Learning rate decay:     0.0921034037198
11.03.2018 03:15:18 INFO      Number of epochs:        50
11.03.2018 03:15:18 INFO      NC experiments:          (1001 alternate + 10000 null) experiments with 36 alternate events each
11.03.2018 03:15:39 INFO    Starting training of score regression
Train on 8000000 samples, validate on 2000000 samples
2018-03-11 03:15:39.676119: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 03:15:39.676866: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 03:15:39.676877: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 03:15:39.676882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 03:15:39.676886: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 03:15:40.000305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2018-03-11 03:15:40.000343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-03-11 03:15:40.000349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-03-11 03:15:40.000358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
Epoch 1/50
364s - loss: 0.0622 - val_loss: 0.0376
Epoch 2/50
363s - loss: 0.0379 - val_loss: 0.0308
Epoch 3/50
363s - loss: 0.0310 - val_loss: 0.0265
Epoch 4/50
364s - loss: 0.0280 - val_loss: 0.0193
Epoch 5/50
364s - loss: 0.0242 - val_loss: 0.0187
Epoch 6/50
364s - loss: 0.0219 - val_loss: 0.0224
Epoch 7/50
364s - loss: 0.0206 - val_loss: 0.0124
Epoch 8/50
364s - loss: 0.0171 - val_loss: 0.0112
Epoch 9/50
364s - loss: 0.0154 - val_loss: 0.0113
Epoch 10/50
365s - loss: 0.0142 - val_loss: 0.0127
Epoch 11/50
365s - loss: 0.0134 - val_loss: 0.0094
Epoch 12/50
364s - loss: 0.0119 - val_loss: 0.0077
Epoch 13/50
365s - loss: 0.0106 - val_loss: 0.0078
Epoch 14/50
365s - loss: 0.0098 - val_loss: 0.0063
Epoch 15/50
365s - loss: 0.0091 - val_loss: 0.0066
Epoch 16/50
364s - loss: 0.0085 - val_loss: 0.0050
Epoch 17/50
364s - loss: 0.0082 - val_loss: 0.0054
Epoch 18/50
365s - loss: 0.0079 - val_loss: 0.0043
Epoch 19/50
365s - loss: 0.0074 - val_loss: 0.0045
Epoch 20/50
364s - loss: 0.0069 - val_loss: 0.0041
Epoch 21/50
365s - loss: 0.0067 - val_loss: 0.0043
Epoch 22/50
365s - loss: 0.0064 - val_loss: 0.0030
Epoch 23/50
365s - loss: 0.0062 - val_loss: 0.0032
Epoch 24/50
365s - loss: 0.0059 - val_loss: 0.0037
Epoch 25/50
365s - loss: 0.0058 - val_loss: 0.0027
Epoch 26/50
365s - loss: 0.0056 - val_loss: 0.0025
Epoch 27/50
365s - loss: 0.0055 - val_loss: 0.0031
Epoch 28/50
365s - loss: 0.0054 - val_loss: 0.0027
Epoch 29/50
365s - loss: 0.0052 - val_loss: 0.0025
Epoch 30/50
364s - loss: 0.0051 - val_loss: 0.0028
Epoch 31/50
364s - loss: 0.0050 - val_loss: 0.0026
Epoch 32/50
364s - loss: 0.0050 - val_loss: 0.0025
Epoch 33/50
364s - loss: 0.0049 - val_loss: 0.0027
Epoch 34/50
365s - loss: 0.0048 - val_loss: 0.0032
Epoch 35/50
365s - loss: 0.0048 - val_loss: 0.0021
Epoch 36/50
365s - loss: 0.0047 - val_loss: 0.0025
Epoch 37/50
366s - loss: 0.0047 - val_loss: 0.0020
Epoch 38/50
366s - loss: 0.0046 - val_loss: 0.0023
Epoch 39/50
365s - loss: 0.0046 - val_loss: 0.0023
Epoch 40/50
365s - loss: 0.0045 - val_loss: 0.0020
Epoch 41/50
365s - loss: 0.0045 - val_loss: 0.0022
Epoch 42/50
364s - loss: 0.0045 - val_loss: 0.0021
Epoch 43/50
364s - loss: 0.0044 - val_loss: 0.0020
Epoch 44/50
364s - loss: 0.0044 - val_loss: 0.0019
Epoch 45/50
364s - loss: 0.0044 - val_loss: 0.0019
Epoch 46/50
364s - loss: 0.0044 - val_loss: 0.0019
Epoch 47/50
364s - loss: 0.0044 - val_loss: 0.0019
Epoch 48/50
365s - loss: 0.0043 - val_loss: 0.0018
Epoch 49/50
365s - loss: 0.0043 - val_loss: 0.0018
Epoch 50/50
364s - loss: 0.0043 - val_loss: 0.0018
11.03.2018 08:19:49 INFO    Starting evaluation
11.03.2018 08:19:56 INFO    Starting density estimation
11.03.2018 11:23:52 INFO    Starting theta 100 / 1017
11.03.2018 11:42:02 INFO    Starting theta 200 / 1017
11.03.2018 12:00:10 INFO    Starting theta 300 / 1017
11.03.2018 12:18:18 INFO    Starting theta 400 / 1017
11.03.2018 12:36:27 INFO    Starting theta 500 / 1017
11.03.2018 12:54:34 INFO    Starting theta 600 / 1017
11.03.2018 13:12:34 INFO    Starting theta 700 / 1017
11.03.2018 13:30:42 INFO    Starting theta 800 / 1017
11.03.2018 13:48:54 INFO    Starting theta 900 / 1017
11.03.2018 14:07:06 INFO    Starting theta 1000 / 1017
11.03.2018 14:10:22 INFO    That's it -- have a great day!
slurmstepd: error: Exceeded step memory limit at some point.
