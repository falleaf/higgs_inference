
\documentclass[a4paper,
	oneside,
	captions=nooneline, 
	fleqn, 
	parskip=half,
	%appendixprefix,
	%headsepline,
	bibliography=totoc,
	abstracton,
        %openright,
	%cleardoublepage=plain,
	11pt]{scrartcl}

\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}

\usepackage[mathlf,mathtabular,minionint]{MinionPro}
\usepackage[mathlf,mathtabular,sansmath]{MyriadPro}

\usepackage{color,xcolor}
\definecolor{dark-grey}{rgb}{0.5,0.5,0.5}
\definecolor{dark-red}{rgb}{0.8,0.0,0.0}
\definecolor{dark-blue}{rgb}{0.0,0.0,0.8}
\definecolor{dark-green}{rgb}{0.,0.65,0.}
\definecolor{highlight-color}{rgb}{0.8,0.0,0.0}

\usepackage{amsmath,dsfont,mathtools}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{multirow,booktabs,tabularx,float}
\usepackage[font=normalsize]{subfig}
\usepackage{slashed,braket}
\usepackage{feynmp-auto}
\usepackage[sort&compress]{natbib}
\PassOptionsToPackage{hyphens}{url}\usepackage[pdftex,hidelinks,linkcolor={dark-blue}, citecolor={dark-green}, urlcolor={dark-red},bookmarksopen,linktoc=all]{hyperref}
\usepackage{enumitem} % for user-defined bullets 
\usepackage{pdfpages} % allows to load cover page
\usepackage[stretch=15,shrink=15]{microtype} % Default settings: protrusion=true,factor=1000,expansion=true,auto=true,stretch=20,shrink=20
\usepackage{calc} % arithmetic expressions for lengths
\usepackage{mparhack} % avoids marginpars appearing on the wrong side, remove for final version
\usepackage{tabto}
% \usepackage{tikz}

\input{definitions.tex}

% feynmf stuff
\DeclareGraphicsRule{*}{mps}{*}{}
\unitlength = 1pt

% Itemize with smaller bullets
\setlist[itemize,1]{label=\raisebox{0.1ex}{\scriptsize$\bullet$}}

% less space around items
\setlist{itemsep=1.5pt plus 1.0pt minus 0.5pt,
parsep=1.5pt plus 1.0pt minus 0.5pt,
partopsep=3.0pt plus 1.0pt minus 1.0pt,
topsep=3.0pt plus 1.5pt minus 2.5pt}

% \emph for bold instead of italic
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother 

% always center floats, also smaller font for tables
\makeatletter
\g@addto@macro\@floatboxreset\centering\small
\makeatother

% pdf file info
\makeatletter
\hypersetup{pdftitle=\@title, pdfauthor=\@author}
\makeatother

% don't number subsubsections
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

% Avoid widows and orphans (default: 150)
\widowpenalty=500
\clubpenalty=500

% bugfix, use only from pdftex 1.40.15 on
\pdfsuppresswarningpagegroup=1

\bibliographystyle{jthesis2}
\bibpunct{[}{]}{,}{n}{,}{,}

\renewcommand\bibname{References}
\addto\captionsbritish{\renewcommand{\bibname}{References}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Higgs inference experiments}
\author{Johann Brehmer, Kyle Cranmer, Gilles Louppe, and Juan Pavez}
\date{\today}


\begin{document}

\begin{fmffile}{diagrams}

\maketitle

%\begin{abstract}
%\end{abstract}

\tableofcontents




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We apply different likelihood-free techniques including
\toolfont{carl}~\cite{Cranmer:2015bka, Louppe:2016aov} to estimate
likelihood ratios between dimension-six operator hypotheses in WBF
Higgs production in the four-lepton mode. Even at parton level and
leading order, this process has a 15-dimensional phase space, making
the likelihood intractable. But under some idealized assumptions about
the detector response, we can access the exact likelihood ratio for
the problem, allowing us to evaluate the performance of the different
approaches. This physics problem and the data set is described in more
detail in Section~\ref{sec:problem}.

Our different inference strategies are described in
Section~\ref{sec:strategies}, and the results can be found in
Section~\ref{sec:results}.

The appendices provide a detailed look at the point-by-point
strategies both for the full problem as well as for a simpler
two-dimensional inference problem.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Problem statement}
\label{sec:problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{WBF Higgs to four leptons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \fmfframe(0,15)(15,15){ %(L,T) (R,B)
    \begin{fmfgraph*}(150,70)
      \feynmansetup
      \fmfleft{i2,i1}
      \fmfright{o6,o5,o4,o3,o2,o1}
      \fmflabel{\small $q$}{i1}
      \fmflabel{\small $q$}{i2}
      \fmflabel{\small $q$}{o1}
      \fmflabel{\small $\ell^+$}{o2}
      \fmflabel{\small $\ell^-$}{o3}
      \fmflabel{\small $\ell^+$}{o4}
      \fmflabel{\small $\ell^-$}{o5}
      \fmflabel{\small $q$}{o6}
      \fmf{fermion,tension=4}{i1,v3}
      \fmf{fermion,tension=4}{i2,v4}
      \fmf{fermion,tension=2.5}{v3,o1}
      \fmf{fermion,tension=2.5}{v4,o6}
      \fmf{wiggly,label=\small $W$,, $Z$,label.side=right}{v3,v5}
      \fmf{wiggly,label=\small $W$,, $Z$,label.side=left}{v4,v5}
      \fmf{dashes,label=\small $h$,tension=0.5}{v5,v6}
      \fmf{wiggly,tension=0.3,label=\small $Z$,tension=0.3,label.side=right}{v7,v6}
      \fmf{wiggly,tension=0.3,label=\small $Z$,tension=0.3,label.side=right}{v6,v8}
      \fmf{fermion,tension=0.2}{o2,v7,o3}
      \fmf{fermion,tension=0.2}{o4,v8,o5}
      \fmfv{decoration.shape=circle,foreground=(0.8,,0.,,0.),decoration.size=5}{v5,v6}
    \end{fmfgraph*}
  }
  \caption{Feynman diagram for WBF Higgs production in the $4 \ell $
    mode. The red dots show the Higgs-gauge interactions affected by
    the dimension-six operators of our analysis.}
  \label{fig:information_wbf_4l_diag}
\end{figure}

We analyze WBF\,/\,VBF Higgs production with a Higgs decay into four
leptons:
%
\begin{equation}
  q q \to q q \; h \to q q \; Z Z \to q q \; \ell^+ \ell^- \; \ell^+ \ell^-
\end{equation}
%
with $\ell = e, \mu$, as shown in
\autoref{fig:information_wbf_4l_diag}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parameter space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The dominant signatures of heavy new physics are expected to be
described by two dimension-six operators. In the
Hagiwara-Ishihara-Szalapski-Zeppenfeld
conventions~\cite{Hagiwara:1993ck}, they read
%
\begin{equation}
  \lgr{} = \lgr{SM}
  + \frac {f_{W}} {\Lambda^2} \; \underbrace{\dfrac{\im g}{2} \, (D^\mu\phi)^\dagger \, \sigma^a \, D^\nu\phi \; W_{\mu\nu}^a}_{\ope{W}} \;
  {} - \frac {f_{WW}} {\Lambda^2} \;  \underbrace{\frac{g^2}{4} \, (\phisq) \; W^a_{\mu\nu} \, W^{\mu\nu\, a}}_{\ope{WW}}
\end{equation} 
%
with real Wilson coefficients $f_W$ and $f_{WW}$ and unknown new physics
scale $\Lambda$.

We parameterise these coefficients in a dimensionless form as
%
\begin{equation}
  \boldtheta = \frac {v^2} {\Lambda^2} \twovec{f_W} {f_{WW}} \,,
\end{equation}
%
where $v=246~\gev$ is the SM Higgs vev. The SM therefore corresponds
to $\boldtheta_{SM} = \boldzero$, and parameters should always be in
the range $-1 < |\theta_i| < 1$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Event generation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We generate event samples in exactly the same way as described in
Refs.~\cite{Brehmer:2016nyr, johann_thesis}, using
\toolfont{MadGraph~5}~\cite{Alwall:2014hca} and its add-on
\toolfont{MadMax}~\cite{Cranmer:2006zs, Plehn:2013paa,
  Kling:2016lay}. This combination allows us to generate weighted
event samples $\{ \boldx, p (\boldx | \boldtheta ) \}$ including the
full likelihood $ p (\boldx | \boldtheta )$ for arbitrary
$\boldtheta$.

For this particularly clean channel, shown in
\autoref{fig:information_wbf_4l_diag}, the backgrounds are not the
limiting factor, so we omit them for our toy study: a calculation with
\toolfont{MadGraph~5} shows that in the relevant phase-space region
the cross section of the dominant irreducible $ZZ^* \,jj$ background
is more than one order of magnitude smaller than the SM Higgs signal.
For this truth-level analysis we include neither a parton shower nor
detector effects: we assume that the four-momenta of the leptons and
quarks can be measured precisely, the latter in the form of jets. For
the leptons, we also assume that the charge and flavour is always
measured exactly.

% \begin{figure}
%   \includegraphics[width=0.45\textwidth]{figures/pointwise_tuning_2d/sample_weights.pdf}
%   \caption{Distribution of event weights for the SM hypothesis, for
%     the main training sample as well as for the final evaluation
%     sample}.
%   \label{fig:sample_weights}
% \end{figure}

We generate a sample of $5.55 \cdot 10^6$ weighted events, which we
randomly split into a main sample of $5.00 \cdot 10^6$ events for
training and intermediate testing plus a smaller sample of
$0.55 \cdot 10^6$ events for the final evaluation.

In these original samples, the weights are not are not always the same,
with individual events in the training sample carrying an individual probability
 to be drawn of up to $10^{-3}$. This is, of course, far from ideal ideal, but unfortunately
a consequence of using $\toolfont{MadMax}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Observables}
\label{sec:features}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We train classifiers and estimate the likelihood ratio $r(\boldx)$
based on different sets of kinematic features.
%
\begin{enumerate}
\item First, we consider a simple \emph{two-dimensional feature space}
  consisting of
  % 
  \begin{itemize}
  \item the transverse momentum of the hardest jet, $p_{T,j1}$, as well as
  \item the azimuthal angle between the two jets, $\Delta \phi_{jj}$.
  \end{itemize}
  % 
  These observables are known to be sensitive to the operators of
  interest. Broadly speaking, the jet $p_T$ distribution provides a
  probe of $\ope{W}$, while $\Delta \phi_{jj}$ is more sensitive to
  $\ope{WW}$~\cite{johann_thesis, Brehmer:2016nyr}.
%
\item We extend this to a \emph{medium set}, consisting of
  %
  \begin{itemize}
    \item the energy, transverse momentum, pseudorapidity $\eta$, and azimuthal angle
  $\phi$ of each of the two jets, ordered by $p_T$;
  \item the energy, transverse momentum, pseudorapidity $\eta$, and
    azimuthal angle $\phi$ of the four-lepton system (which reconstructs the Higgs);
  \item the invariant mass of the dijet system, $m_{jj}$;
  \item the separation in pseudorapidity between the two jets,
    $\Delta \eta_{jj}$; and
  \item the separation in azimuthal angle between the two jets,
    $\Delta \phi_{jj}$.
  \end{itemize}
  %
  This set of 15 observables completely characterises the production
  of an on-shell Higgs in weak boson fusion at leading order (which
  has a five-dimensional phase space). We aim to answer the question
  whether training \toolfont{carl} on these 15 observables is enough
  to estimate the true likelihood ratio based on the fully
  differential kinematics.
%
\item In a next step, we use the \emph{full kinematics} of the
  leading-order process including the Higgs decay. We parameterise the
  four-momenta of the four leptons and the two jets with their energy,
  transverse momentum, pseudorapidity $\eta$, and azimuthal angle
  $\phi$. In this way we include 24 variables, 9 more than the 15
  dimensions of the phase space at leading
  order.\footnote{$ (2+6) \cdot 4_{\text{four-momenta}} -
    4_{\text{$E$, $p$ conservation}} - 8_{\text{on-shell conditions}}
    - 4_{\text{beam directions}} - 1_{\text{overall $\phi$}} = 15$}
%
\item Finally, we add some \emph{derived variables} to this full set
  and check whether this improves the classifier performance. In
  addition to the 24 variables of the previous set, we include the
  following quantities:
  % 
  \begin{itemize}
  \item the invariant mass of the dijet system, $m_{jj}$;
  \item the separation in pseudorapidity between the two jets,
    $\Delta \eta_{jj}$;
  \item the separation in azimuthal angle between the two jets,
    $\Delta \phi_{jj}$;
  \item the invariant masses of the two reconstructed $Z$ bosons,
    $m_{Z1}$ and $m_{Z2}$;
  \item the energy, transverse momentum, pseudorapidity, and azimuthal
    angle of the four-lepton system; and finally 
  \item the energy, transverse momentum, pseudorapidity, and azimuthal
    angle of the reconstructed $Z$ bosons.
  \end{itemize}
  % 
  All in all, this makes 42 observables. We do not (yet) explicitly
  take into account the decay angles that characterize the
  $h \to 4\ell$ decay chain since they carry less information on
  $\ope{W}$ and $\ope{WW}$ than production-side
  observables~\cite{johann_thesis, Brehmer:2016nyr}.
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exact likelihood ratio}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

These samples $\{ \boldx, p (\boldx | \boldtheta ) \}$ allow for a
straightforward calculation of the truth-level likelihood ratio. When
the full set of kinematic variables is taken into account, it is given
by
%
\begin{equation}
  r_{\text{full}}(\boldx) = \frac {p(\boldx | \boldtheta_0)} {p(\boldx | \boldtheta_1)} \,.
\end{equation}
%
We compare the medium set of observables, the full kinematics, and the
full kinematics including derived observables to this exact ratio.

For the reduced two-dimensional feature space discussed in the
appendix, we first have to integrate (sum) over the unobserved
directions in phase space:
%
\begin{equation}
  r_{\text{2d}}(\boldv_0) =
  \dfrac
  {\sum\limits_{\boldx | \boldv(\boldx) \approx \boldv_0} p(\boldx | \boldtheta_0)}
  {\sum\limits_{\boldx' | \boldv(\boldx) \approx \boldv_0} p(\boldx' | \boldtheta_1)} \,.
\end{equation}
%
Here $\boldv \in \mathbf{R}^2$ are the two-dimensional observables,
$\boldx$ denote 15-dimensional phase-space vectors, and
$\boldv(\boldx) \approx \boldv_0$ means that the two-dimensional
observables calculated from $\boldx$ are (approximately) equal to
$\boldv_0$. This is realised by binning the two-dimensional feature
space and summing over all events $\boldx$ in the same bin. We use 20
equidistant bins for $\Delta \phi_{jj}$ times 25 bins for
$p_{T,j1}$. The jet momentum bins have a varying size: 10~\gev up to a
$p_T$ of 100~\gev, 20~\gev up to 200~\gev, 40~\gev up to 400~\gev,
80~\gev up to 1200~\gev, and an overflow bin for $p_T > 1200~\gev$. Note
that for simplicity we will later use $\boldx$ to label the two
observables of this process. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Methods}
\label{sec:strategies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Availability of score and likelihood ratio}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Kyle's tricks\dots


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inference strategies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%A first class of algorithms approaches the inference problem point by
%point in parameter space, learning the likelihood ratio
%$r (\boldx; \boldtheta_0, \boldtheta_1) \equiv p(\boldx |
%\boldtheta_0) / p(\boldx | \boldtheta_1)$
%between a fixed pair of hypotheses. Only in the end the different
%analysed pairs $(\boldtheta_0, \boldtheta_1)$ are compared and
%interpolated.

%Instead of estimating the likelihood ratio separately for different
%pairs $(\boldtheta_0, \boldtheta_1)$, we can also train one
%parameterized classifier or regressor
%$\hat{r}(\boldx; \boldtheta_0 , \boldtheta_1)$. This allows us to go
%beyond the likelihood-free setting and include the information from
%the score when it is available, or to use physics knowledge about the
%$\boldtheta$ dependence.

%\dots

% In the \emph{likelihood-free setting}, we use the normal \toolfont{carl}
% approach. We first define a ``raw'' version: for the mapping
% $\hat{r} (\hat{s})$ we simply assume a perfectly trained classifier,
% $\hat{s}(\boldx) = p(\boldx|\boldtheta_0) / ( p(\boldx|\boldtheta_0) +
% p(\boldx|\boldtheta_1) )$.  Second, we use a calibrated version, where
% we generate a new calibration histogram for each of the 1016 values of
% $\boldtheta_0$ used in the evaluation. For the calibration we always
% use the same (weighted) events, which is not possible in the most
% general likelihood-free setup (but may be in situations like the EFT
% described here).

% For the setup in which the \emph{score is tractable}, we define two
% parameterized estimators: one is just trained by regressing on the
% score data. The second uses a combination of \toolfont{carl}'s
% cross-entropy loss and the mean squared error of the score regression
% problem. A parameter $\alpha$ weights these two terms in the loss
% function, aiming for an approximately equal size of the two
% terms. Again, we consider a raw and a calibrated version of this
% setup.

% Finally, we define two regressors for the case in which the
% \emph{likelihood is tractable}: standard regression on $\log r$, both
% for the point-by-point and the parameterized setup. For the
% parameterized setup, we also use a combination of regression on
% $\log r$ and on the score. The regression models are not calibrated. 

\begin{itemize}
\item $\boldx$ available:
  \begin{description}
  \item[Approximate Bayesian\,/\,Frequentist Computation (ABC\,/\,AFC)] (AFC is equivalent to density estimation in $\boldx$ space)
  \item[carl] \ 
    \begin{itemize}
    \item Point by point in $\boldtheta$
    \item Parameterized
    \end{itemize}
  \item[Masked autoregressive flows] (not implemented yet)
  \item[Conditional GANs] with subsequent score regression (not implemented yet)
  \end{description}
%
\item $\boldx$ and scores available:
  \begin{description}
  \item[Local (SM) score regression] \ 
    \begin{itemize}
    \item Density estimation (``calibration'') in score space (allows to use asymptotics even when result is not one to one with true likelihood ratio)
    \item Density estimation (``calibration'') in $\boldt \cdot \Delta \boldtheta$ space
    \end{itemize}
  \item[Parameterized carl + score]
  \item[Flows + score] (not implemented yet)
  \end{description}
%
\item $\boldx$, scores, and likelihood ratios available:
  \begin{description}
  \item[Ratio regression] \ 
    \begin{itemize}
    \item Point by point in $\boldtheta$
    \item Parameterized
    \end{itemize}
  \item[Parameterized ratio regression + score]
  \end{description}
\end{itemize}
    




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibration methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider and compare different calibration strategies:
%
\begin{description}
  \item[Histogram:] \dots
  \item[Isotonic:] \dots
  \item[Parametric:] We also consider two parametric calibration procedures. First, there is sigmoid calibration (Platt's method)\dots
    %
    As an alternative we consider the strictly
    monotonic parametric function
    \begin{equation}
      s_{\text{calibrated}} (s_{\text{raw}} ; \alpha)
      = \frac { (1 - \alpha) s_{\text{raw}} }
      { (1 - \alpha) s_{\text{raw}} + \alpha (1 - s_{\text{raw}}) }
      \label{eq:analytic_calibration_function}
    \end{equation}
    %
    with parameter $0 < \alpha < 1$ as a calibration curve. The value of this parameter is
    determined by fitting
    Equation~\eqref{eq:analytic_calibration_function} to the raw and
    calibrated values of $\hat{s}$ of a different calibration
    method (for now, we are using histograms).
    %
  \item[Kernel density estimation (KDE):] \dots
  \item[Gradient-adaptive KDE:] \dots
\end{description}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training samples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider four different training samples. In the \emph{baseline} setup, the
training sample consists of 1000 pairs $(\boldtheta_0, \boldtheta_1)$.
%
\begin{equation}
  \boldtheta_1 = (0.393, 0.492)
  \label{eq:theta1}
\end{equation}
%
is fixed, while the values of $\boldtheta_0$ are chosen randomly (with
a flat distribution over $[-1,1]^2]$). For each of these pairs, we
draw 5\,000 events according to $\boldtheta_0$ and 5\,000 events
according to $\boldtheta_1$. This amounts to a total of $10^7$ events.

In our alternative ``\emph{random $\boldtheta$}'' training sample draw
independent values of $\boldtheta_0$ for each event, from a flat prior
in $[-1,1]^2$. $\boldtheta_1$ is fixed as before. The sample consists
of 9998227 events, 4998047 of which were drawn according to
$\boldtheta_1$.

Finally, for some scenarios we analyse a training sample where only
the 15 basis points of the morphing procedure are used as values of
$\boldtheta_0$. Again a total of $10^7$ events are used.

In all cases, our evaluation sample consists of $100\,000$ events
drawn according to the SM. We calculate the exact likelihood ratio for
these events for a total of 1016 values of $\boldtheta_0$, 1000 of
which are the same as those used in the first training sample. Again
we fix $\boldtheta_1$ as in Equation~\eqref{eq:theta1}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Estimator design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the \emph{baseline} model, our estimators consist of a fully
connected neural network. In this way, the network can learn
(almost) any dependence on $\boldtheta$ it wants.

In addition we consider a ``\emph{physics-aware model}'' that uses the
knowledge about the compositional structure of the likelihood function
(as in the morphing setup):
%
\begin{equation}
  \hat{r} (\boldx; \boldtheta) = \sum_i w_i(\boldtheta) \, \hat{r}_i (\boldx)
\end{equation}
%
where we have explicitly left out the dependence on $\boldtheta_1$,
which we always keep fixed. The sum goes over the 15 basis samples
used in the morphing setup, and the functions $w_i(\boldtheta)$ are
analytically known. Following this structure, our network consists of
15 independent estimators $\hat{r}_i (\boldx)$, weighted with the
$w_i(\boldtheta)$.

For all architectures, We use between one and three hidden layers of
100 units with $\tanh$ activation functions for the baseline model,
and two hidden layers of 50 units with $\tanh$ activation functions.
We implement all setups in \toolfont{keras} with a
\toolfont{TensorFlow} backend inside a \toolfont{sklearn} wrapper and
train for 20 epochs with early stopping. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Metrics}
\label{sec:metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\dots


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neyman construction and asymptotics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\dots



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Uncertainty diagnostics}
\label{sec:diagnostics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We are experimenting with diagnostic tools that might provide some
measure of the uncertainty of the estimators $\hat{r}(\boldx)$. There are different ideas:
%
\begin{enumerate}
\item \emph{Ensemble variance}: repeating the full calculation with
  different random seeds, one can extract an uncertainty measure from
  the ensemble variance. This would only capture statistical
  uncertainties, not a systematic bias. Obviously this increases the
  computational cost.
  % 
\item \emph{Denominator variation}: the relative log likelihood
  contours (compared to the best-fit point) should not depend on the
  choice of the denominator hypothesis
  $\boldtheta_1$~\cite{Cranmer:2015bka}. We can therefore use
  quantities like
  % 
  \begin{multline}
    \Delta \log \hat{r} (\boldx; \boldtheta_0, \boldtheta_1) 
    = \Bigl| (\log \hat{r} (\boldx; \boldtheta_0, \boldtheta_1) - \max_\boldtheta \log \hat{r} (\boldx; \boldtheta, \boldtheta_1) ) \\
      - (\log \hat{r}' (\boldx; \boldtheta_0, \boldtheta_1') - \max_\boldtheta \log \hat{r}' (\boldx; \boldtheta_0, \boldtheta_1') ) \Bigr| \,.
    \label{eq:diagnostics3}
  \end{multline}
  % 
  as a measure of systematic uncertainties. Probably such a number
  only presents a lower bound for the true uncertainty. Again, this
  increases the computational cost.
  % 
\item \emph{Toy experiments}: we perform a series of mock experiments
  with different numbers of events and calculate the variance in the
  distribution of $\log \hat{r}{\boldx}$ as a function of the number
  of events. We fit the variance as a function of the event number
  with
  % 
  \begin{equation}
    \var [\log \hat{r}(\boldx; \boldtheta_0, \boldtheta_1) ] (n) = a/n + b \,.
  \end{equation}
  % 
  Then $\sqrt{a/n}$ represents the statistical uncertainty from a
  limited number of events, while
  % 
  \begin{equation}
    \Delta \log \hat{r} (\boldx; \boldtheta_0, \boldtheta_1) = \sqrt{b}
    \label{eq:diagnostics1}
  \end{equation}
  % 
  might provide a measure of a residual systematic uncertainty in the
  expectation values with large $n$. There is no guarantee that this
  strategy works.
  % 
\item \emph{Identity check}: for $\boldtheta_0 = \boldtheta_1$ the log
  likelihood ratio should identically vanish. A non-zero value of
  % 
  \begin{equation}
    \Delta \log \hat{r} (\boldx) = \log \hat{r}(\boldx; \boldtheta_0 = \boldtheta_1, \boldtheta_1) 
    \label{eq:diagnostics2}
  \end{equation}
  % 
  might therefore hint at a systematic uncertainty. Again, there is no
  guarantee that this number says anything about the behaviour at other
  values of $\boldtheta_0$. Also, any constant bias in
  $ \log \hat{r} (\boldx) $ would cancel out in the likelihood contours,
  which consider the difference in the log likelihood with respect to
  the best-fit point.
%
\item \emph{Score expectation value}: we know that
  $E[\mathbf{t}(\boldx | \boldtheta_0) | \boldtheta_0] = 0$. Perhaps
  there is a way to translate non-zero expectation values to an
  uncertainty measure on $\log r(\boldx)$?
\end{enumerate}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Results}
\label{sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Tables~\ref{tbl:comparison1} to
\ref{tbl:comparison_score} compare the performance of all
different setups tested.

\begin{table}
  \footnotesize
  \begin{tabular}{ll rr rr rr}
    \toprule
    Algorithm & Setup & \multicolumn{2}{c}{$\MSE_\boldx \left[ \log  r(\boldx; \boldtheta_0, \boldtheta_1) \right]$}
    & \multicolumn{2}{c}{$\MSE_\boldtheta \left[ E[\log r(\boldx;
      \boldtheta, \boldtheta_{\text{den}})] \right]$} \\
    \cmidrule{3-4} \cmidrule{5-6}
    && $\boldtheta_0 = \boldtheta_{T}$ & $\boldtheta_0 = \boldtheta_{NT}$
      & $\boldtheta_{\text{den}} = \boldtheta_1$ & $\boldtheta_{\text{den}} = \hat{\boldtheta}_{\text{MLE}}$ \\
    \midrule
   AFC (density est.\ in $\boldx$) & 2d (2) &  &  &  & \\
   AFC (density est.\ in $\boldx$) & 5d (2) & \emph{0.176} & \emph{0.323} & \emph{32.9} & \emph{20.9}\\
   \midrule
   carl (PbP, raw) & PbP (2) & \emph{0.091} & \emph{0.147} & \emph{369.4} & \emph{269.6}\\
   \midrule
   carl (PbP, cal.) & PbP (2) & \emph{0.047} & \emph{0.072} & \emph{12.3} & \emph{8.3}\\
   \midrule
   carl (param., raw) & Baseline (2) & 0.098 & 0.167 & 191.2 & 12.9\\
    & Random $\boldtheta$ (2) & \emph{0.063} & \emph{0.122} & \emph{10.3} & \emph{6.1}\\
    & Physics-aware, baseline (2) &  &  & (93.8) & (83.3)\\
    & Physics-aware, basis (2) &  &  &  & \\
   \midrule
   carl (param., cal.) & Baseline (2) & 0.058 & 0.113 & \emph{0.9} & \emph{1.1}\\
    & Random $\boldtheta$ (2) & \emph{0.057} & \emph{0.112} & 2.8 & 2.8\\
    & Physics-aware, baseline (2) &  &  &  & \\
    & Physics-aware, basis (2) &  &  &  & \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different inference strategies, part 1.
    In this table we show likelihood-free strategies based on a
    point-by-point (PbP) or parameterized setup.
    The numbers in brackets denote the number of hidden layers in the
    neural networks.  We show two types metrics: the mean squared
    error of $\log r$ for a fixed pair of hypotheses
    over different phase-space points and the mean squared error of the expectation 
    value of $\log r$ for different pairs of hypotheses. For the first case,
    the benchmark point $\boldtheta_{T}$ was part of all training samples
    (except for the random $\boldtheta$ sample), while the benchmark point
    $\boldtheta_{NT}$ is unknown to the parameterized models (it was still part
    of the point-by-point training). In the latter
    case, the expectation value sums over $\boldx$ following the SM
    distribution.}
  \label{tbl:comparison1}
\end{table}


\begin{table}
  \footnotesize
  \begin{tabular}{ll rr rr rr}
    \toprule
    Algorithm & Setup & \multicolumn{2}{c}{$\MSE_\boldx \left[ \log  r(\boldx; \boldtheta_0, \boldtheta_1) \right]$}
    & \multicolumn{2}{c}{$\MSE_\boldtheta \left[ E[\log r(\boldx; \boldtheta, \boldtheta_{\text{den}})] \right]$} \\
    \cmidrule{3-4} \cmidrule{5-6}
    && $\boldtheta_0 = \boldtheta_{T}$ & $\boldtheta_0 = \boldtheta_{NT}$
      & $\boldtheta_{\text{den}} = \boldtheta_1$ & $\boldtheta_{\text{den}} = \hat{\boldtheta}_{\text{MLE}}$ \\
   \midrule
   carl + score (param., raw) & Baseline (2) & \emph{0.063} & \emph{0.070} & 219.6 & \emph{2.8}\\
    & Random $\boldtheta$ (2) & 0.101 & 0.190 & 130.7 & 42.5\\
    & Physics-aware, baseline (2) &  &  & (\emph{69.0}) & (58.3)\\
    & Physics-aware, basis (2) &  &  &  & \\
   \midrule
   carl + score (param., cal.) & Baseline (2) & \emph{0.020} & \emph{0.032} & \emph{0.5} & \emph{0.2}\\
    & Random $\boldtheta$ (2) & 0.065 & 0.145 & 20.9 & 18.8\\
    & Physics-aware, baseline (2) &  &  &  & \\
    & Physics-aware, basis (2) &  &  &  & \\
   \midrule
   SM score regression & Fixed density est. on $t$  (2) & 0.036 & 0.034 & \emph{2.4} & \emph{1.2}\\
    & Dynamic density est. on $t$  (2) & \emph{0.033} & 0.033 & 2.7 & 1.4\\
    & Density est. on $t \cdot \theta$  (2) & 0.049 & \emph{0.033} & 7.9 & 5.3\\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different inference strategies, part 2.
    In this table we show parameterized architectures that utilize the score information.
    The numbers in brackets denote the number of hidden layers in the
    neural networks.  We show two types metrics: the mean squared
    error of $\log r$ for a fixed pair of hypotheses
    over different phase-space points and the mean squared error of the expectation 
    value of $\log r$ for different pairs of hypotheses. For the first case,
    the benchmark point $\boldtheta_{T}$ was part of all training samples
    (except for the random $\boldtheta$ sample), while the benchmark point
    $\boldtheta_{NT}$ is unknown to the parameterized models (it was still part
    of the point-by-point training). In the latter
    case, the expectation value sums over $\boldx$ following the SM
    distribution. Results in brackets mean that some NaN values had to be ignored.}
  \label{tbl:comparison2}
\end{table}


\begin{table}
  \footnotesize
  \begin{tabular}{ll rr rr rr}
    \toprule
    Algorithm & Setup & \multicolumn{2}{c}{$\MSE_\boldx \left[ \log  r(\boldx; \boldtheta_0, \boldtheta_1) \right]$}
    & \multicolumn{2}{c}{$\MSE_\boldtheta \left[ E[\log r(\boldx; \boldtheta, \boldtheta_{\text{den}})] \right]$} \\
    \cmidrule{3-4} \cmidrule{5-6}
    && $\boldtheta_0 = \boldtheta_{T}$ & $\boldtheta_0 = \boldtheta_{NT}$
      & $\boldtheta_{\text{den}} = \boldtheta_1$ & $\boldtheta_{\text{den}} = \hat{\boldtheta}_{\text{MLE}}$ \\
    \midrule
   Ratio regression (PbP, raw) & PbP (2) & \emph{0.004} & \emph{0.009} & \emph{10.8} & \emph{12.1}\\
   \midrule
   Ratio regression (param., raw) & Baseline (2) & 0.014 & \emph{0.025} & 2.2 & 2.2\\
    & Random $\boldtheta$ (2) & \emph{0.013} & 0.029 & \emph{0.9} & \emph{0.9}\\
    & Physics-aware, baseline (2) & 0.277 & 0.457 & 32.4 & 53.2\\
    & Physics-aware, basis (2) &  &  & 9267.4 & 17490.1\\
   \midrule
   Ratio regr.\ + score (param., raw) & Baseline (2) & \emph{0.009} & \emph{0.017} & 2.6 & \emph{0.7}\\
    & Random $\boldtheta$ (2) & 0.013 & 0.027 & \emph{1.2} & 1.2\\
    & Physics-aware, baseline (2) &  &  & 195.2 & 194.7\\
    & Physics-aware, basis (2) &  &  &  & \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different inference strategies, part 3.
    In this table we show point-by-point (PbP) and parameterized
    architectures that require a tractable likelihood.
    The numbers in brackets denote the number of hidden layers in the
    neural networks.  We show two types metrics: the mean squared
    error of $\log r$ for a fixed pair of hypotheses
    over different phase-space points and the mean squared error of the expectation 
    value of $\log r$ for different pairs of hypotheses. For the first case,
    the benchmark point $\boldtheta_{T}$ was part of all training samples
    (except for the random $\boldtheta$ sample), while the benchmark point
    $\boldtheta_{NT}$ is unknown to the parameterized models (it was still part
    of the point-by-point training). In the latter
    case, the expectation value sums over $\boldx$ following the SM
    distribution.  Results in brackets mean that some NaN values had to be ignored.}
  \label{tbl:comparison3}
\end{table}


\begin{table}
  \footnotesize
  \begin{tabular}{ll rr rr}
    \toprule
    Algorithm & Setup & \multicolumn{2}{c}{$\MSE_\boldx \left[ \boldt (\boldx; \boldtheta_0, \boldtheta_1) \right]$} \\
    \cmidrule{3-4} 
    && $\boldtheta_0 = \boldtheta_{T}$ & $\boldtheta_0 = \boldtheta_{NT}$ \\
    \midrule
   carl & Baseline (2) & \emph{2.89} & \emph{759.03}\\
    & Random $\boldtheta$ (2) & 3.06 & 759.45\\
    & Physics-aware, baseline (2) &  & \\
    & Physics-aware, basis (2) &  & \\
   \midrule
   carl + score & Baseline (2) & \emph{1.37} & \emph{755.46}\\
    & Random $\boldtheta$ (2) & 2.44 & 759.26\\
    & Physics-aware, baseline (2) &  & \\
    & Physics-aware, basis (2) &  & \\
   \midrule
   Ratio regression & Baseline (2) & 10.08 & 767.38\\
    & Random $\boldtheta$ (2) & \emph{4.52} & \emph{760.64}\\
    & Physics-aware, baseline (2) & 509.49 & 857.51\\
    & Physics-aware, basis (2) &  & \\
   \midrule
   Ratio regr.\ + score & Baseline (2) & \emph{1.42} & \emph{755.32}\\
    & Random $\boldtheta$ (2) & 1.60 & 756.35\\
    & Physics-aware, baseline (2) &  & \\
    & Physics-aware, basis (2) &  & \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different inference strategies, part 4.
    In this table we compare the estimation of the scores evaluated at arbitrary thetas in the
    different parameterized models. The benchmark point $\boldtheta_{T}$
    was part of all training samples (except for the random $\boldtheta$ sample),
    while the benchmark point $\boldtheta_{NT}$ is unknown to the parameterized
    models.
    The numbers in brackets denote the number of hidden layers in the
    neural networks.  Results in brackets mean that some NaN values had to be ignored.}
  \label{tbl:comparison_score}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We show the results of the parameterized baseline experiments in
Figures~\ref{fig:baseline_r_histo} to
\ref{fig:baseline_expected_llr_contours}. We find that
calibration is important, and that the combinatino of
$\toolfont{carl}$ with score information works well. The tested
diagnostic tool for uncertainties, unfortunately, does not seem to be
particularly useful: it wrongly assisngs sizeable errors to the ground
truth.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/r_histograms_vanilla.pdf}%
  \caption{Likelihood ratio histograms for individual phase-space points
    $\boldx$, for one particular hypothesis comparison
    $(\boldtheta_0, \boldtheta_1)$. We show the baseline version of
    the different models.}
  \label{fig:baseline_r_histo}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/r_scatter_vanilla.pdf}%
  \caption{Likelihood ratio scatter plot for individual phase-space points
    $\boldx$, for one particular hypothesis comparison
    $(\boldtheta_0, \boldtheta_1)$. We show the baseline version of
    the different models.}
  \label{fig:baseline_r_scatter}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/r_error_histograms_vanilla.pdf}%
  \caption{Histogram of the error on $\log r$ for one particular hypothesis comparison
    $(\boldtheta_0, \boldtheta_1)$. We show the baseline version of
    the different models.}
  \label{fig:baseline_r_error_histograms}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/score_scatter_vanilla.pdf}%
  \caption{Score scatter plots for individual phase-space points $\boldx$, for one
    particular hypothesis comparison $(\boldtheta_0, \boldtheta_1)$.
    We show the baseline version of the different parameterized
    models.}
  \label{fig:baseline_score_scatter}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/theta_dependence_comparison_vanilla.pdf}%
  \caption{Exact and estimated likelihood ratios as a function of
    $\boldtheta_0$, for four different individual events $\boldx$
    (columns). We show the baseline version of the different
    parameterized models (rows).}
  \label{fig:baseline_theta_dependence}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/expected_likelihood_scatter_vanilla.pdf}%
  \caption{Scatter plot of the expected likelihood ratios for
    different values of $\boldtheta_0$, taking the expectation value
    over $\boldx$. We show the baseline version of the different
    parameterized models.}
  \label{fig:baseline_expected_llr_scatter}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/expected_likelihood_errors_over_theta_vanilla.pdf}%
  \caption{Error in estimating the expected log likelihood ratio over $\boldtheta_0$,
    taking the expectation value
    over $\boldx$. For the point-by-point (PbP) versions, the larger dots show the points
    where the estimators are trained (the smaller dots are interpolated). We show the baseline version of the different
    parameterized models.}
  \label{fig:baseline_expected_llr_errors}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/results/likelihood_contours_vanilla.pdf}%
  \caption{Contours of the expected likelihood ratio at
    $-2 \Delta \log r = 4$ (solid) and $16$ (dashed). The offset from the SM is due to a sanitization
    procedure in the preprocessing of the evaluation sample and will be corrected in future runs. We show the baseline version of
    the different models.}
  \label{fig:baseline_expected_llr_contours}
\end{figure}

The plots shown are based on fully connected neural networks with
three hidden layers of 100 units. We are currently experimenting with
more shallow or deeper networks.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibration strategies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Table~\ref{tbl:calibration_strategies} we compare the performance
of different calibration strategies. The corresponding calibration
curves are shown in Figures~\ref{fig:calibration_curves1} and \ref{fig:calibration_curves2}.

\begin{table}
  \small
  \begin{tabular}{ll rr}
    \toprule
    Calibration strategy & Settings & \multicolumn{2}{c}{$\MSE_\boldx \left[ \log  r(\boldx; \boldtheta_0, \boldtheta_1) \right]$} \\
    \cmidrule{3-4} 
                         && PbP regression & PbP carl \\
   \midrule
   No calibration &  & $\mathbf{0.0241}$ & $\mathbf{0.272}$ \\
   \midrule
   Histogram & 25 equidistant bins & $0.0277$ & $0.040$ \\
    & 50 equidistant bins & $0.0259$ & $0.038$ \\
    & 100 equidistant bins & $0.0255$ & $0.038$ \\
    & 200 equidistant bins & $0.0255$ & $0.038$ \\
    & 500 equidistant bins & $0.0264$ & $0.039$ \\
    & 25 equidistant bins, linear interpolation & $0.0278$ & $0.039$ \\
    & 50 equidistant bins, linear interpolation & $0.0261$ & $0.038$ \\
    & 100 equidistant bins, linear interpolation & $0.0254$ & $0.038$ \\
    & 200 equidistant bins, linear interpolation & $0.0254$ & $0.038$ \\
    & 500 equidistant bins, linear interpolation & $0.0260$ & $0.038$ \\
    & 25 equidistant bins, spline interpolation & $0.0272$ & $0.038$ \\
    & 50 equidistant bins, spline interpolation & $0.0258$ & $\mathbf{0.038}$ \\
    & 100 equidistant bins, spline interpolation & $\mathbf{0.0253}$ & $0.038$ \\
    & 200 equidistant bins, spline interpolation & $0.0258$ & $0.050$ \\
    & 500 equidistant bins, spline interpolation & $0.0279$ & $0.051$ \\
    & 25 variable bins & $0.0362$ & $0.045$ \\
    & 50 variable bins & $0.0286$ & $0.039$ \\
    & 100 variable bins & $0.0442$ & $0.054$ \\
    & 200 variable bins & $0.0437$ & $0.054$ \\
    & 500 variable bins & $0.0422$ & $0.049$ \\
   \midrule
   Isotonic & no interpolation & $0.0247$ & $\mathbf{0.037}$ \\
    & linear interpolation & $\mathbf{0.0245}$ & $0.037$ \\
   \midrule
   Parametric & sigmoid & $0.0406$ & $0.042$ \\
    &  Equation~\eqref{eq:analytic_calibration_function} & $\mathbf{0.0238}$ & $\mathbf{0.038}$ \\
   \midrule
   KDE & fixed bandwidth & $\mathbf{0.0418}$ & $0.058$ \\
    & adaptive ($\alpha=0.25$) & $0.0439$ & $0.058$ \\
    & adaptive ($\alpha=0.5$) & $0.0458$ & $0.056$ \\
    & adaptive ($\alpha=0.75$) & $0.0474$ & $0.053$ \\
    & adaptive ($\alpha=1$) & $0.0545$ & $\mathbf{0.052}$ \\
   \midrule
   KDE & grad-adaptive ($\beta=0.2$) & $\mathbf{0.0282}$ & $0.040$ \\
    & grad-adaptive ($\beta=0.5$) & $0.0304$ & $\mathbf{0.039}$ \\
    & grad-adaptive ($\beta=1$) & $0.0331$ & $0.042$ \\
    & grad-adaptive ($\beta=2$) & $0.0439$ & $0.049$ \\
    & grad-adaptive ($\beta_{\text{num}} = 1.66$, $\beta_{\text{den}} = 1.13$) & $0.0326$ & $0.040$ \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different calibration methods.
    As test scenario we use two inference strategies, carl and regression, both in a
    point-by-point setup for one particular benchmark $\boldtheta_0$.}
  \label{tbl:calibration_strategies}
\end{table}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/calibration/calibration_curves.pdf}%
  \caption{Calibration curves $r_{\text{calibrated}}(s_{\text{raw}})$ for different calibration
    strategies. The parametric approach is defined in
    Equation~\eqref{eq:analytic_calibration_function}.  Top:
    point-by-point regression. Bottom: point-by-point carl.}
  \label{fig:calibration_curves1}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/calibration/calibration_s_curves.pdf}%
  \caption{Calibration curves $s_{\text{calibrated}}(s_{\text{raw}})$ for different calibration
    strategies. The parametric approach is defined in
    Equation~\eqref{eq:analytic_calibration_function}.  Top:
    point-by-point regression. Bottom: point-by-point carl.}
  \label{fig:calibration_curves2}
\end{figure}

Instead of learning the classifier output $s(\boldx)$, we can learn the score $\boldt(\boldx)$,
which is the sufficient statistics in the local model, and perform density estimation in either
$\boldtheta$ space or in one-dimensional $\Delta \boldtheta \cdot \boldtheta$ space. In
Table~\ref{tbl:score_density_estimation_strategies} and Figure~\ref{fig:score_density_estimation}
we compare different strategies.

\begin{table}
  \small
  \begin{tabular}{ll r}
    \toprule
    Density estimation strategy & Settings & $\MSE_\boldx \left[ \log  r(\boldx; \boldtheta_0, \boldtheta_1) \right]$ \\
   \midrule
    $t$ histogram & $10^2$ equidistant bins & $0.229$ \\
    & $20^2$ equidistant bins & $0.211$ \\
    & $30^2$ equidistant bins & $0.102$ \\
    & $40^2$ equidistant bins & $0.108$ \\
    & $42^2$ optimized bins & $\mathbf{0.010}$ \\
   \midrule
   $t$ KDE & fixed bandwidth & $0.174$ \\
    & adaptive ($\alpha=0.25$) & $0.127$ \\
    & adaptive ($\alpha=0.5$) & $0.102$ \\
    & adaptive ($\alpha=0.75$) & $0.089$ \\
    & adaptive ($\alpha=1$) & $\mathbf{0.086}$ \\
   \midrule
   Local model fit &  & $\mathbf{0.057}$ \\
   \midrule
   $t \cdot \Delta \theta$ histogram & 25 equidistant bins & $0.124$ \\
    & 50 equidistant bins & $0.041$ \\
    & 100 equidistant bins & $0.024$ \\
    & 200 equidistant bins & $0.012$ \\
    & 500 equidistant bins & $0.009$ \\
    & 25 equidistant bins, linear interpolation & $0.164$ \\
    & 50 equidistant bins, linear interpolation & $0.045$ \\
    & 100 equidistant bins, linear interpolation & $0.025$ \\
    & 200 equidistant bins, linear interpolation & $0.010$ \\
    & 500 equidistant bins, linear interpolation & $0.008$ \\
    & 25 equidistant bins, spline interpolation & $0.388$ \\
    & 50 equidistant bins, spline interpolation & $0.066$ \\
    & 100 equidistant bins, spline interpolation & $0.032$ \\
    & 200 equidistant bins, spline interpolation & $0.010$ \\
    & 500 equidistant bins, spline interpolation & $\mathbf{0.008}$ \\
    & 25 variable bins & $0.029$ \\
    & 50 variable bins & $0.014$ \\
    & 100 variable bins & $0.010$ \\
    & 200 variable bins & $0.009$ \\
    & 500 variable bins & $0.008$ \\
    & 146 optimized bins & $0.008$ \\
   \midrule
   $t \cdot \Delta \theta$ KDE & fixed bandwidth & $0.124$ \\
    & adaptive ($\alpha=0.25$) & $0.090$ \\
    & adaptive ($\alpha=0.5$) & $0.068$ \\
    & adaptive ($\alpha=0.75$) & $0.053$ \\
    & adaptive ($\alpha=1$) & $\mathbf{0.046}$ \\
   \midrule
   $t \cdot \Delta \theta$ KDE & grad-adaptive ($\beta=0.2$) & $\mathbf{0.035}$ \\
    & grad-adaptive ($\beta=0.5$) & $0.046$ \\
    & grad-adaptive ($\beta=1$) & $0.043$ \\
    & grad-adaptive ($\beta=2$) & $0.046$ \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of different density estimation strategies after estimating the score.}
  \label{tbl: score_density_estimation_strategies}
\end{table}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/score_density_estimation/density_estimation_curves.pdf}%
  \caption{Comparison of different density estimation strategies after estimating the score.}
  \label{fig:score_density_estimation}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Physics-aware setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The physics-aware setup is based on the analytic
form of the morphing weights $w_i (\boldtheta)$. These factors
crucially depend on the basis points in the morphing procedure, as we
show in Figures~\ref{fig:morphing_weights1} and
\ref{fig:morphing_weights2}. Here we stick to the choice
shown in Figure~\ref{fig:morphing_weights2}, which
reduces the weights and the cancellations between them compared to
other basis choices. For now we only analyse the training sample
consisting only of these basis values of $\boldtheta_0$.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/morphing/morphing_original.pdf}%
  \caption{Morphing weights $w_i(\boldtheta)$ for basis points chosen
    only in one quadrant of the parameter space of interest.}
  \label{fig:morphing_weights1}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/morphing/morphing_optimized2.pdf}%
  \caption{Morphing weights $w_i(\boldtheta)$ for basis points
    distributed over the full relevant parameter space.}
  \label{fig:morphing_weights2}
\end{figure}

\dots



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Conclusions}
\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\dots



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix.tex}

\clearpage
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{fmffile}

\end{document}
