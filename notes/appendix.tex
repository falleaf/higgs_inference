%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Two-dimensional likelihood}
\label{sec:appendix_2d}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Instead of estimating the density over the full 15-dimensional
phase-space, we also check some of the approaches on the much simpler
problem of estimating the density of the two-dimensional distribution
$p(p_{T,j1}, \Delta \phi_{jj} ; \boldtheta)$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmark hypothesis test}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For two benchmark parameter points
%
\begin{equation}
  \boldtheta_0 = \twovec{-0.2} {-0.2} \,, \quad
  \boldtheta_1 = \twovec{0.2} {0.2}
  \label{eq:pointwise_tuning_benchmarks}
\end{equation}
%
we first analyse how well \toolfont{carl} can estimate the likelihood ratio
%
\begin{equation}
  r(\boldx) = \frac {p(\boldx | \boldtheta_0)}  {p(\boldx | \boldtheta_1)}
  \label{eq:pointwise_tuning_r}
\end{equation}
%
depending on the classifier, its hyperparameters, and a number of
different settings.

For each of the four feature sets outlined above (2D, medium set, full
kinematics, full kinematics plus derived variables), we tune the
parameters in two steps. First, we perform a randomized scan over
the hyperparameters, maximizing the classification power between event
samples sampled from $p(\boldx | \boldtheta_0)$ and
$p(\boldx | \boldtheta_1)$. In a second step we finetune these
parameters on the mean squared error between the \toolfont{carl}
estimate $\log \hat{r}(\boldx)$ and the true value $\log r(\boldx)$.

The first classifier we consider is a \emph{random forest}, more
precisely extremely randomized trees, in the
\toolfont{sklearn.ensemble.ExtraTreesClassifier} implementation. The
following parameters are tuned:
%
\begin{itemize}
  \item \toolfont{n\_estimators}, the number of trees in the forest;
  \item \toolfont{max\_features}, the number of features considered in
    the search for the best split;
  \item \toolfont{max\_depth}, the maximum depth of the trees;
  \item \toolfont{min\_samples\_split}, the minimum fraction of events
    at a node required for a split; and
  \item \toolfont{min\_samples\_leaf}, the minimum fraction of events
    in a leaf.
\end{itemize}

A \emph{multi-layer perceptron} implemented as a
\toolfont{sklearn.neural\_network.MLPClassifier} is our second
classifier. We optimize the following parameters:
%
\begin{itemize}
  \item \toolfont{hidden\_layer\_sizes}, describing the number of hidden
  layers and the number of neurons in each layer;
  \item \toolfont{activation}, the activation function; and
  \item \toolfont{alpha}, an $L^2$ penalty term.
\end{itemize}

For both classifiers, the kinematic features are first rescaled to a
normal distribution with a
\toolfont{sklearn.preprocessing.StandardScaler}.



\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/r_over_x.pdf}
  \caption{Truth likelihood ratio $r(\boldx)$ as defined in
    \autoref{eq:pointwise_tuning_r} for the 2D case. The effect of the
    binning in the calculation of $r_{\text{2d}}(\boldx)$ is clearly
    visible.}
  \label{fig:pointwise_tuning_2d_r_x}
\end{figure}

First we only consider two kinematic features, the leading jet
momentum and the azimuthal angle between the two
jets. \autoref{fig:pointwise_tuning_2d_r_x} shows how the truth
likelihood ratio between the two points defined in
\autoref{eq:pointwise_tuning_benchmarks} depends on these observables.

\begin{table}
\begin{tabular}{ll rrr}
  \toprule 
  Classifier & Parameter & Range & Good & Best \\
  \midrule
  Random forest & \toolfont{n\_estimators} & $50 \dots 200$ & $50 \dots 200$  & $100$ \\
  & \toolfont{max\_features} & $1, 2$ & $2$ & $2$ \\
  & \toolfont{max\_depth} & $1 \dots 20, \infty$ & $6\dots 8$ & $8$ \\
  & \toolfont{min\_samples\_split} & $0 \dots 1$ & $10^{-4} \dots 10^{-3}$ & $10^{-3}$ \\
  & \toolfont{min\_samples\_leaf} & $0 \dots 0.5$ & $0$ & $0$ \\
  \midrule
  Neural network & number of hidden layers & $1,2,3$ & $2$ & $2$\\
  & neurons at each layer & $2\dots 20$ per layer & $(20,20)$ & $(20,20)$\\
  & activation function & $\tanh, \relu, \logistic$ & $\tanh, \relu$ & $\tanh$ \\
  & $\alpha$ & $0\dots 100$ & $0\dots 1$ & $10^{-3}$\\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the classification problem between
  $\boldtheta_0$ and $\boldtheta_1$ for the 2D case. For both classifier
  and each parameter we show the considered range, the range of values
  with good results (\ie ROC AUC close to the optimum),
  and the (rounded) optimal value. }
 \label{tbl:pointwise_tuning_2d_parameters}
\end{table}

We first tune hyperparameters with a randomized scan on the
classification problem between unweighted event samples drawn from
$p(\boldx | \boldtheta_0)$ and $p(\boldx | \boldtheta_1)$. Using
\toolfont{sklearn.model\_selection.RandomizedSearchCV}, we optimize on
the ROC AUC. We give the optimal parameters in
\autoref{tbl:pointwise_tuning_2d_parameters}. Key to a good
performance for the random forest is a maximal depth around 8. Only
considering one feature at each splitting or requiring a large number
of events for a splitting or at each leaf lead to a worse
performance. Above a certain threshold, the number of trees does not
have a huge effect. For the neural network, two hidden layers with 20
neurons each and tanh or ReLU activation functions perform best, while
the value of the regulator $\alpha$ is not very important.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_over_x_grid_rf.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_vs_r_rf.pdf}%
  \caption{Likelihood ratio estimation with the tuned random forest
    for the 2D case. Left: Distribution of
    \toolfont{carl}'s estimate $\hat{r}(\boldx)$ as a function of the
    observables. Right: scatter plot between the true $r(\boldx)$ and
    the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_tuning_2d_rf_performance}
\end{figure}

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_over_x_grid_mlp.pdf}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_vs_r_mlp.pdf}
  \caption{Likelihood ratio estimation with the tuned neural network
    for the 2D case. Left: Distribution of
    \toolfont{carl}'s estimate $\hat{r}(\boldx)$ as a function of the
    observables. Right: scatter plot between the true $r(\boldx)$ and
    the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_tuning_2d_mlp_performance}
\end{figure}

In Figures~\ref{fig:pointwise_tuning_2d_rf_performance} and
\ref{fig:pointwise_tuning_2d_mlp_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_tuning_2d_parameters}.

Next, we vary all parameters one by one and see how this affects the
mean squared error of $\log \hat{r}(\boldx)$. The results are shown in
Figures~\ref{fig:pointwise_tuning_2d_rf_tuning} to
\ref{fig:pointwise_tuning_2d_carl_tuning}.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_rf_n_estimators.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_rf_max_depths.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_rf_min_samples_leaf.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_rf_min_samples_splits.pdf}%
  \caption{carl performance as a function of the random forest
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points for the 2D
    case. Each data point shows the mean of five calculations, the
    error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_2d_rf_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_mlp_hidden_layer_sizes.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_mlp_activation.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_mlp_alpha.pdf}%
  \caption{carl performance as a function of the neural network
    hyperparameters. Mean squred error of the estimated log likelihood
    ratio between two benchmark points for the 2D case. Each data
    point shows the mean of five calculations, the error bars are
    $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_2d_mlp_tuning}
\end{figure}

The parameters that are best for the classification problem also work
best for the estimation of the likelihood ratio with
$\toolfont{carl}$. The performance of the random forest crucially
depends on the maximal depth, while the neural network works well with
pretty much any setup, as long as each layer has enough neurons. To
reduce training time, we therefore define a final setup of the neural
network with a reduced number of 8 neurons in each hidden layer. For
the random forest, we stick to the parameters in the right column of
\autoref{tbl:pointwise_tuning_2d_parameters}.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_training_sample_size.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_carl_bins.pdf}%
  \caption{carl performance as a function of the training sample size
    (left) and the number of bins in the \toolfont{carl} calibration
    histograms.  We show the mean squred error of the estimated log
    likelihood ratio between two benchmark points for the 2D
    case. Each data point shows the mean of five calculations, the
    error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_2d_carl_tuning}
\end{figure}

\autoref{fig:pointwise_tuning_2d_carl_tuning} shows the effect of the
size of the training samples as well as the binning of
\toolfont{carl}'s calibration histograms.  Regardless of the
problematic distribution of weights in the input samples (see
\autoref{fig:sample_weights}), the performance becomes much better
with larger training samples. We pick a size of 200\,000 events as a
balance between computation time and performance. The calibration
histograms should have at least 20 bins, and variable bin widths do
not seem to work well at all.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_weighted_events.pdf}%
  \caption{carl performance with weighted training samples. We show
    the mean squred error of the estimated log likelihood ratio between
    two benchmark points based on the two-dimensional feature
    space.}
  \label{fig:pointwise_tuning_2d_weighted}
\end{figure}

As demonstrated in \autoref{fig:pointwise_tuning_2d_weighted},
training random forests with weighted samples does not improve the
performance, even if the full training sample of 5 million events is
used. However, we did not try to tune the random forest parameters
with weighted input.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_final.pdf}%
  \caption{carl performance for the 2D case with \toolfont{sklearn}
    default hyperparameters, the best parameters according to the
    initial hyperparameter scan, and the final parameters as defined
    in the text. We show the mean squred error of the estimated log
    likelihood ratio between two benchmark points. Each data point
    shows the mean of five calculations, the error bars are $95\%$
    confidence intervals.}
  \label{fig:pointwise_tuning_2d_final}
\end{figure}

We summarise \toolfont{carl}'s performance before and after tuning and
compare the two classifiers in
\autoref{fig:pointwise_tuning_2d_final}. We find that the neural
network performs slightly better, and is much less dependent on the
choice of parameters. The output of \toolfont{carl} with this final
neural network setup is shown in
\autoref{fig:pointwise_tuning_2d_final_performance}. As a matter of
fact, the smooth neural net output might be closer to the real
likelihood ratio than our calculated $r(\boldx)$ with its binning
artefacts.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_over_x_grid_final.pdf}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_vs_r_final.pdf}
  \caption{Likelihood ratio estimation with the final neural network
    for the 2D case. Left: Distribution of \toolfont{carl}'s estimate
    $\hat{r}(\boldx)$ as a function of the observables. Right: scatter
    plot between the true $r(\boldx)$ and the estimate
    $\hat{r}(\boldx)$}.
  \label{fig:pointwise_tuning_2d_final_performance}
\end{figure}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After tuning the parameters for one specific choice of benchmark
points, we now check how well these settings work for the likelihood ratio
between two different benchmark points
%
\begin{equation}
  \boldtheta_0' = \twovec{0.5} {-0.5} \,, \quad
  \boldtheta_1' = \twovec{-0.5} {0.5} \,.
  \label{eq:pointwise_validation_benchmarks}
\end{equation}

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/r_over_x_grid_alt.pdf}
  \caption{Truth likelihood ratio $r(\boldx)$ between the validation
    benchmark points for the 2D case as a function of the
    observables.}
  \label{fig:pointwise_validation_2d_r_x}
\end{figure}

\autoref{fig:pointwise_validation_2d_r_x} shows the truth likelihood
ratio between the validation benchmark points of
\autoref{eq:pointwise_validation_benchmarks} as a function of the two
observables.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_over_x_grid_rf_alt.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_vs_r_rf_alt.pdf}%
  \caption{Likelihood ratio estimation with the tuned random forest on
    separate validation hypotheses for the 2D case. Left: Distribution
    of \toolfont{carl}'s estimate $\hat{r}(\boldx)$ as a function of
    the observables. Right: scatter plot between the true $r(\boldx)$
    and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_validation_2d_rf_performance}
\end{figure}

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_over_x_grid_final_alt.pdf}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/rhat_vs_r_final_alt.pdf}
  \caption{Likelihood ratio estimation with the neural network on
    separate validation hypotheses for the 2D case.  Left:
    Distribution of \toolfont{carl}'s estimate $\hat{r}(\boldx)$ as a
    function of the observables. Right: scatter plot between the true
    $r(\boldx)$ and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_validation_2d_mlp_performance}
\end{figure}

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_training_sample_size_alt.pdf}
  %\includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_final_alt.pdf}
  \caption{carl performance on the validation hypotheses for different
    training sample sizes and classifiers).  We show the mean squred
    error of the estimated log likelihood ratio between two validation
    benchmark points for the 2D case. Each data point shows the mean
    of five calculations, the error bars are $95\%$ confidence
    intervals.}
  \label{fig:pointwise_validation_2d_overview}
\end{figure}

In Figures~\ref{fig:pointwise_validation_2d_rf_performance} to
\ref{fig:pointwise_validation_2d_overview} we show
\toolfont{carl}'s estimation based on the final settings tuned in
\autoref{sec:pointwise_tuning_2d}. How this performance depends on the
choice of hyperparameters is illustrated in
\autoref{fig:pointwise_validation_2d_tuning}.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_rf_max_depths_alt.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_2d/mse_mlp_hidden_layer_sizes_alt.pdf}%
  \caption{carl performance on the validation hypotheses for different
    hyperparameters.  We show the mean squred error of the estimated
    log likelihood ratio between two validation benchmark points for
    the 2D case. Each data point shows the mean of five calculations,
    the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_validation_2d_tuning}
\end{figure}

All in all, we find that the neural net performs very well without
further fine-tuning, \ie that the same settings optimised on the
comparison of $\boldtheta_0$ versus $\boldtheta_1$ also work well for
$\boldtheta_0'$ versus $\boldtheta_1'$. The random forest, on the
other hand, is not so flexible. The two different hypothesis
comparisons require different choices of the maximal depth of the
trees, and what works best for one choice of parameter points leads to
a poor performance for a different choice. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood contours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After optimizing our classifier setup on the distinction between two
distinct hypotheses, and validating the results on another two
hypotheses, we now turn towards the more relevant problem of
estimating $\boldtheta$ based on some measurements. We generate
25\,000 toy events sampled from the probability distribution for the
SM,
%
\begin{equation}
  \boldtheta_{\text{observed}} = \twovec 0 0 \,.
\end{equation} 
%
For 100 points randomly sampled in $\boldtheta \in [-0.9,0.9]^2$, we
calculate the true expected likelihood ratio to
%
\begin{equation}
  \boldtheta_{1} = \twovec {-0.23} {0.30}
\end{equation}
%
as well as the corresponding \toolfont{carl} estimate. Finally, we
interpolate between these points with a Gaussian Process with Mat\'ern
kernel with $\nu = 0.5$.

Following the results of the previous sections, for the
two-dimensional feature space we use a neural network with two hidden
layers of 8 neurons each, a $\tanh$ activation function, and a
regulator $\alpha = 0.001$. The training samples consist of 200\,000
unweighted samples each, and the $\toolfont{carl}$ default for the
fixed-size binning of the calibration histograms is used.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_truth_vs_carl_2d.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_gp_carl_2d.pdf}%
  \caption{Inference from truth likelihood ratio and \toolfont{carl}'s
    estimate for the 2D case. Left: scatter plot showing the
    difference between the exact expected likelihood ratio for 100
    randomly sampled points and $\boldtheta_1$ and \toolfont{carl}'s
    estimate. Right: true (white) and approximate (orange) likelihood
    contours, using a Gaussian Process for interpolation. The white
    and orange dots show the exact and approximate maximum-likelihood
    estimators. The green and red dots show
    $\boldtheta_{\text{observed}}$ and $\boldtheta_1$,
    respectively. Finally, the small grey dots show the sampled
    parameter points at which the likelihood ratio was evaluated.}
  \label{fig:pointwise_inference_2d}
\end{figure}

In \autoref{fig:pointwise_inference_2d} we show the results for the
two-dimensional feature space. The approximate likelihood map agrees
impressively well with the exact one. 










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Detailed tuning of the point-by-point algorithms}
\label{sec:pointwise}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One class of algorithms simplifies the inference problem by estimating
the likelihood ratio $r(\boldx ; \boldtheta_0, \boldtheta_1)$ for
different pairs $(\boldtheta_0, \boldtheta_1)$ separately. We analyse
two approaches: calibrated classification (\toolfont{carl}), and
regression on the likelihood ratio.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibrated classifiers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Benchmark hypothesis test}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For two benchmark parameter points
%
\begin{equation}
  \boldtheta_0 = \twovec{-0.2} {-0.2} \,, \quad
  \boldtheta_1 = \twovec{0.2} {0.2}
  \label{eq:pointwise_tuning_benchmarks}
\end{equation}
%
we first analyse how well \toolfont{carl} can estimate the likelihood ratio
%
\begin{equation}
  r(\boldx) = \frac {p(\boldx | \boldtheta_0)}  {p(\boldx | \boldtheta_1)}
  \label{eq:pointwise_tuning_r}
\end{equation}
%
depending on the classifier, its hyperparameters, and a number of
different settings. 

\begin{figure}
  \includegraphics[width=\textwidth]{figures/appendix/pointwise_tuning_full/p_over_x.pdf}
  \caption{Distributions of all kinematic observables for $\boldtheta_0$ (blue) and $\boldtheta_1$ (orange).}
  \label{fig:pointwise_tuning_p_over_x}
\end{figure}

In \autoref{fig:pointwise_tuning_p_over_x} we show the distributions
of all 42 considered kinematic variables for these two parameter
points.

For each of the four feature sets outlined above (2D, medium set, full
kinematics, full kinematics plus derived variables), we tune the
parameters in two steps. First, we perform a randomized scan over
the hyperparameters, maximizing the classification power between event
samples sampled from $p(\boldx | \boldtheta_0)$ and
$p(\boldx | \boldtheta_1)$. In a second step we finetune these
parameters on the mean squared error between the \toolfont{carl}
estimate $\log \hat{r}(\boldx)$ and the true value $\log r(\boldx)$.

The first classifier we consider is a \emph{random forest}, more
precisely extremely randomized trees, in the
\toolfont{sklearn.ensemble.ExtraTreesClassifier} implementation. The
following parameters are tuned:
%
\begin{itemize}
  \item \toolfont{n\_estimators}, the number of trees in the forest;
  \item \toolfont{max\_features}, the number of features considered in
    the search for the best split;
  \item \toolfont{max\_depth}, the maximum depth of the trees;
  \item \toolfont{min\_samples\_split}, the minimum fraction of events
    at a node required for a split; and
  \item \toolfont{min\_samples\_leaf}, the minimum fraction of events
    in a leaf.
\end{itemize}

A \emph{multi-layer perceptron} implemented as a
\toolfont{sklearn.neural\_network.MLPClassifier} is our second
classifier. We optimize the following parameters:
%
\begin{itemize}
  \item \toolfont{hidden\_layer\_sizes}, describing the number of hidden
  layers and the number of neurons in each layer;
  \item \toolfont{activation}, the activation function; and
  \item \toolfont{alpha}, an $L^2$ penalty term.
\end{itemize}

For both classifiers, the kinematic features are first rescaled to a
normal distribution with a
\toolfont{sklearn.preprocessing.StandardScaler}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the medium feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/r_over_x_2d.pdf}
  \caption{Truth likelihood ratio $r(\boldx)$ as defined in
    \autoref{eq:pointwise_tuning_r} for the fully differential
    kinematics as a function of two particular observables.}
  \label{fig:pointwise_tuning_full_r_x}
\end{figure}

We now try to see if we can estimate the fully differential likelihood
ratio equally well. This is a much more difficult problem:
\autoref{fig:pointwise_tuning_full_r_x} shows a scatter plot of the
truth likelihood ratio between the two points defined in
\autoref{eq:pointwise_tuning_benchmarks} over the jet $p_T$ and
$\Delta \phi_{jj}$, showing sizable fluctuations between close points
due to the other directions in phase space. First we train
$\toolfont{carl}$ on the medium set of 15 observables defined in
\autoref{sec:features}.

\begin{table}
\small
\begin{tabular}{ll r rrrr }
  \toprule 
  & Parameter & Range & Random & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  RF & \toolfont{n\_estimators} & $50 \dots 200$ & $100$ &&& $100$ \\
  & \toolfont{max\_features} & $1 \dots 24$ & $6$ &&& $6$ \\
  & \toolfont{max\_depth} & $1 \dots 20, \infty$ & $12$ &&& $12$ \\
  & \toolfont{min\_samples\_split} & $0 \dots 1$ & $0$ &&& $0$ \\
  & \toolfont{min\_samples\_leaf} & $0 \dots 0.5$ & $0$ &&& $0$ \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $2$ & $1$ & $1$ & $2$\\
  & neurons at each layer & $2\dots 100$ per layer & $(8,8)$ & $(5)$ & $(5)$ & $(8,8)$\\
  & activation function & $\tanh, \relu, \logistic$ & $\logistic$ & $\logistic$ & $\logistic$ & $\logistic$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $10^{-9}$ & $0$ & $0$\\
  & initial learning rate & $0.0001 \dots 0.01$ & & $0.01$ & $0.0067$ & $0.001$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the classification problem between
  $\boldtheta_0$ and $\boldtheta_1$ using the medium feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range and the best settings as determined by a randomized
  search CV, a Bayesian optimization procedure geared towards exploitation, and a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_tuning_smart_parameters}
\end{table}

We first tune hyperparameters with a randomized scan on the
classification problem between unweighted event samples drawn from
$p(\boldx | \boldtheta_0)$ and $p(\boldx | \boldtheta_1)$. Using
\toolfont{sklearn.model\_selection.RandomizedSearchCV}, we optimize on
the ROC AUC. We give the optimal parameters in
\autoref{tbl:pointwise_tuning_full_parameters}. The optimal random
forests are now quite a bit deeper than in the 2d case.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_smart_rf.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_smart_mlp.pdf}%
  \caption{Likelihood ratio estimation with the tuned classifiers
    (left: random forest, right: neural network) using the medium feature set as
    input. We show a scatter plot between the true $r(\boldx)$ and the
    estimate $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_performance}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_x_smart_rf.pdf}
  \caption{Correlation of $r(\boldx)$ (blue) and $\toolfont{carl}$
    estimate $\hat{r}(\boldx)$ (orange) with various kinematic
    observables. As classifier we use the tuned random forest with the
    medium feature set as input. To guide the eye we also show fitted
    second-order polynomials.}
  \label{fig:pointwise_tuning_smart_rf_rhat_vs_x}
\end{figure}

In \autoref{fig:pointwise_tuning_smart_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_tuning_smart_parameters}. The performance is
much worse than in the 2d
case. \autoref{fig:pointwise_tuning_smart_rf_rhat_vs_x} shows a
scatter plot of the kinematic observables versus the exact ratio
$r(\boldx)$ and the estimate $\hat{r}(\boldx)$.

Next, we vary all parameters one by one and see how this affects the
mean squared error of $\log \hat{r}(\boldx)$. The results are shown in
Figures~\ref{fig:pointwise_tuning_smart_rf_tuning}.
and \ref{fig:pointwise_tuning_smart_mlp_tuning}.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_rf_n_estimators.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_rf_max_depths.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_rf_max_features.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_rf_min_samples_splits.pdf}%
  \caption{carl performance as a function of the random forest
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points using the medium
    feature set as input. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_smart_rf_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_mlp_hidden_layer_sizes.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_mlp_activation.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_mlp_alpha.pdf}%
  \caption{carl performance as a function of the neural network
  hyperparameters. Mean squred error of the estimated log likelihood
  ratio between two benchmark points using the medium feature set as
  input. Each data point shows the mean of three calculations, the
  error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_smart_mlp_tuning}
\end{figure}

For the random forest, the results of the hyperparameter scan lead to
a good performance, and there is no obvious reason to change any of
the settings in the right column of
\autoref{tbl:pointwise_tuning_smart_parameters}. For the neural
network, we define a final setup with a $\relu$ activation function
instead of the logistic one, with all other settings in the right
column of \autoref{tbl:pointwise_tuning_smart_parameters} unchanged.

% \begin{figure}
%   \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_smart_training_sample_size.pdf}%
%   \caption{carl performance as a function of the training sample
%   size.  We show the mean squred error of the estimated log
%   likelihood ratio between two benchmark points using the medium
%   feature set as input. Each data point shows the mean of three
%   calculations, the error bars are $95\%$ confidence intervals.}
%   \label{fig:pointwise_tuning_smart_carl_tuning}
% \end{figure}

% \autoref{fig:pointwise_tuning_smart_carl_tuning} shows the effect of
% the size of the training samples.  Again, we choose 200\,000 events
% per sample as a compromise between computation time and performance. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the full feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now we use the full kinematics including the Higgs decay patterns,
parametrized by the 24 energies, transverse momenta, and angles given
in configuration 2 in
\autoref{sec:features}.

\begin{table}
\small
\begin{tabular}{ll r rrrr }
  \toprule 
  & Parameter & Range & Random & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  RF & \toolfont{n\_estimators} & $50 \dots 200$ & $200$ &&& $200$ \\
  & \toolfont{max\_features} & $1 \dots 24$ & $20$ &&& $20$ \\
  & \toolfont{max\_depth} & $1 \dots 20, \infty$ & $12$ &&& $12$ \\
  & \toolfont{min\_samples\_split} & $0 \dots 1$ & $0.001$ &&& $0.001$ \\
  & \toolfont{min\_samples\_leaf} & $0 \dots 0.5$ & $0$ &&& $0$ \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $2$ & $2$ & $2$ & $2$\\
  & neurons at each layer & $2\dots 100$ per layer & $(4,12)$ & $(50,50)$ & $(50,20)$ & $(4,12)$\\
  & activation function & $\tanh, \relu, \logistic$ & $\logistic$ & $\logistic$ & $\logistic$ & $\logistic$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $0$ & $10^{-9}$ & $0$\\
  & initial learning rate & $0.0001 \dots 0.01$ & & $0.0013$ & $0.0018$ & $0.001$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the classification problem between
  $\boldtheta_0$ and $\boldtheta_1$ using the full feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range and the best settings as determined by a randomized
  search CV, a Bayesian optimization procedure geared towards exploitation, and a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_tuning_full_parameters}
\end{table}

We first tune hyperparameters with a randomized scan on the
classification problem between unweighted event samples drawn from
$p(\boldx | \boldtheta_0)$ and $p(\boldx | \boldtheta_1)$. Using
\toolfont{sklearn.model\_selection.RandomizedSearchCV}, we optimize on
the ROC AUC. We give the optimal parameters in
\autoref{tbl:pointwise_tuning_full_parameters}. The optimal random
forests are now quite a bit deeper than in the 2d case. For the neural
networks we find a surprising preference for small hidden layers, and
logistic activation functions are now preferred.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_full_rf.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_full_mlp.pdf}%
  \caption{Likelihood ratio estimation with the tuned classifiers
    (left: random forest, right: neural network) using the full
    feature set as input. We show a scatter plot between the true
    $r(\boldx)$ and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_tuning_full_rf_performance}
\end{figure}

In \autoref{fig:pointwise_tuning_full_rf_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_tuning_full_parameters}. The performance is
much worse than in the 2d case.

Next, we vary all parameters one by one and see how this affects the
mean squared error of $\log \hat{r}(\boldx)$. The results are shown in
Figures~\ref{fig:pointwise_tuning_full_rf_tuning} to
\ref{fig:pointwise_tuning_full_carl_tuning}. For the random forest, we
again find that the parameters tuned on the classification problem
already lead to a good performance for the estimation of
$r(\boldx)$. For the neural network, switching to the $\relu$
activation function improves the estimation, so we use it in our final
setup.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_rf_n_estimators.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_rf_max_depths.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_rf_max_features.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_rf_min_samples_splits.pdf}%
  \caption{carl performance as a function of the random forest
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points using the full
    feature set as input. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_full_rf_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_mlp_hidden_layer_sizes.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_mlp_activation.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_mlp_alpha.pdf}%
  \caption{carl performance as a function of the neural network
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points using the full
    feature set as input. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_full_mlp_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_training_sample_size.pdf}%
  \caption{carl performance as a function of the training sample size.
    We show the mean squred error of the estimated log likelihood
    ratio between two benchmark points using the full feature set as
    input. Each data point shows the mean of three calculations, the
    error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_full_carl_tuning}
\end{figure}

\autoref{fig:pointwise_tuning_full_carl_tuning} shows the effect of
the size of the training samples.  Again, we choose 200\,000 events
per sample as a compromise between computation time and performance.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the full plus derived feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Finally, we analyse whether the estimator is improved if we include an
additional 18 derived quantities, see \autoref{sec:features}. The
results of the hyperparameter scan are given in
\autoref{tbl:pointwise_tuning_full_parameters}.

\begin{table}
\small
\begin{tabular}{ll r rrrr }
  \toprule 
  & Parameter & Range & Random & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  RF & \toolfont{n\_estimators} & $50 \dots 200$ & $200$ &&& $200$ \\
  & \toolfont{max\_features} & $1 \dots 42$ & $42$ &&& $42$ \\
  & \toolfont{max\_depth} & $1 \dots 20, \infty$ & $10$ &&& $10$ \\
  & \toolfont{min\_samples\_split} & $0 \dots 1$ & $0.001$ &&& $0.001$ \\
  & \toolfont{min\_samples\_leaf} & $0 \dots 0.5$ & $0$ &&& $0$ \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $2$ & $2$ & $2$ & $2$\\
  & neurons at each layer & $2\dots 100$ per layer & $(4,12)$ & $(5,5)$ & $(100,100)$ & $(4,12)$\\
  & activation function & $\tanh, \relu, \logistic$ & $\logistic$ & $\logistic$ & $\logistic$ & $\logistic$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $10^{-8}$ & $0$ & $0$\\
  & initial learning rate & $0.0001 \dots 0.01$ & & $0.00022$ & $0.00064$ & $0.001$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the classification problem between
  $\boldtheta_0$ and $\boldtheta_1$ using the full plus derived
  feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range and the best settings as determined by a randomized
  search CV, a Bayesian optimization procedure geared towards exploitation, and a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_tuning_derived_parameters}
\end{table}

In \autoref{fig:pointwise_tuning_derived_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_tuning_derived_parameters}. The performance is
slightly better than without the derived variables.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_derived_rf.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_derived_mlp.pdf}%
  \caption{Likelihood ratio estimation with the tuned classifiers
    (left: random forest, right: neural network) using the full plus
    derived feature set as input. We show a scatter plot between the
    true $r(\boldx)$ and the estimate $\hat{r}(\boldx)$.}.
  \label{fig:pointwise_tuning_derived_performance}
\end{figure}

Again, we vary the parameters one by one and check if we can improve
the performance, see
Figures~\ref{fig:pointwise_tuning_derived_rf_tuning} to
\ref{fig:pointwise_tuning_derived_carl_tuning}. The results mirror
those in the previous section. The only improvement we can find is
switching to the $\relu$ activation function for the neural network in
our final setup, all other settings remain as in the right column of
\autoref{tbl:pointwise_tuning_derived_parameters}.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_rf_n_estimators.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_rf_max_depths.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_rf_max_features.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_rf_min_samples_splits.pdf}%
  \caption{carl performance as a function of the random forest
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points using the full
    plus derived feature set as input. Each data point shows the mean
    of three calculations, the error bars are $95\%$ confidence
    intervals.}
  \label{fig:pointwise_tuning_derived_rf_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_mlp_hidden_layer_sizes.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_mlp_activation.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_mlp_alpha.pdf}%
  \caption{carl performance as a function of the neural network
    hyperparameters. We show the mean squred error of the estimated
    log likelihood ratio between two benchmark points using the full
    plus derived feature set as input. Each data point shows the mean
    of three calculations, the error bars are $95\%$ confidence
    intervals.}
  \label{fig:pointwise_tuning_derived_mlp_tuning}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_derived_training_sample_size.pdf}%
  \caption{carl performance as a function of the training sample size.
    We show the mean squred error of the estimated log likelihood
    ratio between two benchmark points using the full plus derived
    feature set as input. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_derived_carl_tuning}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Calibration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now stick to a random forest trained on the medium feature set with
hyperparameters as given in the last column of
\autoref{tbl:pointwise_tuning_smart_parameters} and have a closer look
at \toolfont{carl}'s calibration procedure.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_smart_rf_uncalibrated.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/rhat_vs_r_smart_rf.pdf}%
  \caption{Likelihood ratio estimation with uncalibrated (left) vs
    calibrated (right) random forests. We show a scatter plot between
    the true $r(\boldx)$ and the estimate $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_calibration}
\end{figure}

First, \autoref{fig:pointwise_tuning_smart_calibration} shows the
output of an uncalibrated classifier and compares it to the calibrated
classifier. Clearly, calibration improves the performance. 

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_calibration_method.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_cv_folds.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_calibration_bins.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_full_calibration_bins_variable.pdf}%
  \caption{Performance of \toolfont{carl} as a function of the
    calibration method and its parameters. Top left: different
    calibration algorithms. Top right: different cross-validation
    setups for the histogram validation. Bottom left: different number
    of bins for the histogram calibration. Bottom right: Same, also
    showing the results for variable bin widths. We show the mean
    squred error of the estimated log likelihood ratio between two
    benchmark points using the medium feature set as input. Each data
    point shows the mean of three calculations, the error bars are
    $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_smart_calibration_tuning}
\end{figure}

\autoref{fig:pointwise_tuning_smart_calibration_tuning} shows how the
performance of \toolfont{carl} depends on the calibration method and
its settings. The best results come from the simple histogram
calibration (with equidistant bin sizes) or an isotonic calibration.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_one_smart_rf.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_average_smart_rf.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/average_s_raw_vs_rhat_smart_rf.pdf}%
  \caption{Details of the histogram calibration procedure. Top left:
    correlation between raw $s(\boldx)$ from one random forest and the
    corresponding calibrated values. Top right: correlation between
    average raw $s(\boldx)$ from all random forests in the
    cross-validator and the average calibrated $s(\boldx)$. Bottom:
    correlation between average raw $s(\boldx)$ from all random
    forests in the cross-validator and the final output
    $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_calibration_histogram1}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/calibration_histos_one_smart_rf.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_one_log_smart_rf.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_smart_rf.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_log_smart_rf.pdf}%
  \caption{Calibration histograms. Top left: actual calibration
    histogram for one random forest. Top right: distribution of the
    raw and calibrated values of $s(\boldx)$ for one random
    forest. Bottom left: distribution of the average raw and average
    calibrated values of $s(\boldx)$ for the ensemble of all random
    forests. Bottom right: same, but with a logarithmic $y$ axis.}
  \label{fig:pointwise_tuning_smart_calibration_histogram2}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_one_smart_rf_isotonic.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_average_smart_rf_isotonic.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/average_s_raw_vs_rhat_smart_rf_isotonic.pdf}%
  \caption{Details of the isotonic calibration procedure. Top left:
    correlation between raw $s(\boldx)$ from one random forest and the
    corresponding calibrated values. Top right: correlation between
    average raw $s(\boldx)$ from all random forests in the
    cross-validator and the average calibrated $s(\boldx)$. Bottom:
    correlation between average raw $s(\boldx)$ from all random
    forests in the cross-validator and the final output
    $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_calibration_isotonic}
\end{figure}

In \autoref{fig:pointwise_tuning_smart_calibration_histogram1} and
\autoref{fig:pointwise_tuning_smart_calibration_histogram2} we
illuminate the relation between raw and calibrated $s(\boldx)$ for the
default histogram-based
calibration with fixed bin widths. \autoref{fig:pointwise_tuning_smart_calibration_isotonic}
does the same for isotonic calibration. Both calibration procedures
work as expected.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_one_smart_rf_var_binwidth.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_average_smart_rf_var_binwidth.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/average_s_raw_vs_rhat_smart_rf_var_binwidth.pdf}%
  \caption{Details of the histogram calibration procedure with
    variable bin width, with different binnings for nominator and
    denominator. Top left: correlation between raw $s(\boldx)$ from
    one random forest and the corresponding calibrated values. Top
    right: correlation between average raw $s(\boldx)$ from all random
    forests in the cross-validator and the average calibrated
    $s(\boldx)$. Bottom: correlation between average raw $s(\boldx)$
    from all random forests in the cross-validator and the final
    output $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_calibration_histogram_varwidth1}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/calibration_histos_one_smart_rf_var_binwidth.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_one_log_smart_rf_var_binwidth.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_smart_rf_var_binwidth.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_log_smart_rf_var_binwidth.pdf}%
  \caption{Calibration histograms with variable bin width, with
    different binnings for nominator and denominator. Top left: actual
    calibration histogram for one random forest. Top right:
    distribution of the raw and calibrated values of $s(\boldx)$ for
    one random forest. Bottom left: distribution of the average raw
    and average calibrated values of $s(\boldx)$ for the ensemble of
    all random forests. Bottom right: same, but with a logarithmic $y$
    axis.}
  \label{fig:pointwise_tuning_smart_calibration_histogram_varwidth2}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_one_smart_rf_var_binwidth_common.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_raw_vs_cal_average_smart_rf_var_binwidth_common.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/average_s_raw_vs_rhat_smart_rf_var_binwidth_common.pdf}%
  \caption{Details of the histogram calibration procedure with
    variable bin width, with common binning for nominator and
    denominator. Top left: correlation between raw $s(\boldx)$ from
    one random forest and the corresponding calibrated values. Top
    right: correlation between average raw $s(\boldx)$ from all random
    forests in the cross-validator and the average calibrated
    $s(\boldx)$. Bottom: correlation between average raw $s(\boldx)$
    from all random forests in the cross-validator and the final
    output $\hat{r}(\boldx)$.}
  \label{fig:pointwise_tuning_smart_calibration_histogram_varwidth3}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/calibration_histos_one_smart_rf_var_binwidth_common.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_one_log_smart_rf_var_binwidth_common.pdf}\\%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_smart_rf_var_binwidth_common.pdf}%
  \includegraphics[width=0.45\textwidth]{figures/appendix/pointwise_tuning_full/s_histos_average_log_smart_rf_var_binwidth_common.pdf}%
  \caption{Calibration histograms with variable bin width, with common
    binning for nominator and denominator. Top left: actual
    calibration histogram for one random forest. Top right:
    distribution of the raw and calibrated values of $s(\boldx)$ for
    one random forest. Bottom left: distribution of the average raw
    and average calibrated values of $s(\boldx)$ for the ensemble of
    all random forests. Bottom right: same, but with a logarithmic $y$
    axis.}
  \label{fig:pointwise_tuning_smart_calibration_histogram_varwidth4}
\end{figure}

Figures~\ref{fig:pointwise_tuning_smart_calibration_histogram_varwidth1}
to \ref{fig:pointwise_tuning_smart_calibration_histogram_varwidth4}
show the same plots for the histogram calibration procedure with
variable bin width. Different bin boundaries for the nominator and
denominator histograms lead to problems (see the weird behaviour at
$s \gtrsim 0.6$). But even with a common binning, the variable-width
calibration setup performs worse than fixed-width histograms.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Comparison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We summarize our different attempts to estimate $r(\boldx)$ for the
high-dimensional $\boldx$ in
\autoref{fig:pointwise_tuning_full_final}. No setup performs
significantly better than a random forest trained on the medium set of
15 production observables.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/mse_final.pdf}%
  \caption{carl performance for \toolfont{sklearn} default
    hyperparameters, the best parameters according to the initial
    hyperparameter scan, and the final parameters as defined in the
    text, using three different feature sets as inputs. We show
    the mean squred error of the estimated log likelihood ratio
    between two benchmark points based on the fully differential
    feature space. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_tuning_full_final}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Likelihood contours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After optimizing our classifier setup on the distinction between two
distinct hypotheses, and validating the results on another two
hypotheses, we now turn towards the more relevant problem of
estimating $\boldtheta$ based on some measurements. We generate
25\,000 toy events sampled from the probability distribution for the
SM,
%
\begin{equation}
  \boldtheta_{\text{observed}} = \twovec 0 0 \,.
\end{equation} 
%
For 100 points randomly sampled in $\boldtheta \in [-0.9,0.9]^2$, we
calculate the true expected likelihood ratio to
%
\begin{equation}
  \boldtheta_{1} = \twovec {-0.23} {0.30}
\end{equation}
%
as well as the corresponding \toolfont{carl} estimate. Finally, we
interpolate between these points with a Gaussian Process with Mat\'ern
kernel with $\nu = 0.5$.

Based on the tuning described above, we train $\toolfont{carl}$ on the
medium set of 15 production-side observables. We use a random forest
with 100 estimators, a maximal tree depth of 10, which considers 6
randomly chosen features for each splitting. The training samples
consist of 200\,000 unweighted samples each, and the $\toolfont{carl}$
default for the fixed-size binning of the calibration histograms is
used.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_truth_vs_carl_full.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_gp_carl_full.pdf}%
  \caption{Inference from truth likelihood ratio and \toolfont{carl}'s
    estimate for the fully differential case. Left: scatter plot
    showing the difference between the exact expected likelihood ratio
    for 100 randomly sampled points and $\boldtheta_1$ and
    \toolfont{carl}'s estimate. Right: true (white) and approximate
    (orange) likelihood contours, using a Gaussian Process for
    interpolation. The white and orange dots show the exact and
    approximate maximum-likelihood estimators. The green and red dots
    show $\boldtheta_{\text{observed}}$ and $\boldtheta_1$,
    respectively. Finally, the small grey dots show the sampled
    parameter points at which the likelihood ratio was evaluated.}
  \label{fig:pointwise_inference_full}
\end{figure}

The results are shown in \autoref{fig:pointwise_inference_full}. There
are visible differences between exact and approximate likelihood, but
overall the agreement is still pretty good.

We have also checked that a deep and wide neural network does not lead
to a better performance in this inference problem.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next, we check how this performance of calibrated classifiers compares
to regressor directly trained on the ratio $r(\boldx)$\,---\,which is
possible on our truth-level sample, but not in a likelihood-free
setup. Note that in this setup the regressors are not
calibrated. Since the calibrated classifiers work flawlessly for the
simple 2D case, we now stick to the fully differential case.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Benchmark hypothesis test}
\label{sec:pointwise_regression_tuning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Again, we first tune the settings on the likelihood ratio between the
two parameter points
%
\begin{equation}
  \boldtheta_0 = \twovec{-0.2} {-0.2} \,, \quad
  \boldtheta_1 = \twovec{0.2} {0.2}
  \label{eq:pointwise_regression_tuning_benchmarks}
\end{equation}

For now, we only use neural networks (brief experiments with random
forests and Gaussian processes lead to different computational
problems and errors).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{What to learn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this regression setup, one has to be careful which quantity should
be learned by the regressors, or with the definition of the loss
function. The naive choice of minimizing the squared error of the
likelihood ratio
$r(\boldx) = p(\boldx | \boldtheta_0) / p(\boldx | \boldtheta_1)$ does
not work very well. First, this loss function changes significantly
under the exchange $\boldtheta_0 \leftrightarrow \boldtheta_1$.
Second, individual phase-space points with
$p(\boldx | \boldtheta_1) \ll 1$ will have a very large weight and
dominate the loss function, potentially decreasing the performance of
the regressor for the majority of the phase-space points with
$r \approx 1$. Instead, we find that it is advantageous to optimize on
the squared error of $\log r(\boldx)$. This quantity is invariant
under $\boldtheta_0 \leftrightarrow \boldtheta_1$, and the
squared-error loss function is not dominated by few extreme
phase-space points.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_smart_mlp.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_smart_mlp_switched.pdf}\\%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_smart_mlp_logr.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_smart_mlp_logr_switched.pdf}\\%
  \caption{Likelihood ratio estimation with regression, learning
    $r(\boldx) = p(\boldx | \boldtheta_0) / p(\boldx | \boldtheta_1) $
    (top left), $r' (\boldx) = 1/r(\boldx)$ (top right),
    $\log r(\boldx)$ (bottom left), and $\log r'(\boldx)$ (bottom
    right). In each case we use the medium feature set as input and
    train a neural network regressor with four hidden layers of 100
    neurons each. We show a scatter plot between the true $r(\boldx)$
    and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_regression_tuning_what}
\end{figure}

We demonstrate this
\autoref{fig:pointwise_regression_tuning_what}. Training $r(\boldx)$
directly can work for the right phase-space points (top left panel),
but can dramatically fail for other ratios (top right). Training
$\log r(\boldx)$ does not depend on the choice of numerator and works
reliably in all analysed ratios (bottom panels).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the medium feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\small
\begin{tabular}{ll r rrr }
  \toprule 
  & Parameter & Range & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $3$ & $5$ & $3$\\
  & neurons in last hidden layer & $5\dots 100$ & $5$ & $20$ & $5$\\
  & neurons in other hidden layers & $5\dots 100$ & $100$ & $50$ & $100$\\
  & activation function & $\tanh, \relu, \logistic$ & $\tanh$ & $\relu$ & $\tanh$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $0.02$ & $0$\\
  & initial learning rate & $0.0001 \dots 0.01$ & $0.0002$ & $0.0003$ & $0.0002$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the regression problem of $r(\boldx)$ between
  $\boldtheta_0$ and $\boldtheta_1$ using the medium feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range, the best settings as determined by 
  a Bayesian optimization procedure geared towards exploitation, and the best settings
  determined by a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_regression_tuning_smart_parameters}
\end{table}

For the regression setup, we simply tune the hyperparameters with a
Bayesian optimization procedure on the regression problem to estimate
the likelihood ratio $r(\boldx)$ between $p(\boldx | \boldtheta_0)$
and $p(\boldx | \boldtheta_1)$. Using \toolfont{skopt.BayesSearchCV},
we optimize on the mean squared error. We give the optimal parameters
in \autoref{tbl:pointwise_regression_tuning_full_parameters}. Unlike
for the calibrated classifiers, the neural networks are now
significantly more complex.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_smart_mlp.pdf}%
  \caption{Likelihood ratio estimation with regression using the
    medium feature set as input. We show a scatter plot between the
    true $r(\boldx)$ and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_regression_tuning_smart_performance}
\end{figure}

In \autoref{fig:pointwise_regression_tuning_smart_performance} we show
how well \toolfont{carl} can estimate the true likelihood ratio
$r(\boldx)$ with the tuned regressor given in the right column of
\autoref{tbl:pointwise_regression_tuning_smart_parameters}. We find
impressive agreement.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the full feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now we use the full kinematics including the Higgs decay patterns,
parametrized by the 24 energies, transverse momenta, and angles given
in configuration 3 in
\autoref{sec:features}.

\begin{table}
\small
\begin{tabular}{ll r rrr }
  \toprule 
  & Parameter & Range & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $5$ & $5$ & $5$\\
  & neurons in last hidden layer & $5\dots 100$ & $10$ & $10$ & $10$\\
  & neurons in other hidden layers & $5\dots 100$ & $50$ & $50$ & $50$\\
  & activation function & $\tanh, \relu, \logistic$ & $\tanh$ & $\tanh$ & $\tanh$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $0$ & $0$ \\
  & initial learning rate & $0.0001 \dots 0.01$ & $0.0002$ & $0.0003$ & $0.0002$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the regression problem of $r(\boldx)$ between
  $\boldtheta_0$ and $\boldtheta_1$ using the full feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range and the best settings as determined by a randomized
  search CV, a Bayesian optimization procedure geared towards exploitation, and a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_regression_tuning_full_parameters}
\end{table}

We first tune hyperparameters with a randomized scan on the
classification problem between unweighted event samples drawn from
$p(\boldx | \boldtheta_0)$ and $p(\boldx | \boldtheta_1)$. Using
\toolfont{sklearn.model\_selection.RandomizedSearchCV}, we optimize on
the ROC AUC. We give the optimal parameters in
\autoref{tbl:pointwise_regression_tuning_full_parameters}. Again, we
find a preference for around four hidden layers with 100 neurons each.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_full_mlp.pdf}%
  \caption{Likelihood ratio estimation with the tuned regressor
    using the full
    feature set as input. We show a scatter plot between the true
    $r(\boldx)$ and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_regression_tuning_full_rf_performance}
\end{figure}

In \autoref{fig:pointwise_regression_tuning_full_rf_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_regression_tuning_full_parameters}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimators for the full plus derived feature set}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Finally, we analyse whether the estimator is improved if we include an
additional 18 derived quantities, see \autoref{sec:features}. The
results of the hyperparameter scan are given in
\autoref{tbl:pointwise_regression_tuning_full_parameters}.

\begin{table}
\small
\begin{tabular}{ll r rrr }
  \toprule 
  & Parameter & Range & Bayes 1 & Bayes 2 & Benchmark \\
  \midrule
  NN & number of hidden layers & $1\dots 5$ & $3$ & $3$ & $3$\\
  & neurons in last hidden layer & $5\dots 100$ & $50$ & $100$ & $50$\\
  & neurons in other hidden layers & $5\dots 100$ & $100$ & $100$ & $100$\\
  & activation function & $\tanh, \relu, \logistic$ & $\tanh$ & $\logistic$ & $\tanh$ \\
  & $\alpha$ & $0\dots 100$ & $0$ & $0.00004$ & $0$ \\
  & initial learning rate & $0.0001 \dots 0.01$ & $0.0003$ & $0.0006$ & $0.0003$ \\
  \bottomrule
\end{tabular}
\caption{Hyperparameter scan on the regression problem of $r(\boldx)$ between
  $\boldtheta_0$ and $\boldtheta_1$ using the full plus derived feature set as input. For
  each parameter of the random forrest (RF) and the neural network (NN)
  we show the considered range and the best settings as determined by a randomized
  search CV, a Bayesian optimization procedure geared towards exploitation, and a Bayesian optimization procedure
  geared towards exploration. The final column shows our baseline parameters for the next step of the
  optimization procedure.}
 \label{tbl:pointwise_regression_tuning_derived_parameters}
\end{table}

We first tune hyperparameters with a randomized scan on the
classification problem between unweighted event samples drawn from
$p(\boldx | \boldtheta_0)$ and $p(\boldx | \boldtheta_1)$. Using
\toolfont{sklearn.model\_selection.RandomizedSearchCV}, we optimize on
the ROC AUC. We give the optimal parameters in
\autoref{tbl:pointwise_regression_tuning_derived_parameters}. Again, we
find a preference for around four hidden layers with 100 neurons each.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/rhat_vs_r_derived_mlp.pdf}%
  \caption{Likelihood ratio estimation with the tuned regressor
    using the full
    feature plus derived set as input. We show a scatter plot between the true
    $r(\boldx)$ and the estimate $\hat{r}(\boldx)$}.
  \label{fig:pointwise_regression_tuning_derived_rf_performance}
\end{figure}

In \autoref{fig:pointwise_regression_tuning_derived_rf_performance} we show how well
\toolfont{carl} can estimate the true likelihood ratio $r(\boldx)$
with the tuned classifiers given in the right column of
\autoref{tbl:pointwise_regression_tuning_derived_parameters}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Comparison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We summarize our different attempts to estimate $r(\boldx)$ for the
high-dimensional $\boldx$ with different regressors in
\autoref{fig:pointwise_regression_tuning_full_final}. The different
input sets make a negligible difference, so for the sake of
computation time we from now on stick to the simple medium set.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/mse_final.pdf}%
  \caption{Performance of \toolfont{carl} with regression for \toolfont{sklearn} default
    hyperparameters, the best parameters according to the initial
    hyperparameter scan, and the final parameters as defined in the
    text, using three different feature sets as inputs. We show
    the mean squred error of the estimated log likelihood ratio
    between two benchmark points based on the fully differential
    feature space. Each data point shows the mean of three
    calculations, the error bars are $95\%$ confidence intervals.}
  \label{fig:pointwise_regression_tuning_full_final}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Likelihood contours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After optimizing our regressor setup on the likelihood ratio between
two distinct hypotheses, we now turn towards the more relevant problem
of estimating $\boldtheta$ based on some measurements. We generate
25\,000 toy events sampled from the probability distribution for the
SM,
%
\begin{equation}
  \boldtheta_{\text{observed}} = \twovec 0 0 \,.
\end{equation} 
%
For 100 points randomly sampled in $\boldtheta \in [-0.9,0.9]^2$, we
calculate the true expected likelihood ratio to
%
\begin{equation}
  \boldtheta_{1} = \twovec {-0.23} {0.30}
\end{equation}
%
as well as the corresponding \toolfont{carl} estimate. Finally, we
interpolate between these points with a Gaussian Process with Mat\'ern
kernel with $\nu = 0.5$.

We train $\toolfont{carl}$ on the medium feature set. We use a neural
network regressor with two hidden layers of 100 neurons each and
another hidden layer with 5 neurons, the $\tanh$ activation function,
$\alpha=0$, and an initial learning rate of $0.0002$. The training
samples consist of 200\,000 unweighted samples each.

\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_truth_vs_regression_full.pdf}%
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_inference/llr_gp_regression_vs_truth_full.pdf}%
  \caption{Inference from truth likelihood ratio and \toolfont{carl}'s
    estimate for the fully differential case with regression. Left: scatter plot
    showing the difference between the exact expected likelihood ratio
    for 100 randomly sampled points and $\boldtheta_1$ and
    \toolfont{carl}'s estimate. Right: true (white) and approximate
    (cyan) likelihood contours, using a Gaussian Process for
    interpolation. The white and cyan dots show the exact and
    approximate maximum-likelihood estimators. The green and red dots
    show $\boldtheta_{\text{observed}}$ and $\boldtheta_1$,
    respectively. Finally, the small grey dots show the sampled
    parameter points at which the likelihood ratio was evaluated.}
  \label{fig:pointwise_regression_inference_full}
\end{figure}

The results are shown in
\autoref{fig:pointwise_regression_inference_full}. We see that
$\toolfont{carl}$ with regression captures the likelihood function
qualitatively well and slightly better than the calibrated
classifiers.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ROC curves}
\label{sec:appendix_ROC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_tuning_full/roc_smart_rf.pdf}
  \includegraphics[height=0.45\textwidth]{figures/appendix/pointwise_regression_tuning_full/roc_smart_mlp_logr.pdf}
  \caption{ROC curves for the classification between event samples
    based on $\boldtheta_0$ and $\boldtheta_1$ as defined in
    \autoref{eq:pointwise_tuning_benchmarks}. Left: calibrated
    classifiers (with random forests as described in
    \autoref{sec:pointwise_tuning_full}). Right: regression (with a
    neural network as described in
    \autoref{sec:pointwise_regression_tuning}.}
  \label{fig:pointwise_tuning_roc}
\end{figure}

In Figure~\ref{fig:pointwise_tuning_roc} we show some ROC curves
corresponding to the classification problem of
\autoref{sec:pointwise_tuning_full}. We compare the TPRs and FPRs of
six different scores:
%
\begin{itemize}
  \item the raw output $\hat{s}(\boldx)$ of one random forest, using the medium feature set as input and the tuned settings described in \autoref{sec:pointwise_tuning_full};
  \item the same output, calibrated with the histogram method;
  \item the average of the outputs of five such calibrated random forests;
  \item the corresponding $\toolfont{carl}$ estimate $\hat{r}(\boldx)$;
  \item the estimate $\hat{r}(\boldx)$ based on a regressor as described in \autoref{sec:pointwise_regression_tuning}; and
  \item the true $r(\boldx)$, which defines the optimal classifier according to the Neyman-Pearson lemma.
\end{itemize}
%
The resulting ROC curves are pretty much indistinguishable. This shows
that the classification problem between two hypotheses is solved
nearly optimally by our learning setup, and that this is much easier
than the regression\,/\,density estimation problem of learning
$r(\boldx)$.