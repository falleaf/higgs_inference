{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Concatenate, Multiply, Reshape, ActivityRegularization\n",
    "from keras import losses, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "from carl.learning.calibration import HistogramCalibrator, IsotonicCalibrator\n",
    "\n",
    "from awkde import GaussianKDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation between s and r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(r):\n",
    "    return 1./(1. + r)\n",
    "\n",
    "def r(s):\n",
    "    return (1.-s)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "theta1 = 708\n",
    "theta = 9\n",
    "\n",
    "# Data\n",
    "data_dir = '../data'\n",
    "\n",
    "X_train = np.load(data_dir + '/unweighted_events/X_train_point_by_point_9.npy')\n",
    "y_train = np.load(data_dir + '/unweighted_events/y_train_point_by_point_9.npy')\n",
    "r_train = np.load(data_dir + '/unweighted_events/r_train_point_by_point_9.npy')\n",
    "\n",
    "X_calibration = np.load(data_dir + '/unweighted_events/X_calibration.npy')\n",
    "weights_calibration = np.load(\n",
    "    data_dir + '/unweighted_events/weights_calibration.npy')\n",
    "\n",
    "X_test = np.load(data_dir + '/unweighted_events/X_test.npy')\n",
    "r_test = np.load(data_dir + '/unweighted_events/r_test.npy')[9]\n",
    "    \n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(X_train, dtype=np.float64))\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_calibration_transformed = scaler.transform(X_calibration)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "# keras requires training data with the same output as the NN\n",
    "log_r_gradients_train = np.zeros((r_train.shape[0], 43))\n",
    "log_r_gradients_train[:,0] = np.log(r_train)\n",
    "y_gradients_train = np.zeros((r_train.shape[0], 43))\n",
    "y_gradients_train[:,0] = y_train\n",
    "\n",
    "# Finally, s points for the calibration curves\n",
    "s_eval = np.linspace(0., 1., 200).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = [0,1,2,3,39,40,41] # j1 momentum + delta eta, delta phi, mjj\n",
    "X_small = X_train_transformed[::,X_indices]\n",
    "cov_X = np.cov(X_small, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 42\n",
    "\n",
    "def stack_layer(layers):\n",
    "    def f(x):\n",
    "        for k in range(len(layers)):\n",
    "            x = layers[k](x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def hidden_layers(n,\n",
    "                  hidden_layer_size=100,\n",
    "                  activation='tanh',\n",
    "                  dropout_prob=0.0):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        if dropout_prob > 0.:\n",
    "            s = stack_layer([\n",
    "                Dropout(dropout_prob),\n",
    "                Dense(hidden_layer_size, activation=activation)\n",
    "            ])\n",
    "        else:\n",
    "            s = stack_layer([Dense(hidden_layer_size, activation=activation)])\n",
    "        r.append(s)\n",
    "    return stack_layer(r)\n",
    "\n",
    "def loss_function_regression(y_true, y_pred):\n",
    "    return losses.mean_squared_error(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "def make_regressor(n_hidden_layers=3,\n",
    "                   hidden_layer_size=100,\n",
    "                   activation='tanh',\n",
    "                   dropout_prob=0.0):\n",
    "\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(n_features,))\n",
    "\n",
    "    # Network\n",
    "    hidden_layer = Dense(hidden_layer_size, activation=activation)(input_layer)\n",
    "    if n_hidden_layers > 1:\n",
    "        hidden_layer_ = hidden_layers(n_hidden_layers - 1,\n",
    "                                      hidden_layer_size=hidden_layer_size,\n",
    "                                      activation=activation,\n",
    "                                      dropout_prob=dropout_prob)\n",
    "        hidden_layer = hidden_layer_(hidden_layer)\n",
    "    log_r_hat_layer = Dense(1, activation='linear')(hidden_layer)\n",
    "    s_hat_layer = Lambda(lambda x : 1. / 1. + x)(log_r_hat_layer)\n",
    "\n",
    "    # gradients with respect to x\n",
    "    gradient_layer = Lambda(lambda x: K.gradients(x[0], x[1])[0],\n",
    "                            output_shape=(n_features,))([s_hat_layer, input_layer])\n",
    "\n",
    "    # Combine outputs\n",
    "    output_layer = Concatenate()([log_r_hat_layer, gradient_layer])\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function_regression,\n",
    "                  optimizer=optimizers.Adam(clipnorm=1.))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.1499 - val_loss: 0.1864\n",
      "Epoch 2/20\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.0683 - val_loss: 0.1353\n",
      "Epoch 3/20\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.0490 - val_loss: 0.1139\n",
      "Epoch 4/20\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.0363 - val_loss: 0.0990\n",
      "Epoch 5/20\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.0295 - val_loss: 0.0660\n",
      "Epoch 6/20\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.0253 - val_loss: 0.0587\n",
      "Epoch 7/20\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.0224 - val_loss: 0.0803\n",
      "Epoch 8/20\n",
      "80000/80000 [==============================] - 5s 59us/step - loss: 0.0211 - val_loss: 0.0542\n",
      "Epoch 9/20\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.0185 - val_loss: 0.0645\n",
      "Epoch 10/20\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.0169 - val_loss: 0.0451\n",
      "Epoch 11/20\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.0166 - val_loss: 0.0530\n",
      "Epoch 12/20\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.0152 - val_loss: 0.0500\n",
      "Epoch 13/20\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.0140 - val_loss: 0.0494\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4b4619b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train score regression\n",
    "regr = KerasRegressor(lambda: make_regressor(n_hidden_layers=2),\n",
    "                     epochs=20, verbose=1, validation_split=0.2,\n",
    "                     callbacks=[EarlyStopping(verbose=1, patience=3)])\n",
    "\n",
    "regr.fit(X_train_transformed, log_r_gradients_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19994/19994 [==============================] - 1s 28us/step\n",
      "48238/48238 [==============================] - 1s 25us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on calibration sample\n",
    "calibration_predictions = regr.predict(X_calibration_transformed)\n",
    "log_r_hat_regr_calibration = calibration_predictions[:,0]\n",
    "s_hat_regr_calibration = s(np.exp(log_r_hat_regr_calibration))\n",
    "gradients_regr_calibration = calibration_predictions[:,1:]\n",
    "\n",
    "s_hat_regr_calibration_carlinput = np.hstack((s_hat_regr_calibration,s_hat_regr_calibration))\n",
    "y_regr_calibration_carlinput = np.hstack((np.zeros_like(s_hat_regr_calibration), np.ones_like(s_hat_regr_calibration)))\n",
    "w_regr_calibration_carlinput = np.hstack((weights_calibration[theta,::], weights_calibration[theta1,::]))\n",
    "\n",
    "# Evaluate on test sample\n",
    "r_hat_regr_test = np.exp(regr.predict(X_test_transformed)[:,0])\n",
    "s_hat_regr_test = s(r_hat_regr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.9999787129069173 10.0\n"
     ]
    }
   ],
   "source": [
    "#Calculate KDE bandwidths\n",
    "grad_cov_grad = np.diag( gradients_regr_calibration[:,X_indices].dot(\n",
    "    cov_X.dot(gradients_regr_calibration[:,X_indices].T)))\n",
    "\n",
    "#mean_bw_nom = 1.5*1.5/11.4 * 1.5/2.37 * 1.5/1.8\n",
    "#mean_bw_den = 1.1*1.1/5.85 * 1.09/1.27 * 1.09 / 1.2\n",
    "#bandwidths = np.sqrt(grad_cov_grad)\n",
    "#bandwidths = np.clip(bandwidths, np.mean(bandwidths) / 100., np.mean(bandwidths) * 100.)\n",
    "#bandwidths_nom = mean_bw_nom / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_nom = np.clip(bandwidths_nom, 0.1,10.)\n",
    "#bandwidths_den = mean_bw_den / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_den = np.clip(bandwidths_den, 0.1,10.)\n",
    "\n",
    "bandwidths_regr = np.sqrt(grad_cov_grad)\n",
    "bandwidths_regr = np.clip(bandwidths_regr, 0., np.mean(bandwidths_regr) * 10.)\n",
    "bandwidths_regr /= np.mean(bandwidths_regr)\n",
    "bandwidths_regr = np.clip(bandwidths_regr, 0.1,10.)\n",
    "print(np.min(bandwidths_regr), np.mean(bandwidths_regr), np.max(bandwidths_regr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train carl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_carl(y_true, y_pred):\n",
    "    return losses.binary_crossentropy(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "def make_classifier(n_hidden_layers=3,\n",
    "                   hidden_layer_size=100,\n",
    "                   activation='tanh',\n",
    "                   dropout_prob=0.0):\n",
    "\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(n_features,))\n",
    "\n",
    "    # Network\n",
    "    hidden_layer = Dense(hidden_layer_size, activation=activation)(input_layer)\n",
    "    if n_hidden_layers > 1:\n",
    "        hidden_layer_ = hidden_layers(n_hidden_layers - 1,\n",
    "                                      hidden_layer_size=hidden_layer_size,\n",
    "                                      activation=activation,\n",
    "                                      dropout_prob=dropout_prob)\n",
    "        hidden_layer = hidden_layer_(hidden_layer)\n",
    "        \n",
    "    log_r_hat_layer = Dense(1, activation='linear')(hidden_layer)\n",
    "    r_hat_layer = Lambda(lambda x: K.exp(x))(log_r_hat_layer)\n",
    "    s_hat_layer = Lambda(lambda x: 1./(1. + x))(r_hat_layer)\n",
    "\n",
    "    # gradients with respect to x\n",
    "    gradient_layer = Lambda(lambda x: K.gradients(x[0], x[1])[0],\n",
    "                            output_shape=(n_features,))([s_hat_layer, input_layer])\n",
    "\n",
    "    # Combine outputs\n",
    "    output_layer = Concatenate()([s_hat_layer, gradient_layer])\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function_carl,\n",
    "                  optimizer=optimizers.Adam(clipnorm=1.))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "80000/80000 [==============================] - 7s 87us/step - loss: 0.6411 - val_loss: 0.9719\n",
      "Epoch 2/20\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 0.6314 - val_loss: 0.9175\n",
      "Epoch 3/20\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.6298 - val_loss: 0.9579\n",
      "Epoch 4/20\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.6282 - val_loss: 0.9450\n",
      "Epoch 5/20\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 0.6263 - val_loss: 0.9602\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4be8ecf8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train score regression\n",
    "clf = KerasRegressor(lambda: make_classifier(n_hidden_layers=2),\n",
    "                     epochs=20, verbose=1, validation_split=0.2,\n",
    "                     callbacks=[EarlyStopping(verbose=1, patience=3)])\n",
    "\n",
    "clf.fit(X_train_transformed, y_gradients_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19994/19994 [==============================] - 0s 18us/step\n",
      "48238/48238 [==============================] - 1s 14us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on calibration sample\n",
    "calibration_predictions = clf.predict(X_calibration_transformed)\n",
    "log_r_hat_carl_calibration = calibration_predictions[:,0]\n",
    "s_hat_carl_calibration = s(np.exp(log_r_hat_carl_calibration))\n",
    "gradients_carl_calibration = calibration_predictions[:,1:]\n",
    "\n",
    "s_hat_carl_calibration_carlinput = np.hstack((s_hat_carl_calibration,s_hat_carl_calibration))\n",
    "y_carl_calibration_carlinput = np.hstack((np.zeros_like(s_hat_carl_calibration),\n",
    "                                          np.ones_like(s_hat_carl_calibration)))\n",
    "w_carl_calibration_carlinput = np.hstack((weights_calibration[theta,::], weights_calibration[theta1,::]))\n",
    "\n",
    "# Evaluate on test sample\n",
    "r_hat_carl_test = np.exp(regr.predict(X_test_transformed)[:,0])\n",
    "s_hat_carl_test = s(r_hat_carl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.0000320750428637 4.524177275058671\n"
     ]
    }
   ],
   "source": [
    "#Calculate KDE bandwidths\n",
    "grad_cov_grad = np.diag( gradients_carl_calibration[:,X_indices].dot(\n",
    "    cov_X.dot(gradients_carl_calibration[:,X_indices].T)))\n",
    "\n",
    "#mean_bw_nom = 1.5*1.5/11.4 * 1.5/2.37 * 1.5/1.8\n",
    "#mean_bw_den = 1.1*1.1/5.85 * 1.09/1.27 * 1.09 / 1.2\n",
    "#bandwidths = np.sqrt(grad_cov_grad)\n",
    "#bandwidths = np.clip(bandwidths, np.mean(bandwidths) / 100., np.mean(bandwidths) * 100.)\n",
    "#bandwidths_nom = mean_bw_nom / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_nom = np.clip(bandwidths_nom, 0.1,10.)\n",
    "#bandwidths_den = mean_bw_den / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_den = np.clip(bandwidths_den, 0.1,10.)\n",
    "\n",
    "bandwidths_carl = np.sqrt(grad_cov_grad)\n",
    "bandwidths_carl = np.clip(bandwidths_carl, 0., np.mean(bandwidths_carl) * 10.)\n",
    "bandwidths_carl /= np.mean(bandwidths_carl)\n",
    "bandwidths_carl = np.clip(bandwidths_carl, 0.1,10.)\n",
    "print(np.min(bandwidths_carl), np.mean(bandwidths_carl), np.max(bandwidths_carl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration method list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tuples (label1, label2, tool, mode, settings)\n",
    "# label is used in tables and plots\n",
    "# tool can be 'carl' for the built-in calibration methods and 'kde'\n",
    "#   for the adaptive KDE techniques (including the gradient one)\n",
    "# mode can be 'histogram', 'isotonic', ... for tool='carl', and\n",
    "#   'fixed', 'adaptive', 'grad' for tool='kde'\n",
    "# settings is a dict with settings passed to the Calibrator or GaussianKDE objects\n",
    "\n",
    "calibrations = [] \n",
    "\n",
    "def _add(*args):\n",
    "    calibrations.append(args)\n",
    "    \n",
    "# Uncalibrated\n",
    "_add('None', '', 'none', None, None)\n",
    "\n",
    "# carl histogram calibration\n",
    "_add('Histo', '30 fixed bins', 'carl', 'histo', {'bins':30, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '50 fixed bins', 'carl', 'histo', {'bins':50, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '100 fixed bins', 'carl', 'histo', {'bins':100, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '200 fixed bins', 'carl', 'histo', {'bins':100, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '30 variable bins', 'carl', 'histo', {'bins':30, 'independent_binning':False, 'variable_width':True})\n",
    "_add('', '50 variable bins', 'carl', 'histo', {'bins':50, 'independent_binning':False, 'variable_width':True})\n",
    "_add('', '100 variable bins', 'carl', 'histo', {'bins':100, 'independent_binning':False, 'variable_width':True})\n",
    "_add('', '200 variable bins', 'carl', 'histo', {'bins':100, 'independent_binning':False, 'variable_width':True})\n",
    "\n",
    "# other carl methods\n",
    "_add('Isotonic', '', 'carl', 'isotonic', {})\n",
    "\n",
    "# fixed KDE\n",
    "_add('KDE', 'fixed bandwidth', 'kde', 'fixed', {})\n",
    "\n",
    "# adaptive KDE\n",
    "_add('KDE', r'adaptive ($\\alpha=0.25$)', 'kde', 'adaptive', {'alpha':0.25})\n",
    "_add('', r'adaptive ($\\alpha=0.5$)', 'kde', 'adaptive', {'alpha':0.5})\n",
    "_add('', r'adaptive ($\\alpha=0.75$)', 'kde', 'adaptive', {'alpha':0.75})\n",
    "_add('', r'adaptive ($\\alpha=1.$)', 'kde', 'adaptive', {'alpha':1.})\n",
    "\n",
    "# grad-adaptive KDE\n",
    "_add('KDE', r'gradient-adaptive ($\\beta=0.1$)', 'kde', 'grad', {'factor':0.1})\n",
    "_add('', r'gradient-adaptive ($\\beta=0.2$)', 'kde', 'grad', {'factor':0.2})\n",
    "_add('', r'gradient-adaptive ($\\beta=0.5$)', 'kde', 'grad', {'factor':0.5})\n",
    "_add('', r'gradient-adaptive ($\\beta=1$)', 'kde', 'grad', {'factor':1.})\n",
    "_add('', r'gradient-adaptive ($\\beta=2$)', 'kde', 'grad', {'factor':2.})\n",
    "_add('', r'gradient-adaptive ($\\beta=5$)', 'kde', 'grad', {'factor':5.})\n",
    "_add('', r'gradient-adaptive ($\\beta=10$)', 'kde', 'grad', {'factor':10.})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_none(use_regression=False):\n",
    "    \n",
    "    # Optimal calibration curve\n",
    "    r_hat_eval = r(s_eval)\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        r_hat_test = r_hat_regr_test\n",
    "    else:\n",
    "        r_hat_test = r_hat_carl_test\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_eval, r_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_carl(mode, settings, use_regression=False):\n",
    "    \n",
    "    # Fit carl Calibrator object\n",
    "    if mode in ['histogram', 'histo']:\n",
    "        calibrator = HistogramCalibrator(**settings)\n",
    "    elif mode == 'isotonic':\n",
    "        calibrator = IsotonicCalibrator(**settings)\n",
    "    else:\n",
    "        raise ValueError('Unknown mode \"' + mode + '\"')\n",
    "    \n",
    "    if use_regression:\n",
    "        calibrator.fit(s_hat_regr_calibration_carlinput, y_regr_calibration_carlinput,\n",
    "                       sample_weight=w_regr_calibration_carlinput)\n",
    "    else:\n",
    "        calibrator.fit(s_hat_carl_calibration_carlinput, y_carl_calibration_carlinput,\n",
    "                       sample_weight=w_carl_calibration_carlinput)\n",
    "\n",
    "    # Calibration curve\n",
    "    s_hat_calibrated_eval = calibrator.predict(s_eval.reshape((-1,)))\n",
    "    r_hat_calibrated_eval = r(s_hat_calibrated_eval)\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        s_hat_calibrated_test = calibrator.predict(s_hat_regr_test.reshape((-1,)))\n",
    "    else:\n",
    "        s_hat_calibrated_test = calibrator.predict(s_hat_carl_test.reshape((-1,)))\n",
    "    r_hat_calibrated_test = r(s_hat_calibrated_test)\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_calibrated_eval, r_hat_calibrated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_kde(mode, settings, use_regression=False):\n",
    "    \n",
    "    # Settings\n",
    "    settings_ = settings\n",
    "    bandwidth_ = None\n",
    "    \n",
    "    if mode == 'fixed':\n",
    "        settings_ = {'alpha':None}\n",
    "        \n",
    "    elif mode == 'adaptive':\n",
    "        pass\n",
    "    \n",
    "    elif mode == 'grad':\n",
    "        factor = settings_.pop('factor', None)\n",
    "        bandwidth_ = factor * (bandwidths_regr if use_regression else bandwidths_carl)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    \n",
    "    pdf_nom = GaussianKDE(**settings_)\n",
    "    pdf_den = GaussianKDE(**settings_)\n",
    "\n",
    "    # Fit\n",
    "    if use_regression:\n",
    "        pdf_nom.fit(s_hat_regr_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta,::],\n",
    "                    bandwidth=bandwidth_)\n",
    "        pdf_den.fit(s_hat_regr_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta1,::],\n",
    "                    bandwidth=bandwidth_)\n",
    "    else:\n",
    "        pdf_nom.fit(s_hat_carl_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta,::],\n",
    "                    bandwidth=bandwidth_)\n",
    "        pdf_den.fit(s_hat_carl_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta1,::],\n",
    "                    bandwidth=bandwidth_)\n",
    "        \n",
    "    # Extract bandwidths (for debugging purposes)\n",
    "    if mode in ['adaptive', 'grad']:\n",
    "        actual_bandwidths_nom = 1. / pdf_nom._inv_loc_bw\n",
    "        actual_bandwidths_den = 1. / pdf_den._inv_loc_bw\n",
    "    else:\n",
    "        actual_bandwidths_nom = actual_bandwidths_den = np.ones_like(s_hat_carl_calibration)\n",
    "    \n",
    "    # Calibration curve\n",
    "    p_hat_nom_eval = pdf_nom.predict(s_eval)\n",
    "    p_hat_den_eval = pdf_den.predict(s_eval)\n",
    "    r_hat_calibrated_eval = p_hat_nom_eval / p_hat_den_eval\n",
    "    r_hat_calibrated_eval[p_hat_den_eval<=0.] = 1.\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        p_hat_nom_test = pdf_nom.predict(s_hat_regr_test.reshape((-1,1)))\n",
    "        p_hat_den_test = pdf_den.predict(s_hat_regr_test.reshape((-1,1)))\n",
    "        r_hat_calibrated_test = p_hat_nom_test / p_hat_den_test\n",
    "        r_hat_calibrated_test[p_hat_den_test<=0.] = 1.\n",
    "    else:\n",
    "        p_hat_nom_test = pdf_nom.predict(s_hat_carl_test.reshape((-1,1)))\n",
    "        p_hat_den_test = pdf_den.predict(s_hat_carl_test.reshape((-1,1)))\n",
    "        r_hat_calibrated_test = p_hat_nom_test / p_hat_den_test\n",
    "        r_hat_calibrated_test[p_hat_den_test<=0.] = 1.\n",
    "        \n",
    "    # Sanitize output\n",
    "    r_hat_calibrated_eval[np.invert(np.isfinite(r_hat_calibrated_eval))] = 1.\n",
    "    r_hat_calibrated_test[np.invert(np.isfinite(r_hat_calibrated_test))] = 1.\n",
    "    r_hat_calibrated_eval = np.clip(r_hat_calibrated_eval, np.exp(-10.), np.exp(10.))\n",
    "    r_hat_calibrated_test = np.clip(r_hat_calibrated_test, np.exp(-10.), np.exp(10.))\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_calibrated_eval, r_hat_calibrated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None  : MSEs = 0.006777024745008884 , 0.006777024745008884 in 8.100000002286833e-05 s\n",
      "Histo 30 fixed bins : MSEs = 0.3058480786006157 , 0.009055184789813705 in 0.016470999999967262 s\n",
      " 50 fixed bins : MSEs = 0.30907803402293377 , 0.007497971021512815 in 0.018483000000003358 s\n",
      " 100 fixed bins : MSEs = 0.3103523863016211 , 0.007171646029595624 in 0.020187999999961903 s\n",
      " 200 fixed bins : MSEs = 0.3103523863016211 , 0.007171646029595624 in 0.01793099999997594 s\n",
      " 30 variable bins : MSEs = 0.30945557008414837 , 0.02024720443266887 in 0.044408000000032644 s\n",
      " 50 variable bins : MSEs = 0.3096005635519163 , 0.012146235239453759 in 0.061776000000008935 s\n",
      " 100 variable bins : MSEs = 0.3107389107132852 , 0.00865970588109433 in 0.1155130000000213 s\n"
     ]
    }
   ],
   "source": [
    "r_hat_regr_calibrated_eval_list = []\n",
    "r_hat_carl_calibrated_eval_list = []\n",
    "r_hat_regr_calibrated_test_list = []\n",
    "r_hat_carl_calibrated_test_list = []\n",
    "mse_logr_regr_list = []\n",
    "mse_logr_carl_list = []\n",
    "\n",
    "for label1, label2, tool, mode, settings in calibrations:\n",
    "    \n",
    "    \n",
    "    # Do calibration\n",
    "    time_start = time.clock()\n",
    "    \n",
    "    if tool == 'none':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_none(True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_none(False)\n",
    "        \n",
    "    if tool == 'carl':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_carl(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_carl(mode, settings, False)\n",
    "        \n",
    "    elif tool == 'kde':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_kde(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_kde(mode, settings, False)\n",
    "        \n",
    "    calibration_time = time.clock() - time_start\n",
    "        \n",
    "    # Store results for plots\n",
    "    r_hat_regr_calibrated_eval_list.append(r_hat_regr_calibrated_eval)\n",
    "    r_hat_carl_calibrated_eval_list.append(r_hat_carl_calibrated_eval)\n",
    "    r_hat_regr_calibrated_test_list.append(r_hat_regr_calibrated_test)\n",
    "    r_hat_carl_calibrated_test_list.append(r_hat_carl_calibrated_test)\n",
    "    \n",
    "    # Metrics\n",
    "    mse_logr_regr = mean_squared_error(np.log(r_test), np.log(r_hat_regr_calibrated_test))\n",
    "    mse_logr_carl = mean_squared_error(np.log(r_test), np.log(r_hat_carl_calibrated_test))\n",
    "    mse_logr_regr_list.append(mse_logr_regr)\n",
    "    mse_logr_carl_list.append(mse_logr_carl)\n",
    "    \n",
    "    print(label1, label2, ': MSEs =', mse_logr_carl, ',', mse_logr_regr, 'in', calibration_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TablePrinter:\n",
    "    \n",
    "    def __init__(self, precisions, header=None):\n",
    "        \n",
    "        # Functions for metrics\n",
    "        self.n_metrics = len(precisions)\n",
    "        self.precisions = precisions\n",
    "        \n",
    "        # Total table and current block\n",
    "        self.table = ''\n",
    "        self.block_entries = []\n",
    "\n",
    "        # Formatting options\n",
    "        self.indent = '   '\n",
    "        self.col_sep = ' & '\n",
    "        self.end_row = r'\\\\'\n",
    "        self.midrule = r'\\midrule'\n",
    "        self.end_line = '\\n'\n",
    "        self.emphasis_begin = r'\\mathbf{'\n",
    "        self.emphasis_end = r'}'\n",
    "        \n",
    "        # Header\n",
    "        self.table = ''\n",
    "        if header is not None:\n",
    "            self.table += self.indent + header + self.end_row + self.end_line\n",
    "    \n",
    "    \n",
    "    def finalise_block(self):\n",
    "\n",
    "        # Find best performance\n",
    "        block_metrics = [line[2:] for line in self.block_entries]\n",
    "        block_metrics = np.array(block_metrics)\n",
    "        block_best = []\n",
    "        for i in range(self.n_metrics):\n",
    "            try:\n",
    "                block_best.append(np.nanargmin(block_metrics[:,i]))\n",
    "            except ValueError:\n",
    "                block_best.append(-1)\n",
    "\n",
    "        # Format entries\n",
    "        text = ''\n",
    "        for i, line in enumerate(self.block_entries):\n",
    "            \n",
    "            # Labels\n",
    "            text += self.indent + line[0] + self.col_sep + line[1] + self.col_sep\n",
    "            \n",
    "            # Metrics\n",
    "            for j in range(self.n_metrics):\n",
    "                if np.isfinite(line[j + 2]):\n",
    "                    text += self.format_number(line[j + 2], self.precisions[j], emphasize=(i == block_best[j]))\n",
    "                if j == len(line) - 3:\n",
    "                    text += self.end_row + self.end_line\n",
    "                else:\n",
    "                    text += self.col_sep\n",
    "\n",
    "        # Add to document and reset for next block\n",
    "        self.table += text\n",
    "        self.block_entries = []\n",
    "    \n",
    "    \n",
    "    def new_block(self):\n",
    "        self.finalise_block()\n",
    "        self.table += self.indent + self.midrule + self.end_line\n",
    "    \n",
    "    \n",
    "    def format_number(self,\n",
    "                      number,\n",
    "                      precision=2,\n",
    "                      trailing_zeros=True,\n",
    "                      fix_minus_zero=True,\n",
    "                      latex_math_mode=True,\n",
    "                      emphasize=False):\n",
    "        if precision == 0:\n",
    "            temp =  str(int(round(number,precision)))\n",
    "        elif trailing_zeros:\n",
    "            temp =  ('{:.' + str(precision) + 'f}').format(round(number,precision))\n",
    "        else:\n",
    "            temp =  str(round(number,precision))\n",
    "        if fix_minus_zero and len(temp) > 0:\n",
    "            if temp[0] == '-' and float(temp) == 0.:\n",
    "                temp = temp[1:]\n",
    "        if latex_math_mode:\n",
    "            if emphasize:\n",
    "                temp = '$\\mathbf{' + temp + '}$'\n",
    "            else:\n",
    "                temp = '$' + temp + '$'\n",
    "        elif emphasize:\n",
    "            temp = r'\\emph{' + temp + r'}'\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "    def add(self, col1, col2, values, folder='parameterized'):\n",
    "        \n",
    "        # Label columns\n",
    "        line = [col1, col2]\n",
    "        self.block_entries.append(values)\n",
    "    \n",
    "    \n",
    "    def print(self):\n",
    "        self.finalise_block()\n",
    "        return self.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = TablePrinter(precisions=[3,3])\n",
    "\n",
    "for i, ((label1, label2, _, _, _), mse_regr, mse_carl) in enumerate(zip(calibrations,\n",
    "                                                        mse_logr_regr_list, mse_logr_carl_list)):\n",
    "    \n",
    "    if i > 0 and label1 != '':\n",
    "        table.finalise_block\n",
    "        \n",
    "    table.add(label1, label2, [mse_regr, mse_carl])\n",
    "    \n",
    "table.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(s_hat_calibration[::20], fix_bandwidths_nom[::20], c='C2', s=15., alpha=0.2, label='fixed bw')\n",
    "plt.scatter(s_hat_calibration[::20], adaptive_bandwidths_nom[::20], c='C1', s=15., alpha=0.2, label='adaptive')\n",
    "plt.scatter(s_hat_calibration[::20], grad_bandwidths_nom[::20], c='C0', s=15., alpha=0.2, label='grad-adaptive')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$s$')\n",
    "plt.ylabel(r'$h/h_0$')\n",
    "plt.xlim(0.25,1.02)\n",
    "plt.ylim(-0.1,25.)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(s_hat_calibration[::20], fix_bandwidths_den[::20], c='C2', s=15., alpha=0.2, label='fixed bw')\n",
    "plt.scatter(s_hat_calibration[::20], adaptive_bandwidths_den[::20], c='C1', s=15., alpha=0.2, label='adaptive')\n",
    "plt.scatter(s_hat_calibration[::20], grad_bandwidths_den[::20], c='C0', s=15., alpha=0.2, label='grad-adaptive')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r'$s$')\n",
    "plt.ylabel(r'$h/h_0$')\n",
    "plt.xlim(0.25,1.02)\n",
    "plt.ylim(-0.1,25.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(adaptive_bandwidths_den), np.mean(grad_bandwidths_den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.,5.))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(si,p_nom_fix_eval, c='C2', ls=':', label='fixed bw')\n",
    "plt.plot(si,p_nom_adaptive_eval, c='C1', ls='--', label='adaptive')\n",
    "plt.plot(si,p_nom_grad_eval, c='C0', ls='-', label='grad-adaptive')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$p_{nom}(s)$')\n",
    "plt.legend()\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,13.)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(si,p_den_fix_eval, c='C2', ls=':', label='fixed bw')\n",
    "plt.plot(si,p_den_adaptive_eval, c='C1', ls='--', label='adaptive')\n",
    "plt.plot(si,p_den_grad_eval, c='C0', ls='-', label='grad-adaptive')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$p_{den}(s)$')\n",
    "plt.legend()\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,8.)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(si,r_fix_eval, c='C2', ls=':', label='fixed bw')\n",
    "plt.plot(si,r_adaptive_eval, c='C1', ls='--', label='adaptive')\n",
    "plt.plot(si,r_grad_eval, c='C0', ls='-', label='grad-adaptive')\n",
    "plt.xlabel('raw s')\n",
    "plt.ylabel('r(s)')\n",
    "plt.legend()\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,2.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects on r estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot([0.,2.],[0.,2.],ls='--', c='0.7')\n",
    "plt.scatter(r_test[::100], r_hat_test[::100], s=15., alpha=0.3)\n",
    "plt.xlim(0.,2.)\n",
    "plt.ylim(0.,2.)\n",
    "plt.xlabel('$r$ (truth)')\n",
    "plt.ylabel(r'$\\hat{r}$ (raw)')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot([0.,2.],[0.,2.],ls='--', c='0.7')\n",
    "plt.scatter(r_test[::100], r_hat_calibrated_fix_test[::100], s=15., alpha=0.3)\n",
    "plt.xlabel('$r$ (truth)')\n",
    "plt.ylabel(r'$\\hat{r}$ (calibrated, fixed bw)')\n",
    "plt.xlim(0.,2.)\n",
    "plt.ylim(0.,2.)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot([0.,2.],[0.,2.],ls='--', c='0.7')\n",
    "plt.scatter(r_test[::100], r_hat_calibrated_adaptive_test[::100], s=15., alpha=0.3)\n",
    "plt.xlabel('$r$ (truth)')\n",
    "plt.ylabel(r'$\\hat{r}$ (calibrated, adaptive bw)')\n",
    "plt.xlim(0.,2.)\n",
    "plt.ylim(0.,2.)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot([0.,2.],[0.,2.],ls='--', c='0.7')\n",
    "plt.scatter(r_test[::100], r_hat_calibrated_grad_test[::100], s=15., alpha=0.3)\n",
    "plt.xlabel('$r$ (truth)')\n",
    "plt.ylabel(r'$\\hat{r}$ (calibrated, grad-adaptive bw)')\n",
    "plt.xlim(0.,2.)\n",
    "plt.ylim(0.,2.)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(s_hat_test[::10], r_test[::10], c='0.6', s=5., alpha=0.2)\n",
    "\n",
    "plt.plot(si,r_fix_eval, c='C2', ls=':', lw=2., label='KDE, fixed bw')\n",
    "plt.plot(si,r_adaptive_eval, c='C1', ls='--', lw=2., label='KDE, adaptive')\n",
    "plt.plot(si,r_grad_eval, c='C0', ls='-', lw=2., label='KDE, grad-adaptive')\n",
    "\n",
    "plt.plot(si,r_histo_eval, c='C3', ls='-.', lw=2., label='histogram')\n",
    "plt.plot(si,r_iso_eval, c='C4', ls='--', lw=2., label='isotonic')\n",
    "\n",
    "plt.xlabel('raw s')\n",
    "plt.ylabel('r(s)')\n",
    "plt.legend()\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,2.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KDE, fixed bw:', mean_squared_error())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:higgs_inference]",
   "language": "python",
   "name": "conda-env-higgs_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
