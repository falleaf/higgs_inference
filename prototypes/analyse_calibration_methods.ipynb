{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import lmfit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Concatenate, Multiply, Reshape, ActivityRegularization\n",
    "from keras import losses, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "from carl.learning.calibration import HistogramCalibrator, IsotonicCalibrator, SigmoidCalibrator\n",
    "\n",
    "from awkde import GaussianKDE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation between s and r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(r):\n",
    "    return 1./(1. + r)\n",
    "\n",
    "def r(s):\n",
    "    return (1.-s)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "theta1 = 708\n",
    "theta = 9\n",
    "\n",
    "#X to take into account\n",
    "X_indices_import = [0,1,2,3,39,40,41] # j1 momentum + delta eta, delta phi, mjj\n",
    "n_features = len(X_indices_import)\n",
    "X_indices = list(range(n_features))\n",
    "\n",
    "# Data\n",
    "data_dir = '../data'\n",
    "figure_dir = '../figures/calibration'\n",
    "\n",
    "X_train = np.load(data_dir + '/unweighted_events/X_train_point_by_point_' + str(theta) + '.npy')[:,X_indices_import]\n",
    "y_train = np.load(data_dir + '/unweighted_events/y_train_point_by_point_' + str(theta) + '.npy')\n",
    "r_train = np.load(data_dir + '/unweighted_events/r_train_point_by_point_' + str(theta) + '.npy')\n",
    "\n",
    "X_calibration = np.load(data_dir + '/unweighted_events/X_calibration.npy')[::10, X_indices_import]\n",
    "weights_calibration = np.load(\n",
    "    data_dir + '/unweighted_events/weights_calibration.npy')[:,::10]\n",
    "\n",
    "X_test = np.load(data_dir + '/unweighted_events/X_test.npy')[:,X_indices_import]\n",
    "r_test = np.load(data_dir + '/unweighted_events/r_test.npy')[theta]\n",
    "    \n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(X_train, dtype=np.float64))\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_calibration_transformed = scaler.transform(X_calibration)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "# keras requires training data with the same output as the NN\n",
    "log_r_gradients_train = np.zeros((r_train.shape[0], 43))\n",
    "log_r_gradients_train[:,0] = np.log(r_train)\n",
    "y_gradients_train = np.zeros((r_train.shape[0], 43))\n",
    "y_gradients_train[:,0] = y_train\n",
    "\n",
    "# Finally, s points for the calibration curves\n",
    "s_eval = np.linspace(0., 1., 200).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix over a subset of the X for the grad-adaptive approach\n",
    "cov_X = np.cov(X_train_transformed[::,X_indices], rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features = 42\n",
    "n_features = X_train_transformed.shape[1]\n",
    "\n",
    "def stack_layer(layers):\n",
    "    def f(x):\n",
    "        for k in range(len(layers)):\n",
    "            x = layers[k](x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def hidden_layers(n,\n",
    "                  hidden_layer_size=100,\n",
    "                  activation='tanh',\n",
    "                  dropout_prob=0.0):\n",
    "    r = []\n",
    "    for k in range(n):\n",
    "        if dropout_prob > 0.:\n",
    "            s = stack_layer([\n",
    "                Dropout(dropout_prob),\n",
    "                Dense(hidden_layer_size, activation=activation)\n",
    "            ])\n",
    "        else:\n",
    "            s = stack_layer([Dense(hidden_layer_size, activation=activation)])\n",
    "        r.append(s)\n",
    "    return stack_layer(r)\n",
    "\n",
    "def loss_function_regression(y_true, y_pred):\n",
    "    return losses.mean_squared_error(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "def make_regressor(n_hidden_layers=3,\n",
    "                   hidden_layer_size=100,\n",
    "                   activation='tanh',\n",
    "                   dropout_prob=0.0):\n",
    "\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(n_features,))\n",
    "\n",
    "    # Network\n",
    "    hidden_layer = Dense(hidden_layer_size, activation=activation)(input_layer)\n",
    "    if n_hidden_layers > 1:\n",
    "        hidden_layer_ = hidden_layers(n_hidden_layers - 1,\n",
    "                                      hidden_layer_size=hidden_layer_size,\n",
    "                                      activation=activation,\n",
    "                                      dropout_prob=dropout_prob)\n",
    "        hidden_layer = hidden_layer_(hidden_layer)\n",
    "    log_r_hat_layer = Dense(1, activation='linear')(hidden_layer)\n",
    "    s_hat_layer = Lambda(lambda x : 1. / 1. + x)(log_r_hat_layer)\n",
    "\n",
    "    # gradients with respect to x\n",
    "    gradient_layer = Lambda(lambda x: K.gradients(x[0], x[1])[0],\n",
    "                            output_shape=(n_features,))([s_hat_layer, input_layer])\n",
    "\n",
    "    # Combine outputs\n",
    "    output_layer = Concatenate()([log_r_hat_layer, gradient_layer])\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function_regression,\n",
    "                  optimizer=optimizers.Adam(clipnorm=1.))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "800000/800000 [==============================] - 35s 44us/step - loss: 0.0889 - val_loss: 0.1698\n",
      "Epoch 2/20\n",
      "800000/800000 [==============================] - 34s 43us/step - loss: 0.0749 - val_loss: 0.1614\n",
      "Epoch 3/20\n",
      "800000/800000 [==============================] - 35s 43us/step - loss: 0.0724 - val_loss: 0.1580\n",
      "Epoch 4/20\n",
      "800000/800000 [==============================] - 35s 44us/step - loss: 0.0709 - val_loss: 0.1572\n",
      "Epoch 5/20\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0701 - val_loss: 0.1578\n",
      "Epoch 6/20\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0692 - val_loss: 0.1507\n",
      "Epoch 7/20\n",
      "800000/800000 [==============================] - 34s 43us/step - loss: 0.0686 - val_loss: 0.1619\n",
      "Epoch 8/20\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0683 - val_loss: 0.1487\n",
      "Epoch 9/20\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0678 - val_loss: 0.1449\n",
      "Epoch 10/20\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0673 - val_loss: 0.1484\n",
      "Epoch 11/20\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0670 - val_loss: 0.1570\n",
      "Epoch 12/20\n",
      "800000/800000 [==============================] - 35s 43us/step - loss: 0.0666 - val_loss: 0.1493\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce7a49e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train PbP regression\n",
    "regr = KerasRegressor(lambda: make_regressor(n_hidden_layers=2),\n",
    "                     epochs=20, verbose=1, validation_split=0.2,\n",
    "                     callbacks=[EarlyStopping(verbose=1, patience=3)])\n",
    "\n",
    "regr.fit(X_train_transformed, log_r_gradients_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99955/99955 [==============================] - 1s 11us/step\n",
      "48238/48238 [==============================] - 1s 16us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on calibration sample\n",
    "calibration_predictions = regr.predict(X_calibration_transformed)\n",
    "log_r_hat_regr_calibration = calibration_predictions[:,0]\n",
    "s_hat_regr_calibration = s(np.exp(log_r_hat_regr_calibration))\n",
    "gradients_regr_calibration = calibration_predictions[:,1:]\n",
    "\n",
    "s_hat_regr_calibration_carlinput = np.hstack((s_hat_regr_calibration,s_hat_regr_calibration))\n",
    "y_regr_calibration_carlinput = np.hstack((np.zeros_like(s_hat_regr_calibration), np.ones_like(s_hat_regr_calibration)))\n",
    "w_regr_calibration_carlinput = np.hstack((weights_calibration[theta,::], weights_calibration[theta1,::]))\n",
    "\n",
    "# Evaluate on test sample\n",
    "r_hat_regr_test = np.exp(regr.predict(X_test_transformed)[:,0])\n",
    "s_hat_regr_test = s(r_hat_regr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.0003328491679455 10.0\n"
     ]
    }
   ],
   "source": [
    "#Calculate KDE bandwidths\n",
    "\n",
    "# Don't do this, will make N_x^2 * N_components large matrix in memory\n",
    "#grad_cov_grad = np.diag( gradients_regr_calibration[:,X_indices].dot(\n",
    "#    cov_X.dot(gradients_regr_calibration[:,X_indices].T)))\n",
    "\n",
    "# Alternative:\n",
    "grad_cov = cov_X.dot(gradients_regr_calibration[:,X_indices].T)\n",
    "grad_cov_grad = np.sum(grad_cov[:,:] * gradients_regr_calibration[:,X_indices].T, axis=0)\n",
    "\n",
    "# Old code:\n",
    "#mean_bw_nom = 1.5*1.5/11.4 * 1.5/2.37 * 1.5/1.8\n",
    "#mean_bw_den = 1.1*1.1/5.85 * 1.09/1.27 * 1.09 / 1.2\n",
    "#bandwidths = np.sqrt(grad_cov_grad)\n",
    "#bandwidths = np.clip(bandwidths, np.mean(bandwidths) / 100., np.mean(bandwidths) * 100.)\n",
    "#bandwidths_nom = mean_bw_nom / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_nom = np.clip(bandwidths_nom, 0.1,10.)\n",
    "#bandwidths_den = mean_bw_den / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_den = np.clip(bandwidths_den, 0.1,10.)\n",
    "\n",
    "# Get bandwidths and clip\n",
    "bandwidths_regr = np.sqrt(np.copy(grad_cov_grad))\n",
    "bandwidths_regr = np.clip(bandwidths_regr, 0., np.mean(bandwidths_regr) * 10.)\n",
    "bandwidths_regr /= np.mean(bandwidths_regr)\n",
    "bandwidths_regr = np.clip(bandwidths_regr, 0.1,10.)\n",
    "print(np.min(bandwidths_regr), np.mean(bandwidths_regr), np.max(bandwidths_regr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train carl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_carl(y_true, y_pred):\n",
    "    return losses.binary_crossentropy(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "def make_classifier(n_hidden_layers=3,\n",
    "                   hidden_layer_size=100,\n",
    "                   activation='tanh',\n",
    "                   dropout_prob=0.0):\n",
    "\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(n_features,))\n",
    "\n",
    "    # Network\n",
    "    hidden_layer = Dense(hidden_layer_size, activation=activation)(input_layer)\n",
    "    if n_hidden_layers > 1:\n",
    "        hidden_layer_ = hidden_layers(n_hidden_layers - 1,\n",
    "                                      hidden_layer_size=hidden_layer_size,\n",
    "                                      activation=activation,\n",
    "                                      dropout_prob=dropout_prob)\n",
    "        hidden_layer = hidden_layer_(hidden_layer)\n",
    "        \n",
    "    log_r_hat_layer = Dense(1, activation='linear')(hidden_layer)\n",
    "    r_hat_layer = Lambda(lambda x: K.exp(x))(log_r_hat_layer)\n",
    "    s_hat_layer = Lambda(lambda x: 1./(1. + x))(r_hat_layer)\n",
    "\n",
    "    # gradients with respect to x\n",
    "    gradient_layer = Lambda(lambda x: K.gradients(x[0], x[1])[0],\n",
    "                            output_shape=(n_features,))([s_hat_layer, input_layer])\n",
    "\n",
    "    # Combine outputs\n",
    "    output_layer = Concatenate()([s_hat_layer, gradient_layer])\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function_carl,\n",
    "                  optimizer=optimizers.Adam(clipnorm=1.))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.6281 - val_loss: 0.9490\n",
      "Epoch 2/20\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.6259 - val_loss: 0.9374\n",
      "Epoch 3/20\n",
      "800000/800000 [==============================] - 44s 56us/step - loss: 0.6256 - val_loss: 0.9437\n",
      "Epoch 4/20\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.6254 - val_loss: 0.8943\n",
      "Epoch 5/20\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.6254 - val_loss: 0.8839\n",
      "Epoch 6/20\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.6253 - val_loss: 0.9331\n",
      "Epoch 7/20\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.6251 - val_loss: 0.9445\n",
      "Epoch 8/20\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.6251 - val_loss: 0.9145\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce86754a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train score regression\n",
    "clf = KerasRegressor(lambda: make_classifier(n_hidden_layers=2),\n",
    "                     epochs=20, verbose=1, validation_split=0.2,\n",
    "                     callbacks=[EarlyStopping(verbose=1, patience=3)])\n",
    "\n",
    "clf.fit(X_train_transformed, y_gradients_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99955/99955 [==============================] - 1s 14us/step\n",
      "48238/48238 [==============================] - 1s 13us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on calibration sample\n",
    "calibration_predictions = clf.predict(X_calibration_transformed)\n",
    "s_hat_carl_calibration = calibration_predictions[:,0]\n",
    "gradients_carl_calibration = calibration_predictions[:,1:]\n",
    "\n",
    "s_hat_carl_calibration_carlinput = np.hstack((s_hat_carl_calibration,s_hat_carl_calibration))\n",
    "y_carl_calibration_carlinput = np.hstack((np.zeros_like(s_hat_carl_calibration),\n",
    "                                          np.ones_like(s_hat_carl_calibration)))\n",
    "w_carl_calibration_carlinput = np.hstack((weights_calibration[theta,::], weights_calibration[theta1,::]))\n",
    "\n",
    "# Evaluate on test sample\n",
    "s_hat_carl_test = clf.predict(X_test_transformed)[:,0]\n",
    "r_hat_carl_test = r(s_hat_carl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.002799749736796 6.013009264737933\n"
     ]
    }
   ],
   "source": [
    "# Don't do this, will make N_x^2 * N_components large matrix in memory\n",
    "#grad_cov_grad = np.diag( gradients_regr_calibration[:,X_indices].dot(\n",
    "#    cov_X.dot(gradients_regr_calibration[:,X_indices].T)))\n",
    "\n",
    "# Alternative:\n",
    "grad_cov = cov_X.dot(gradients_carl_calibration[:,X_indices].T)\n",
    "grad_cov_grad = np.sum(grad_cov[:,:] * gradients_carl_calibration[:,X_indices].T, axis=0)\n",
    "\n",
    "# Old code:\n",
    "#mean_bw_nom = 1.5*1.5/11.4 * 1.5/2.37 * 1.5/1.8\n",
    "#mean_bw_den = 1.1*1.1/5.85 * 1.09/1.27 * 1.09 / 1.2\n",
    "#bandwidths = np.sqrt(grad_cov_grad)\n",
    "#bandwidths = np.clip(bandwidths, np.mean(bandwidths) / 100., np.mean(bandwidths) * 100.)\n",
    "#bandwidths_nom = mean_bw_nom / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_nom = np.clip(bandwidths_nom, 0.1,10.)\n",
    "#bandwidths_den = mean_bw_den / np.mean(bandwidths) * bandwidths\n",
    "#bandwidths_den = np.clip(bandwidths_den, 0.1,10.)\n",
    "\n",
    "# Get bandwidths and clip\n",
    "bandwidths_carl = np.sqrt(np.copy(grad_cov_grad))\n",
    "bandwidths_carl = np.clip(bandwidths_carl, 0., np.mean(bandwidths_carl) * 10.)\n",
    "bandwidths_carl /= np.mean(bandwidths_carl)\n",
    "bandwidths_carl = np.clip(bandwidths_carl, 0.1,10.)\n",
    "print(np.min(bandwidths_carl), np.mean(bandwidths_carl), np.max(bandwidths_carl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34212976 0.40427862 0.27189401 ... 0.90825276 0.14757133 0.3195123 ]\n",
      "[0.193929   0.66614473 0.1        ... 1.04826063 0.11897654 0.14002791]\n"
     ]
    }
   ],
   "source": [
    "print(bandwidths_regr)\n",
    "print(bandwidths_carl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration method list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tuples (label1, label2, tool, mode, settings)\n",
    "# label is used in tables and plots\n",
    "# tool can be 'carl' for the built-in calibration methods and 'kde'\n",
    "#   for the adaptive KDE techniques (including the gradient one)\n",
    "# mode can be 'histogram', 'isotonic', ... for tool='carl', and\n",
    "#   'fixed', 'adaptive', 'grad' for tool='kde'\n",
    "# settings is a dict with settings passed to the Calibrator or GaussianKDE objects\n",
    "\n",
    "calibrations = [] \n",
    "\n",
    "def _add(*args):\n",
    "    calibrations.append(args)\n",
    "    \n",
    "# Uncalibrated\n",
    "_add('No calibration', '', 'none', None, None)\n",
    "\n",
    "# Histogram calibration\n",
    "_add('Histogram', '25 equidistant bins', 'carl', 'histo',\n",
    "     {'bins':25, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '50 equidistant bins', 'carl', 'histo',\n",
    "     {'bins':50, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '100 equidistant bins', 'carl', 'histo',\n",
    "     {'bins':100, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '200 equidistant bins', 'carl', 'histo',\n",
    "     {'bins':200, 'independent_binning':False, 'variable_width':False})\n",
    "_add('', '500 equidistant bins', 'carl', 'histo',\n",
    "     {'bins':500, 'independent_binning':False, 'variable_width':False})\n",
    "\n",
    "_add('', '25 equidistant bins, linear interpolation', 'carl', 'histo',\n",
    "     {'bins':25, 'independent_binning':False, 'variable_width':False, 'interpolation':'linear'})\n",
    "_add('', '50 equidistant bins, linear interpolation', 'carl', 'histo',\n",
    "     {'bins':50, 'independent_binning':False, 'variable_width':False, 'interpolation':'linear'})\n",
    "_add('', '100 equidistant bins, linear interpolation', 'carl', 'histo',\n",
    "     {'bins':100, 'independent_binning':False, 'variable_width':False, 'interpolation':'linear'})\n",
    "_add('', '200 equidistant bins, linear interpolation', 'carl', 'histo',\n",
    "     {'bins':200, 'independent_binning':False, 'variable_width':False, 'interpolation':'linear'})\n",
    "_add('', '500 equidistant bins, linear interpolation', 'carl', 'histo',\n",
    "     {'bins':500, 'independent_binning':False, 'variable_width':False, 'interpolation':'linear'})\n",
    "\n",
    "_add('', '25 equidistant bins, spline interpolation', 'carl', 'histo',\n",
    "     {'bins':25, 'independent_binning':False, 'variable_width':False, 'interpolation':'quadratic'})\n",
    "_add('', '50 equidistant bins, spline interpolation', 'carl', 'histo',\n",
    "     {'bins':50, 'independent_binning':False, 'variable_width':False, 'interpolation':'quadratic'})\n",
    "_add('', '100 equidistant bins, spline interpolation', 'carl', 'histo',\n",
    "     {'bins':100, 'independent_binning':False, 'variable_width':False, 'interpolation':'quadratic'})\n",
    "_add('', '200 equidistant bins, spline interpolation', 'carl', 'histo',\n",
    "     {'bins':200, 'independent_binning':False, 'variable_width':False, 'interpolation':'quadratic'})\n",
    "_add('', '500 equidistant bins, spline interpolation', 'carl', 'histo',\n",
    "     {'bins':500, 'independent_binning':False, 'variable_width':False, 'interpolation':'quadratic'})\n",
    "\n",
    "_add('', '25 variable bins', 'carl', 'histo',\n",
    "     {'bins':25, 'independent_binning':True, 'variable_width':True})\n",
    "_add('', '50 variable bins', 'carl', 'histo',\n",
    "     {'bins':50, 'independent_binning':True, 'variable_width':True})\n",
    "_add('', '100 variable bins', 'carl', 'histo',\n",
    "     {'bins':100, 'independent_binning':True, 'variable_width':True})\n",
    "_add('', '200 variable bins', 'carl', 'histo',\n",
    "     {'bins':200, 'independent_binning':True, 'variable_width':True})\n",
    "_add('', '500 variable bins', 'carl', 'histo',\n",
    "     {'bins':500, 'independent_binning':True, 'variable_width':True})\n",
    "\n",
    "# Isotonic\n",
    "_add('Isotonic', 'no interpolation', 'carl', 'isotonic', {'interpolation':False})\n",
    "_add('', 'linear interpolation', 'carl', 'isotonic', {'interpolation':True})\n",
    "\n",
    "# Sigmoid\n",
    "_add('Parametric', 'sigmoid', 'carl', 'sigmoid', {})\n",
    "\n",
    "# Analytic fits\n",
    "_add('', '', 'fit', '',\n",
    "     {'bins':100, 'independent_binning':False, 'variable_width':False})\n",
    "\n",
    "# fixed KDE\n",
    "_add('KDE', 'fixed bandwidth', 'kde', 'fixed', {})\n",
    "\n",
    "# adaptive KDE\n",
    "_add('', r'adaptive ($\\alpha=0.25$)', 'kde', 'adaptive', {'alpha':0.25})\n",
    "_add('', r'adaptive ($\\alpha=0.5$)', 'kde', 'adaptive', {'alpha':0.5})\n",
    "_add('', r'adaptive ($\\alpha=0.75$)', 'kde', 'adaptive', {'alpha':0.75})\n",
    "_add('', r'adaptive ($\\alpha=1$)', 'kde', 'adaptive', {'alpha':1.})\n",
    "\n",
    "# grad-adaptive KDE\n",
    "_add('KDE', r'grad-adaptive ($\\beta=0.2$)', 'kde', 'grad', {'factor':0.2})\n",
    "_add('', r'grad-adaptive ($\\beta=0.5$)', 'kde', 'grad', {'factor':0.5})\n",
    "_add('', r'grad-adaptive ($\\beta=1$)', 'kde', 'grad', {'factor':1.})\n",
    "_add('', r'grad-adaptive ($\\beta=2$)', 'kde', 'grad', {'factor':2.})\n",
    "_add('', r'grad-adaptive ($\\beta_{\\text{num}} = 1.66$, $\\beta_{\\text{den}} = 1.13$)',\n",
    "     'kde', 'grad', {'factor':'auto'})\n",
    "\n",
    "plot_labels = []\n",
    "last_label1 = ''\n",
    "for cal in calibrations:\n",
    "    label2_string = '' if cal[1] == '' else ', ' + cal[1]\n",
    "    if cal[0] == '':\n",
    "        plot_labels.append(last_label1 + label2_string)\n",
    "    else:\n",
    "        plot_labels.append(cal[0] + label2_string)\n",
    "        last_label1 = cal[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_none(use_regression=False):\n",
    "    \n",
    "    # Optimal calibration curve\n",
    "    r_hat_eval = r(s_eval)\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        r_hat_test = r_hat_regr_test\n",
    "    else:\n",
    "        r_hat_test = r_hat_carl_test\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_eval, r_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_carl(mode, settings, use_regression=False):\n",
    "    \n",
    "    # Fit carl Calibrator object\n",
    "    if mode in ['histogram', 'histo']:\n",
    "        calibrator = HistogramCalibrator(**settings)\n",
    "    elif mode == 'isotonic':\n",
    "        calibrator = IsotonicCalibrator(**settings)\n",
    "    elif mode == 'sigmoid':\n",
    "        calibrator = SigmoidCalibrator(**settings)\n",
    "    else:\n",
    "        raise ValueError('Unknown mode \"' + mode + '\"')\n",
    "    \n",
    "    if use_regression:\n",
    "        calibrator.fit(s_hat_regr_calibration_carlinput, y_regr_calibration_carlinput,\n",
    "                       sample_weight=w_regr_calibration_carlinput)\n",
    "    else:\n",
    "        calibrator.fit(s_hat_carl_calibration_carlinput, y_carl_calibration_carlinput,\n",
    "                       sample_weight=w_carl_calibration_carlinput)\n",
    "\n",
    "    # Calibration curve\n",
    "    s_hat_calibrated_eval = calibrator.predict(s_eval.reshape((-1,)))\n",
    "    r_hat_calibrated_eval = r(s_hat_calibrated_eval)\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        s_hat_calibrated_test = calibrator.predict(s_hat_regr_test.reshape((-1,)))\n",
    "    else:\n",
    "        s_hat_calibrated_test = calibrator.predict(s_hat_carl_test.reshape((-1,)))\n",
    "    r_hat_calibrated_test = r(s_hat_calibrated_test)\n",
    "        \n",
    "    # Sanitize output\n",
    "    r_hat_calibrated_eval[np.invert(np.isfinite(r_hat_calibrated_eval))] = 1.\n",
    "    r_hat_calibrated_test[np.invert(np.isfinite(r_hat_calibrated_test))] = 1.\n",
    "    r_hat_calibrated_eval = np.clip(r_hat_calibrated_eval, np.exp(-10.), np.exp(10.))\n",
    "    r_hat_calibrated_test = np.clip(r_hat_calibrated_test, np.exp(-10.), np.exp(10.))\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_calibrated_eval, r_hat_calibrated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_fit(mode, settings, use_regression=False):\n",
    "    \n",
    "    # Calibration function\n",
    "    def calibration_function(x, alpha):\n",
    "        return (1. - alpha) * x / (alpha * (1. - x) + (1. - alpha) * x)\n",
    "    \n",
    "    # First, fit histogram\n",
    "    calibrator = HistogramCalibrator(**settings)\n",
    "    \n",
    "    if use_regression:\n",
    "        calibrator.fit(s_hat_regr_calibration_carlinput, y_regr_calibration_carlinput,\n",
    "                       sample_weight=w_regr_calibration_carlinput)\n",
    "        s_hat_calibrated_calibration = calibrator.predict(s_hat_regr_calibration)\n",
    "    else:\n",
    "        calibrator.fit(s_hat_carl_calibration_carlinput, y_carl_calibration_carlinput,\n",
    "                       sample_weight=w_carl_calibration_carlinput)\n",
    "        s_hat_calibrated_calibration = calibrator.predict(s_hat_carl_calibration)\n",
    "    \n",
    "    ##Fit calibration function to (s raw, s calibrated)\n",
    "    #popt, pcov = curve_fit(calibration_function,\n",
    "    #                       s_hat_regr_calibration if use_regression else s_hat_carl_calibration,\n",
    "    #                       s_hat_calibrated_calibration)\n",
    "    #best_alpha = popt[0]\n",
    "    #print('alpha =', best_alpha)\n",
    "    \n",
    "    # Fit calibration function to (s raw, s calibrated)\n",
    "    model = lmfit.Model(calibration_function)\n",
    "    params = model.make_params()\n",
    "    params['alpha'].set(value=0.5,min=0.05,max=0.95,brute_step=0.001)\n",
    "    result = model.fit(s_hat_calibrated_calibration,\n",
    "                       params,\n",
    "                       x=s_hat_regr_calibration if use_regression else s_hat_carl_calibration,\n",
    "                       method='brute',\n",
    "                       fit_kws={})\n",
    "    #print(result.fit_report())\n",
    "    best_alpha = result.best_values['alpha']\n",
    "\n",
    "    # Calibration curve\n",
    "    s_hat_calibrated_eval = calibration_function(s_eval.reshape((-1,)), best_alpha)\n",
    "    r_hat_calibrated_eval = r(s_hat_calibrated_eval)\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        s_hat_calibrated_test = calibration_function(s_hat_regr_test.reshape((-1,)), best_alpha)\n",
    "    else:\n",
    "        s_hat_calibrated_test = calibration_function(s_hat_carl_test.reshape((-1,)), best_alpha)\n",
    "    r_hat_calibrated_test = r(s_hat_calibrated_test)\n",
    "        \n",
    "    # Sanitize output\n",
    "    r_hat_calibrated_eval[np.invert(np.isfinite(r_hat_calibrated_eval))] = 1.\n",
    "    r_hat_calibrated_test[np.invert(np.isfinite(r_hat_calibrated_test))] = 1.\n",
    "    r_hat_calibrated_eval = np.clip(r_hat_calibrated_eval, np.exp(-10.), np.exp(10.))\n",
    "    r_hat_calibrated_test = np.clip(r_hat_calibrated_test, np.exp(-10.), np.exp(10.))\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_calibrated_eval, r_hat_calibrated_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_kde(mode, settings, use_regression=False):\n",
    "    \n",
    "    # Settings\n",
    "    settings_ = copy.deepcopy(settings)\n",
    "    bandwidth_num = None\n",
    "    bandwidth_den = None\n",
    "    \n",
    "    if mode == 'fixed':\n",
    "        settings_ = {'alpha':None}\n",
    "        \n",
    "    elif mode == 'adaptive':\n",
    "        pass\n",
    "    \n",
    "    elif mode == 'grad':\n",
    "        factor = settings_.pop('factor', 1.)\n",
    "        if factor=='auto':\n",
    "            bandwidth_num = 1.66 * (bandwidths_regr if use_regression else bandwidths_carl)\n",
    "            bandwidth_den = 1.13 * (bandwidths_regr if use_regression else bandwidths_carl)\n",
    "        else:\n",
    "            bandwidth_num = factor * (bandwidths_regr if use_regression else bandwidths_carl)\n",
    "            bandwidth_den = factor * (bandwidths_regr if use_regression else bandwidths_carl)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    \n",
    "    pdf_nom = GaussianKDE(**settings_)\n",
    "    pdf_den = GaussianKDE(**settings_)\n",
    "\n",
    "    # Fit\n",
    "    if use_regression:\n",
    "        pdf_nom.fit(s_hat_regr_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta,::],\n",
    "                    bandwidth=bandwidth_num)\n",
    "        pdf_den.fit(s_hat_regr_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta1,::],\n",
    "                    bandwidth=bandwidth_den)\n",
    "    else:\n",
    "        pdf_nom.fit(s_hat_carl_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta,::],\n",
    "                    bandwidth=bandwidth_num)\n",
    "        pdf_den.fit(s_hat_carl_calibration.reshape((-1,1)),\n",
    "                    weights=weights_calibration[theta1,::],\n",
    "                    bandwidth=bandwidth_den)\n",
    "        \n",
    "    # Extract bandwidths (for debugging purposes)\n",
    "    if mode in ['adaptive', 'grad']:\n",
    "        actual_bandwidths_nom = 1. / pdf_nom._inv_loc_bw\n",
    "        actual_bandwidths_den = 1. / pdf_den._inv_loc_bw\n",
    "    else:\n",
    "        actual_bandwidths_nom = actual_bandwidths_den = np.ones_like(s_hat_carl_calibration)\n",
    "    \n",
    "    print('Bandwidth means:', np.mean(actual_bandwidths_nom), np.mean(actual_bandwidths_den))\n",
    "    \n",
    "    # Calibration curve\n",
    "    p_hat_nom_eval = pdf_nom.predict(s_eval)\n",
    "    p_hat_den_eval = pdf_den.predict(s_eval)\n",
    "    r_hat_calibrated_eval = p_hat_nom_eval / p_hat_den_eval\n",
    "    r_hat_calibrated_eval[p_hat_den_eval<=0.] = 1.\n",
    "    \n",
    "    # Test sample\n",
    "    if use_regression:\n",
    "        p_hat_nom_test = pdf_nom.predict(s_hat_regr_test.reshape((-1,1)))\n",
    "        p_hat_den_test = pdf_den.predict(s_hat_regr_test.reshape((-1,1)))\n",
    "        r_hat_calibrated_test = p_hat_nom_test / p_hat_den_test\n",
    "        r_hat_calibrated_test[p_hat_den_test<=0.] = 1.\n",
    "    else:\n",
    "        p_hat_nom_test = pdf_nom.predict(s_hat_carl_test.reshape((-1,1)))\n",
    "        p_hat_den_test = pdf_den.predict(s_hat_carl_test.reshape((-1,1)))\n",
    "        r_hat_calibrated_test = p_hat_nom_test / p_hat_den_test\n",
    "        r_hat_calibrated_test[p_hat_den_test<=0.] = 1.\n",
    "        \n",
    "    # Sanitize output\n",
    "    r_hat_calibrated_eval[np.invert(np.isfinite(r_hat_calibrated_eval))] = 1.\n",
    "    r_hat_calibrated_test[np.invert(np.isfinite(r_hat_calibrated_test))] = 1.\n",
    "    r_hat_calibrated_eval = np.clip(r_hat_calibrated_eval, np.exp(-10.), np.exp(10.))\n",
    "    r_hat_calibrated_test = np.clip(r_hat_calibrated_test, np.exp(-10.), np.exp(10.))\n",
    "    \n",
    "    # Return results\n",
    "    return(r_hat_calibrated_eval, r_hat_calibrated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 No calibration  : MSEs = 0.27199469643010854 , 0.024088222957109065 in 3.199999991920777e-05 s\n",
      "1 Histogram 25 equidistant bins : MSEs = 0.040066611870105344 , 0.027747168001929202 in 0.028011999999989712 s\n",
      "2  50 equidistant bins : MSEs = 0.038332531089670475 , 0.025882884827298915 in 0.03664600000001883 s\n",
      "3  100 equidistant bins : MSEs = 0.03811305574550204 , 0.025508523581620913 in 0.0352060000000165 s\n",
      "4  200 equidistant bins : MSEs = 0.03824653942834418 , 0.025464332462159525 in 0.04098699999985911 s\n",
      "5  500 equidistant bins : MSEs = 0.03903620980228544 , 0.026423507472111758 in 0.04686800000013136 s\n",
      "6  25 equidistant bins, linear interpolation : MSEs = 0.03880217758729727 , 0.027764868882685218 in 0.039027999999916574 s\n",
      "7  50 equidistant bins, linear interpolation : MSEs = 0.03776968344824742 , 0.02609820985803524 in 0.04572399999983645 s\n",
      "8  100 equidistant bins, linear interpolation : MSEs = 0.03776789896347413 , 0.02535175780294506 in 0.042271999999911714 s\n",
      "9  200 equidistant bins, linear interpolation : MSEs = 0.03797710553617884 , 0.025361778415687465 in 0.046923999999989974 s\n",
      "10  500 equidistant bins, linear interpolation : MSEs = 0.03844187408607524 , 0.025950350813981157 in 0.05544199999985722 s\n",
      "11  25 equidistant bins, spline interpolation : MSEs = 0.03842366060381378 , 0.02723515789168133 in 0.10978999999997541 s\n",
      "12  50 equidistant bins, spline interpolation : MSEs = 0.03773224699618172 , 0.025817472042840475 in 0.03362499999980173 s\n",
      "13  100 equidistant bins, spline interpolation : MSEs = 0.03789657006220198 , 0.025330297031832105 in 0.038741000000072745 s\n",
      "14  200 equidistant bins, spline interpolation : MSEs = 0.04974767206679131 , 0.025758430928280757 in 0.04836599999998725 s\n",
      "15  500 equidistant bins, spline interpolation : MSEs = 0.050718024683773136 , 0.027888163490793434 in 0.06154500000002372 s\n",
      "16  25 variable bins : MSEs = 1.0929574876928974 , 1.5638786002976788 in 0.9799009999999271 s\n",
      "17  50 variable bins : MSEs = 1.0939173131606528 , 1.541965700099934 in 1.860490000000027 s\n",
      "18  100 variable bins : MSEs = 0.9544082854629852 , 1.3631193445505796 in 3.5214789999999994 s\n",
      "19  200 variable bins : MSEs = 0.9220919654294224 , 1.4909162927946509 in 6.857575999999881 s\n",
      "20  500 variable bins : MSEs = 5.3574839554613325 , 9.039990984029275 in 17.826593000000003 s\n",
      "21 Isotonic no interpolation : MSEs = 0.03740657207779264 , 0.02466978741027945 in 0.08666100000004917 s\n",
      "22  linear interpolation : MSEs = 0.03745170904132605 , 0.02448087518264096 in 0.17854899999997542 s\n",
      "23 Parametric sigmoid : MSEs = 0.041633979327974996 , 0.04064374912169886 in 1.1998199999998178 s\n",
      "24   : MSEs = 0.03833662958039634 , 0.023801373816929684 in 1.321753999999828 s\n",
      "Bandwidth means: 1.0 1.0\n",
      "Bandwidth means: 1.0 1.0\n",
      "25 KDE fixed bandwidth : MSEs = 0.057869199366889175 , 0.04178765568299547 in 261.76614500000005 s\n",
      "Bandwidth means: 1.1078288499832922 1.0297712583014715\n",
      "Bandwidth means: 1.1183910000732626 1.0347509916039526\n",
      "26  adaptive ($\\alpha=0.25$) : MSEs = 0.0580592290501747 , 0.04390178332505344 in 762.7927070000003 s\n",
      "Bandwidth means: 1.5794481940468472 1.1344647580975384\n",
      "Bandwidth means: 1.6467436333136383 1.1564463396858862\n",
      "27  adaptive ($\\alpha=0.5$) : MSEs = 0.05576682426280672 , 0.045762423996695716 in 850.3569630000002 s\n",
      "Bandwidth means: 2.941543574485482 1.3570299782541975\n",
      "Bandwidth means: 3.1932075814357948 1.404984856781792\n",
      "28  adaptive ($\\alpha=0.75$) : MSEs = 0.05279399874689426 , 0.047444704554436064 in 818.5012219999999 s\n",
      "Bandwidth means: 6.744729288551141 1.7978492357102809\n",
      "Bandwidth means: 7.502685878921382 1.8475160345437678\n",
      "29  adaptive ($\\alpha=1$) : MSEs = 0.05186150057759917 , 0.054518218622303195 in 800.2113440000003 s\n",
      "Bandwidth means: 0.20006656983358911 0.20006656983358911\n",
      "Bandwidth means: 0.20055994994735918 0.20055994994735918\n",
      "30 KDE grad-adaptive ($\\beta=0.2$) : MSEs = 0.0396038406602464 , 0.028210670071585404 in 300.276664 s\n",
      "Bandwidth means: 0.5001664245839728 0.5001664245839728\n",
      "Bandwidth means: 0.501399874868398 0.501399874868398\n",
      "31  grad-adaptive ($\\beta=0.5$) : MSEs = 0.038986122378459075 , 0.030395867687362174 in 278.1365510000005 s\n",
      "Bandwidth means: 1.0003328491679455 1.0003328491679455\n"
     ]
    }
   ],
   "source": [
    "r_hat_regr_calibrated_eval_list = []\n",
    "r_hat_carl_calibrated_eval_list = []\n",
    "r_hat_regr_calibrated_test_list = []\n",
    "r_hat_carl_calibrated_test_list = []\n",
    "mse_logr_regr_list = []\n",
    "mse_logr_carl_list = []\n",
    "\n",
    "for i, (label1, label2, tool, mode, settings) in enumerate(calibrations):\n",
    "    \n",
    "    # Do calibration\n",
    "    time_start = time.clock()\n",
    "    \n",
    "    if tool == 'none':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_none(True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_none(False)\n",
    "        \n",
    "    elif tool == 'carl':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_carl(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_carl(mode, settings, False)\n",
    "        \n",
    "    elif tool == 'kde':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_kde(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_kde(mode, settings, False)\n",
    "        \n",
    "    elif tool == 'fit':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_fit(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_fit(mode, settings, False)\n",
    "        \n",
    "    calibration_time = time.clock() - time_start\n",
    "        \n",
    "    # Store results for plots\n",
    "    r_hat_regr_calibrated_eval_list.append(r_hat_regr_calibrated_eval)\n",
    "    r_hat_carl_calibrated_eval_list.append(r_hat_carl_calibrated_eval)\n",
    "    r_hat_regr_calibrated_test_list.append(r_hat_regr_calibrated_test)\n",
    "    r_hat_carl_calibrated_test_list.append(r_hat_carl_calibrated_test)\n",
    "    \n",
    "    # Metrics\n",
    "    mse_logr_regr = mean_squared_error(np.log(r_test), np.log(r_hat_regr_calibrated_test))\n",
    "    mse_logr_carl = mean_squared_error(np.log(r_test), np.log(r_hat_carl_calibrated_test))\n",
    "    mse_logr_regr_list.append(mse_logr_regr)\n",
    "    mse_logr_carl_list.append(mse_logr_carl)\n",
    "    \n",
    "    print(i, label1, label2, ': MSEs =', mse_logr_carl, ',', mse_logr_regr, 'in', calibration_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix individual entries\n",
    "\n",
    "need_fixes = []\n",
    "\n",
    "for i, (label1, label2, tool, mode, settings) in enumerate(calibrations):\n",
    "\n",
    "    if i not in need_fixes:\n",
    "        continue\n",
    "\n",
    "    # Do calibration\n",
    "    time_start = time.clock()\n",
    "\n",
    "    if tool == 'none':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_none(True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_none(False)\n",
    "\n",
    "    elif tool == 'carl':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_carl(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_carl(mode, settings, False)\n",
    "\n",
    "    elif tool == 'kde':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_kde(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_kde(mode, settings, False)\n",
    "\n",
    "    elif tool == 'fit':\n",
    "        r_hat_regr_calibrated_eval, r_hat_regr_calibrated_test = calibrate_fit(mode, settings, True)\n",
    "        r_hat_carl_calibrated_eval, r_hat_carl_calibrated_test = calibrate_fit(mode, settings, False)\n",
    "\n",
    "    calibration_time = time.clock() - time_start\n",
    "\n",
    "    # Store results for plots\n",
    "    r_hat_regr_calibrated_eval_list[i] = r_hat_regr_calibrated_eval\n",
    "    r_hat_carl_calibrated_eval_list[i] = r_hat_carl_calibrated_eval\n",
    "    r_hat_regr_calibrated_test_list[i] = r_hat_regr_calibrated_test\n",
    "    r_hat_carl_calibrated_test_list[i] = r_hat_carl_calibrated_test\n",
    "\n",
    "    # Metrics\n",
    "    mse_logr_regr = mean_squared_error(np.log(r_test), np.log(r_hat_regr_calibrated_test))\n",
    "    mse_logr_carl = mean_squared_error(np.log(r_test), np.log(r_hat_carl_calibrated_test))\n",
    "    mse_logr_regr_list[i] = mse_logr_regr\n",
    "    mse_logr_carl_list[i] = mse_logr_carl\n",
    "\n",
    "    print(i, label1, label2, ': MSEs =', mse_logr_carl, ',', mse_logr_regr, 'in', calibration_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TablePrinter:\n",
    "    \n",
    "    def __init__(self, precisions, header=None):\n",
    "        \n",
    "        # Functions for metrics\n",
    "        self.n_metrics = len(precisions)\n",
    "        self.precisions = precisions\n",
    "        \n",
    "        # Total table and current block\n",
    "        self.table = ''\n",
    "        self.block_entries = []\n",
    "\n",
    "        # Formatting options\n",
    "        self.indent = '   '\n",
    "        self.col_sep = ' & '\n",
    "        self.end_row = r' \\\\'\n",
    "        self.midrule = r'\\midrule'\n",
    "        self.end_line = u'\\n'\n",
    "        self.emphasis_begin = r'\\mathbf{'\n",
    "        self.emphasis_end = r'}'\n",
    "        \n",
    "        # Header\n",
    "        self.table = ''\n",
    "        if header is not None:\n",
    "            self.table += self.indent + header + self.end_row + self.end_line\n",
    "    \n",
    "    \n",
    "    def finalise_block(self):\n",
    "\n",
    "        # Find best performance\n",
    "        block_metrics = [line[2:] for line in self.block_entries]\n",
    "        block_metrics = np.array(block_metrics)\n",
    "        block_best = []\n",
    "        for i in range(self.n_metrics):\n",
    "            try:\n",
    "                block_best.append(np.nanargmin(block_metrics[:,i]))\n",
    "            except ValueError:\n",
    "                block_best.append(-1)\n",
    "\n",
    "        # Format entries\n",
    "        text = ''\n",
    "        for i, line in enumerate(self.block_entries):\n",
    "            \n",
    "            # Labels\n",
    "            text += self.indent + line[0] + self.col_sep + line[1] + self.col_sep\n",
    "            \n",
    "            # Metrics\n",
    "            for j in range(self.n_metrics):\n",
    "                if np.isfinite(line[j + 2]):\n",
    "                    text += self.format_number(line[j + 2], self.precisions[j], emphasize=(i == block_best[j]))\n",
    "                if j == len(line) - 3:\n",
    "                    text += self.end_row + self.end_line\n",
    "                else:\n",
    "                    text += self.col_sep\n",
    "\n",
    "        # Add to document and reset for next block\n",
    "        self.table += text\n",
    "        self.block_entries = []\n",
    "    \n",
    "    \n",
    "    def new_block(self):\n",
    "        self.finalise_block()\n",
    "        self.table += self.indent + self.midrule + self.end_line\n",
    "    \n",
    "    \n",
    "    def format_number(self,\n",
    "                      number,\n",
    "                      precision=2,\n",
    "                      trailing_zeros=True,\n",
    "                      fix_minus_zero=True,\n",
    "                      latex_math_mode=True,\n",
    "                      emphasize=False):\n",
    "        if precision == 0:\n",
    "            temp =  str(int(round(number,precision)))\n",
    "        elif trailing_zeros:\n",
    "            temp =  ('{:.' + str(precision) + 'f}').format(round(number,precision))\n",
    "        else:\n",
    "            temp =  str(round(number,precision))\n",
    "        if fix_minus_zero and len(temp) > 0:\n",
    "            if temp[0] == '-' and float(temp) == 0.:\n",
    "                temp = temp[1:]\n",
    "        if latex_math_mode:\n",
    "            if emphasize:\n",
    "                temp = '$\\mathbf{' + temp + '}$'\n",
    "            else:\n",
    "                temp = '$' + temp + '$'\n",
    "        elif emphasize:\n",
    "            temp = r'\\emph{' + temp + r'}'\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "    def add(self, col1, col2, values, folder='parameterized'):\n",
    "        \n",
    "        # Label columns\n",
    "        line = [col1, col2] + values\n",
    "        self.block_entries.append(line)\n",
    "    \n",
    "    \n",
    "    def print(self):\n",
    "        self.finalise_block()\n",
    "        return self.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = TablePrinter(precisions=[4,3])\n",
    "\n",
    "for i, ((label1, label2, _, _, _), mse_regr, mse_carl) in enumerate(zip(calibrations,\n",
    "                                                        mse_logr_regr_list, mse_logr_carl_list)):\n",
    "    \n",
    "    if i > 0 and label1 != '':\n",
    "        table.new_block()\n",
    "        \n",
    "    table.add(label1, label2, [mse_regr, mse_carl])\n",
    "    \n",
    "print(table.print())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_indices = [0,3,17,20,21,23,27]\n",
    "linestyles=(['-'] + ['--',':','-.'] * 5)[:len(show_indices)]\n",
    "\n",
    "plt.figure(figsize=(8.,10.))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "for i, ls in zip(show_indices, linestyles):\n",
    "    plt.plot(s_eval, r_hat_regr_calibrated_eval_list[i], ls=ls,\n",
    "             label=plot_labels[i])\n",
    "\n",
    "plt.scatter(s_hat_regr_test[::10], r_test[::10], c='0.65', s=5., alpha=0.3, zorder=-10,\n",
    "           label='Raw $\\hat{s}$ vs true $r$')\n",
    "\n",
    "plt.xlabel('Raw $\\hat{s}$ (PbP regression)')\n",
    "plt.ylabel('Calibrated $\\hat{r}$ (PbP regression)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,2.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "for i, ls in zip(show_indices, linestyles):\n",
    "    plt.plot(s_eval, r_hat_carl_calibrated_eval_list[i], ls=ls,\n",
    "             label=plot_labels[i])\n",
    "\n",
    "plt.scatter(s_hat_carl_test[::10], r_test[::10], c='0.65', s=5., alpha=0.3, zorder=-10,\n",
    "           label='Raw $\\hat{s}$ vs true $r$')\n",
    "\n",
    "plt.xlabel('Raw $\\hat{s}$ (PbP carl)')\n",
    "plt.ylabel('Calibrated $\\hat{r}$ (PbP carl)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.,2.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_dir + '/calibration_curves.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8.,10.))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "for i, ls in zip(show_indices, linestyles):\n",
    "    plt.plot(s_eval, s(r_hat_regr_calibrated_eval_list[i]), ls=ls,\n",
    "             label=plot_labels[i])\n",
    "\n",
    "plt.scatter(s_hat_regr_test[::10], s(r_test[::10]), c='0.65', s=5., alpha=0.3, zorder=-10,\n",
    "           label='Raw $\\hat{s}$ vs true $s$')\n",
    "\n",
    "plt.xlabel('Raw $\\hat{s}$ (PbP regression)')\n",
    "plt.ylabel('Calibrated $\\hat{s}$ (PbP regression)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.2,1.)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "for i, ls in zip(show_indices, linestyles):\n",
    "    plt.plot(s_eval, s(r_hat_carl_calibrated_eval_list[i]),ls=ls,\n",
    "             label=plot_labels[i])\n",
    "\n",
    "plt.scatter(s_hat_carl_test[::10], s(r_test[::10]), c='0.65', s=5., alpha=0.3, zorder=-10,\n",
    "           label='Raw $\\hat{s}$ vs true $s$')\n",
    "\n",
    "plt.xlabel('Raw $\\hat{s}$ (PbP carl)')\n",
    "plt.ylabel('Calibrated $\\hat{s}$ (PbP carl)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0.2,1.)\n",
    "plt.ylim(0.2,1.)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_dir + '/calibration_s_curves.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
