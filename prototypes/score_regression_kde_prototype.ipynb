{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data_dir = '../data'\n",
    "\n",
    "X_train = np.load(data_dir + '/unweighted_events/X_train_scoreregression.npy')\n",
    "scores_train = np.load(data_dir + '/unweighted_events/scores_train_scoreregression.npy')\n",
    "\n",
    "X_calibration = np.load(data_dir + '/unweighted_events/X_calibration.npy')\n",
    "weights_calibration = np.load(\n",
    "    data_dir + '/unweighted_events/weights_calibration_debug.npy')\n",
    "    \n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(X_train, dtype=np.float64))\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_calibration_transformed = scaler.transform(X_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = 708\n",
    "theta = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regressor(n_hidden_layers=3,\n",
    "                    hidden_layer_size=100,\n",
    "                    activation='tanh',\n",
    "                    dropout_prob=0.0):\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(42,))\n",
    "\n",
    "    # Network\n",
    "    hidden_layer = Dense(hidden_layer_size, activation=activation)(input_layer)\n",
    "    if n_hidden_layers > 1:\n",
    "        hidden_layer_ = hidden_layers(n_hidden_layers - 1,\n",
    "                                      hidden_layer_size=hidden_layer_size,\n",
    "                                      activation=activation,\n",
    "                                      dropout_prob=dropout_prob)\n",
    "        hidden_layer = hidden_layer_(hidden_layer)\n",
    "    score_layer = Dense(2, activation='linear')(hidden_layer)\n",
    "\n",
    "    # Combine outputs\n",
    "    model = Model(inputs=[input_layer], outputs=[score_layer])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.Adam(clipnorm=1.))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = np.load(data_dir + '/unweighted_events/X_train_scoreregression.npy')\n",
    "scores_train = np.load(data_dir + '/unweighted_events/scores_train_scoreregression.npy')\n",
    "\n",
    "X_calibration = np.load(data_dir + '/unweighted_events/X_calibration.npy')\n",
    "weights_calibration = np.load(\n",
    "    data_dir + '/unweighted_events/weights_calibration_debug.npy')\n",
    "    \n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(X_train, dtype=np.float64))\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_calibration_transformed = scaler.transform(X_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train score regression\n",
    "regr = KerasRegressor(lambda: make_regressor(n_hidden_layers=1),\n",
    "                      epochs=1, validation_split=0.142857,\n",
    "                      verbose=2)\n",
    "\n",
    "regr.fit(X_train_transformed, scores_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class weighted_gaussian_kde(object):\n",
    "    \"\"\"Representation of a kernel-density estimate using Gaussian kernels.\n",
    "\n",
    "    Kernel density estimation is a way to estimate the probability density\n",
    "    function (PDF) of a random variable in a non-parametric way.\n",
    "    `gaussian_kde` works for both uni-variate and multi-variate data.   It\n",
    "    includes automatic bandwidth determination.  The estimation works best for\n",
    "    a unimodal distribution; bimodal or multi-modal distributions tend to be\n",
    "    oversmoothed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : array_like\n",
    "        Datapoints to estimate from. In case of univariate data this is a 1-D\n",
    "        array, otherwise a 2-D array with shape (# of dims, # of data).\n",
    "    bw_method : str, scalar or callable, optional\n",
    "        The method used to calculate the estimator bandwidth.  This can be\n",
    "        'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n",
    "        this will be used directly as `kde.factor`.  If a callable, it should\n",
    "        take a `gaussian_kde` instance as only parameter and return a scalar.\n",
    "        If None (default), 'scott' is used.  See Notes for more details.\n",
    "    weights : array_like, shape (n, ), optional, default: None\n",
    "        An array of weights, of the same shape as `x`.  Each value in `x`\n",
    "        only contributes its associated weight towards the bin count\n",
    "        (instead of 1).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : ndarray\n",
    "        The dataset with which `gaussian_kde` was initialized.\n",
    "    d : int\n",
    "        Number of dimensions.\n",
    "    n : int\n",
    "        Number of datapoints.\n",
    "    neff : float\n",
    "        Effective sample size using Kish's approximation.\n",
    "    factor : float\n",
    "        The bandwidth factor, obtained from `kde.covariance_factor`, with which\n",
    "        the covariance matrix is multiplied.\n",
    "    covariance : ndarray\n",
    "        The covariance matrix of `dataset`, scaled by the calculated bandwidth\n",
    "        (`kde.factor`).\n",
    "    inv_cov : ndarray\n",
    "        The inverse of `covariance`.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    kde.evaluate(points) : ndarray\n",
    "        Evaluate the estimated pdf on a provided set of points.\n",
    "    kde(points) : ndarray\n",
    "        Same as kde.evaluate(points)\n",
    "    kde.pdf(points) : ndarray\n",
    "        Alias for ``kde.evaluate(points)``.\n",
    "    kde.set_bandwidth(bw_method='scott') : None\n",
    "        Computes the bandwidth, i.e. the coefficient that multiplies the data\n",
    "        covariance matrix to obtain the kernel covariance matrix.\n",
    "        .. versionadded:: 0.11.0\n",
    "    kde.covariance_factor : float\n",
    "        Computes the coefficient (`kde.factor`) that multiplies the data\n",
    "        covariance matrix to obtain the kernel covariance matrix.\n",
    "        The default is `scotts_factor`.  A subclass can overwrite this method\n",
    "        to provide a different method, or set it through a call to\n",
    "        `kde.set_bandwidth`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Bandwidth selection strongly influences the estimate obtained from the KDE\n",
    "    (much more so than the actual shape of the kernel).  Bandwidth selection\n",
    "    can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n",
    "    methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n",
    "    uses a rule of thumb, the default is Scott's Rule.\n",
    "\n",
    "    Scott's Rule [1]_, implemented as `scotts_factor`, is::\n",
    "\n",
    "        n**(-1./(d+4)),\n",
    "\n",
    "    with ``n`` the number of data points and ``d`` the number of dimensions.\n",
    "    Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n",
    "\n",
    "        (n * (d + 2) / 4.)**(-1. / (d + 4)).\n",
    "\n",
    "    Good general descriptions of kernel density estimation can be found in [1]_\n",
    "    and [2]_, the mathematics for this multi-dimensional implementation can be\n",
    "    found in [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n",
    "           Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n",
    "    .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n",
    "           Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n",
    "           Chapman and Hall, London, 1986.\n",
    "    .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n",
    "           Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n",
    "    .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n",
    "           conditional density estimation\", Computational Statistics & Data\n",
    "           Analysis, Vol. 36, pp. 279-298, 2001.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Generate some random two-dimensional data:\n",
    "\n",
    "    >>> from scipy import stats\n",
    "    >>> def measure(n):\n",
    "    >>>     \"Measurement model, return two coupled measurements.\"\n",
    "    >>>     m1 = np.random.normal(size=n)\n",
    "    >>>     m2 = np.random.normal(scale=0.5, size=n)\n",
    "    >>>     return m1+m2, m1-m2\n",
    "\n",
    "    >>> m1, m2 = measure(2000)\n",
    "    >>> xmin = m1.min()\n",
    "    >>> xmax = m1.max()\n",
    "    >>> ymin = m2.min()\n",
    "    >>> ymax = m2.max()\n",
    "\n",
    "    Perform a kernel density estimate on the data:\n",
    "\n",
    "    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    >>> values = np.vstack([m1, m2])\n",
    "    >>> kernel = stats.gaussian_kde(values)\n",
    "    >>> Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "    Plot the results:\n",
    "\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> fig = plt.figure()\n",
    "    >>> ax = fig.add_subplot(111)\n",
    "    >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "    ...           extent=[xmin, xmax, ymin, ymax])\n",
    "    >>> ax.plot(m1, m2, 'k.', markersize=2)\n",
    "    >>> ax.set_xlim([xmin, xmax])\n",
    "    >>> ax.set_ylim([ymin, ymax])\n",
    "    >>> plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, bw_method=None, weights=None):\n",
    "        self.dataset = np.atleast_2d(dataset)\n",
    "        if not self.dataset.size > 1:\n",
    "            raise ValueError(\"`dataset` input should have multiple elements.\")\n",
    "        self.d, self.n = self.dataset.shape\n",
    "            \n",
    "        if weights is not None:\n",
    "            self.weights = weights / np.sum(weights)\n",
    "        else:\n",
    "            self.weights = np.ones(self.n) / self.n\n",
    "            \n",
    "        # Compute the effective sample size \n",
    "        # http://surveyanalysis.org/wiki/Design_Effects_and_Effective_Sample_Size#Kish.27s_approximate_formula_for_computing_effective_sample_size\n",
    "        self.neff = 1.0 / np.sum(self.weights ** 2)\n",
    "\n",
    "        self.set_bandwidth(bw_method=bw_method)\n",
    "\n",
    "    def evaluate(self, points):\n",
    "        \"\"\"Evaluate the estimated pdf on a set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : (# of dimensions, # of points)-array\n",
    "            Alternatively, a (# of dimensions,) vector can be passed in and\n",
    "            treated as a single point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        values : (# of points,)-array\n",
    "            The values at each point.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError : if the dimensionality of the input points is different than\n",
    "                     the dimensionality of the KDE.\n",
    "\n",
    "        \"\"\"\n",
    "        points = np.atleast_2d(points)\n",
    "\n",
    "        d, m = points.shape\n",
    "        if d != self.d:\n",
    "            if d == 1 and m == self.d:\n",
    "                # points was passed in as a row vector\n",
    "                points = np.reshape(points, (self.d, 1))\n",
    "                m = 1\n",
    "            else:\n",
    "                msg = \"points have dimension %s, dataset has dimension %s\" % (d,\n",
    "                    self.d)\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        # compute the normalised residuals\n",
    "        chi2 = cdist(points.T, self.dataset.T, 'mahalanobis', VI=self.inv_cov) ** 2\n",
    "        # compute the pdf\n",
    "        result = np.sum(np.exp(-.5 * chi2) * self.weights, axis=1) / self._norm_factor\n",
    "\n",
    "        return result\n",
    "\n",
    "    __call__ = evaluate\n",
    "\n",
    "    def scotts_factor(self):\n",
    "        return np.power(self.neff, -1./(self.d+4))\n",
    "\n",
    "    def silverman_factor(self):\n",
    "        return np.power(self.neff*(self.d+2.0)/4.0, -1./(self.d+4))\n",
    "\n",
    "    #  Default method to calculate bandwidth, can be overwritten by subclass\n",
    "    covariance_factor = scotts_factor\n",
    "\n",
    "    def set_bandwidth(self, bw_method=None):\n",
    "        \"\"\"Compute the estimator bandwidth with given method.\n",
    "\n",
    "        The new bandwidth calculated after a call to `set_bandwidth` is used\n",
    "        for subsequent evaluations of the estimated density.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bw_method : str, scalar or callable, optional\n",
    "            The method used to calculate the estimator bandwidth.  This can be\n",
    "            'scott', 'silverman', a scalar constant or a callable.  If a\n",
    "            scalar, this will be used directly as `kde.factor`.  If a callable,\n",
    "            it should take a `gaussian_kde` instance as only parameter and\n",
    "            return a scalar.  If None (default), nothing happens; the current\n",
    "            `kde.covariance_factor` method is kept.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        .. versionadded:: 0.11\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> x1 = np.array([-7, -5, 1, 4, 5.])\n",
    "        >>> kde = stats.gaussian_kde(x1)\n",
    "        >>> xs = np.linspace(-10, 10, num=50)\n",
    "        >>> y1 = kde(xs)\n",
    "        >>> kde.set_bandwidth(bw_method='silverman')\n",
    "        >>> y2 = kde(xs)\n",
    "        >>> kde.set_bandwidth(bw_method=kde.factor / 3.)\n",
    "        >>> y3 = kde(xs)\n",
    "\n",
    "        >>> fig = plt.figure()\n",
    "        >>> ax = fig.add_subplot(111)\n",
    "        >>> ax.plot(x1, np.ones(x1.shape) / (4. * x1.size), 'bo',\n",
    "        ...         label='Data points (rescaled)')\n",
    "        >>> ax.plot(xs, y1, label='Scott (default)')\n",
    "        >>> ax.plot(xs, y2, label='Silverman')\n",
    "        >>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')\n",
    "        >>> ax.legend()\n",
    "        >>> plt.show()\n",
    "\n",
    "        \"\"\"\n",
    "        if bw_method is None:\n",
    "            pass\n",
    "        elif bw_method == 'scott':\n",
    "            self.covariance_factor = self.scotts_factor\n",
    "        elif bw_method == 'silverman':\n",
    "            self.covariance_factor = self.silverman_factor\n",
    "        elif np.isscalar(bw_method) and not isinstance(bw_method, string_types):\n",
    "            self._bw_method = 'use constant'\n",
    "            self.covariance_factor = lambda: bw_method\n",
    "        elif callable(bw_method):\n",
    "            self._bw_method = bw_method\n",
    "            self.covariance_factor = lambda: self._bw_method(self)\n",
    "        else:\n",
    "            msg = \"`bw_method` should be 'scott', 'silverman', a scalar \" \\\n",
    "                  \"or a callable.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self._compute_covariance()\n",
    "\n",
    "    def _compute_covariance(self):\n",
    "        \"\"\"Computes the covariance matrix for each Gaussian kernel using\n",
    "        covariance_factor().\n",
    "        \"\"\"\n",
    "        self.factor = self.covariance_factor()\n",
    "        # Cache covariance and inverse covariance of the data\n",
    "        if not hasattr(self, '_data_inv_cov'):\n",
    "            # Compute the mean and residuals\n",
    "            _mean = np.sum(self.weights * self.dataset, axis=1)\n",
    "            _residual = (self.dataset - _mean[:, None])\n",
    "            # Compute the biased covariance\n",
    "            self._data_covariance = np.atleast_2d(np.dot(_residual * self.weights, _residual.T))\n",
    "            # Correct for bias (http://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_covariance)\n",
    "            self._data_covariance /= (1 - np.sum(self.weights ** 2))\n",
    "            self._data_inv_cov = np.linalg.inv(self._data_covariance)\n",
    "\n",
    "        self.covariance = self._data_covariance * self.factor**2\n",
    "        self.inv_cov = self._data_inv_cov / self.factor**2\n",
    "        self._norm_factor = np.sqrt(np.linalg.det(2*np.pi*self.covariance)) #* self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_nom = gaussian_kde(X_calibration_transformed, weights=weights_calibration[theta])\n",
    "pdf_den = gaussian_kde(X_calibration_transformed, weights=weights_calibration[theta1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot pdfs and ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(-5.0, 5.0, 100)\n",
    "yi = np.linspace(-5.0, 5.0, 100)\n",
    "xx, yy = np.meshgrid(xi, yi)\n",
    "scores_eval = np.asarray((xx.flatten(), yy.flatten())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nom_eval = pdf_nom(scores_eval)\n",
    "p_den_eval = pdf_den(scores_eval)\n",
    "ratio_eval = p_nom_eval / p_den_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_bounds() argument after * must be an iterable, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ce2ae0ab0d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m cs = plt.contourf(xi, yi, np.log(p_nom_eval).reshape((100, 100)), 100, cmap=\"viridis\",\n\u001b[1;32m      5\u001b[0m              vmin=-5., vmax=5.)\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m                                         \u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                                         \u001b[0mFigureClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFigureClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfigLabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[0;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mfig_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FigureClass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_figure_manager_given_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, figsize, dpi, facecolor, edgecolor, linewidth, frameon, subplotpars, tight_layout)\u001b[0m\n\u001b[1;32m    336\u001b[0m             raise ValueError('figure size must be finite not '\n\u001b[1;32m    337\u001b[0m                              '{}'.format(figsize))\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi_scale_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: from_bounds() argument after * must be an iterable, not float"
     ]
    }
   ],
   "source": [
    "plt.figure(12.,4.)\n",
    "\n",
    "\n",
    "\n",
    "plt.subfigure(1,3,1)\n",
    "cs = plt.contourf(xi, yi, np.log(p_nom_eval).reshape((100, 100)), 100, cmap=\"viridis\",\n",
    "             vmin=-5., vmax=5.)\n",
    "cbar = plt.colorbar()\n",
    "plt.xlim('$t_0$')\n",
    "plt.ylim('$t_1$')\n",
    "cbar.set_label(r'$\\log p(t|\\theta_0)$')\n",
    "\n",
    "\n",
    "\n",
    "plt.subfigure(1,3,2)\n",
    "cs = plt.contourf(xi, yi, np.log(p_den_eval).reshape((100, 100)), 100, cmap=\"viridis\",\n",
    "             vmin=-5., vmax=5.)\n",
    "cbar = plt.colorbar()\n",
    "plt.xlim('$t_0$')\n",
    "plt.ylim('$t_1$')\n",
    "cbar.set_label(r'$\\log p(t|\\theta_1)$')\n",
    "\n",
    "\n",
    "\n",
    "plt.subfigure(1,3,2)\n",
    "cs = plt.contourf(xi, yi, np.log(r_eval).reshape((100, 100)), 100, cmap=\"viridis\",\n",
    "             vmin=-5., vmax=5.)\n",
    "cbar = plt.colorbar()\n",
    "plt.xlim('$t_0$')\n",
    "plt.ylim('$t_1$')\n",
    "cbar.set_label(r'$\\log r(t; \\theta_0, \\theta_1)$')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:higgs_inference]",
   "language": "python",
   "name": "conda-env-higgs_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
